{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ca09f9",
   "metadata": {},
   "source": [
    "# Model Compression Toolkit (MCT) Wrapper API Comprehensive Quantization Comparison(tensorflow)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/SonySemiconductorSolutions/mct-model-optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_mct_wrapper.ipynb)\n",
    "\n",
    "## Overview \n",
    "This notebook provides a comprehensive demonstration of the MCT (Model Compression Toolkit) Wrapper API functionality, showcasing five different quantization methods on a MobileNetV2 model. The tutorial systematically compares the implementation, performance characteristics, and accuracy trade-offs of each quantization approach: PTQ (Post-Training Quantization), PTQ with Mixed Precision, GPTQ (Gradient-based PTQ), GPTQ with Mixed Precision, and LQ-PTQ (Low-bit Quantizer PTQ). Each method utilizes the unified MCTWrapper interface for consistent implementation and comparison.\n",
    "\n",
    "## Summary\n",
    "1. **Environment Setup**: Import required libraries and configure MCT with MobileNetV2 model\n",
    "2. **Dataset Preparation**: Load and prepare ImageNet validation dataset with representative data generation\n",
    "3. **PTQ Implementation**: Execute basic Post-Training Quantization with 8-bit precision and bias correction\n",
    "4. **PTQ + Mixed Precision**: Apply intelligent bit-width allocation based on layer sensitivity analysis (75% compression ratio)\n",
    "5. **GPTQ Implementation**: Perform gradient-based optimization with 5-epoch fine-tuning for enhanced accuracy\n",
    "6. **GPTQ + Mixed Precision**: Combine gradient optimization with mixed precision for optimal accuracy-compression trade-off\n",
    "7. **Performance Evaluation**: Comprehensive accuracy assessment and comparison across all quantization methods\n",
    "8. **Results Analysis**: Compare model sizes, inference accuracy, and quantization trade-offs\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0e667e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_VER = '2.14.0'\n",
    "!pip install -q tensorflow~={TF_VER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e138ad45-68d2-4c34-95ac-1a327ce54406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 07:02:27.950685: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-06 07:02:27.991442: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-06 07:02:28.190960: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-06 07:02:28.190993: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-06 07:02:28.192197: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-06 07:02:28.302102: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-06 07:02:28.303276: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-06 07:02:29.113347: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#import importlib\n",
    "#if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "#    !pip install model_compression_toolkit\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/wrapper/sonyfork/mct-model-optimization')\n",
    "\n",
    "import model_compression_toolkit as mct\n",
    "from model_compression_toolkit.core import QuantizationErrorMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0ece2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60853205",
   "metadata": {},
   "source": [
    "Load a pre-trained MobileNetV2 model from Keras, in 32-bits floating-point precision format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e743078d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 07:02:33.229062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-11-06 07:02:33.233043: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "float_model = MobileNetV2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce48a1",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f14d92-5116-40b7-9090-f378bdf3cc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-06 07:02:33--  https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
      "Resolving proxy.mei.co.jp (proxy.mei.co.jp)... 10.77.8.70\n",
      "Connecting to proxy.mei.co.jp (proxy.mei.co.jp)|10.77.8.70|:8080... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 2568145 (2.4M) [application/x-gzip]\n",
      "Saving to: ‘imagenet/ILSVRC2012_devkit_t12.tar.gz’\n",
      "\n",
      "ILSVRC2012_devkit_t 100%[===================>]   2.45M  1.99MB/s    in 1.2s    \n",
      "\n",
      "2025-11-06 07:02:35 (1.99 MB/s) - ‘imagenet/ILSVRC2012_devkit_t12.tar.gz’ saved [2568145/2568145]\n",
      "\n",
      "--2025-11-06 07:02:35--  https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
      "Resolving proxy.mei.co.jp (proxy.mei.co.jp)... 10.77.8.70\n",
      "Connecting to proxy.mei.co.jp (proxy.mei.co.jp)|10.77.8.70|:8080... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 6744924160 (6.3G) [application/x-tar]\n",
      "Saving to: ‘imagenet/ILSVRC2012_img_val.tar’\n",
      "\n",
      "ILSVRC2012_img_val. 100%[===================>]   6.28G  10.7MB/s    in 33m 51s \n",
      "\n",
      "2025-11-06 07:36:27 (3.17 MB/s) - ‘imagenet/ILSVRC2012_img_val.tar’ saved [6744924160/6744924160]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "if not os.path.isdir('imagenet'):\n",
    "    !mkdir imagenet\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
    "    \n",
    "    !cd imagenet && tar -xzf ILSVRC2012_devkit_t12.tar.gz && \\\n",
    "     mkdir ILSVRC2012_img_val && tar -xf ILSVRC2012_img_val.tar -C ILSVRC2012_img_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41859aab",
   "metadata": {},
   "source": [
    "The following code organizes the extracted data into separate folders for each label, making it compatible with Keras dataset loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4bd4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "root = Path('./imagenet')\n",
    "imgs_dir = root / 'ILSVRC2012_img_val'\n",
    "target_dir = root /'val'\n",
    "\n",
    "def extract_labels():\n",
    "    !pip install -q scipy\n",
    "    import scipy\n",
    "    mat = scipy.io.loadmat(root / 'ILSVRC2012_devkit_t12/data/meta.mat', squeeze_me=True)\n",
    "    cls_to_nid = {s[0]: s[1] for i, s in enumerate(mat['synsets']) if s[4] == 0} \n",
    "    with open(root / 'ILSVRC2012_devkit_t12/data/ILSVRC2012_validation_ground_truth.txt', 'r') as f:\n",
    "        return [cls_to_nid[int(cls)] for cls in f.readlines()]\n",
    "\n",
    "if not target_dir.exists():\n",
    "    labels = extract_labels()\n",
    "    for lbl in set(labels):\n",
    "        os.makedirs(target_dir / lbl)\n",
    "    \n",
    "    for img_file, lbl in zip(sorted(os.listdir(imgs_dir)), labels):\n",
    "        shutil.move(imgs_dir / img_file, target_dir / lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8dd14b",
   "metadata": {},
   "source": [
    "These functions generate a `tf.data.Dataset` from image files in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5212d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagenet_preprocess_input(images, labels):\n",
    "    return tf.keras.applications.mobilenet_v2.preprocess_input(images), labels\n",
    "\n",
    "def get_dataset(batch_size, shuffle):\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory='./imagenet/val',\n",
    "        batch_size=batch_size,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=shuffle,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear')\n",
    "    dataset = dataset.map(lambda x, y: (imagenet_preprocess_input(x, y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c490a2e",
   "metadata": {},
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f60a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 1000 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "n_iter = 5\n",
    "\n",
    "dataset = get_dataset(batch_size, shuffle=True)\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    for _ in range(n_iter):\n",
    "        yield [dataset.take(1).get_single_element()[0].numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa8e319",
   "metadata": {},
   "source": [
    "## Model Post-Training quantization using MCTWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd8db92-9151-42d6-b3fa-311f7d528df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator to provide consistent logging and error handling for quantization functions\n",
    "def decorator(func):\n",
    "    \"\"\"\n",
    "    Wrapper decorator that provides standardized execution logging and error handling.\n",
    "    \n",
    "    This decorator enhances quantization functions by:\n",
    "    - Providing clear start/end execution markers for debugging\n",
    "    - Handling success/failure status from quantization operations\n",
    "    - Implementing fail-fast behavior on quantization errors\n",
    "    - Ensuring consistent logging format across all quantization methods\n",
    "    \n",
    "    Usage:\n",
    "        @decorator\n",
    "        def quantization_function(model):\n",
    "            # quantization implementation\n",
    "            return flag, quantized_model\n",
    "    \n",
    "    Args:\n",
    "        func: Function to be decorated (typically a quantization function)\n",
    "    \n",
    "    Returns:\n",
    "        Wrapped function with enhanced logging and error handling capabilities\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Log function execution start with clear delimiter\n",
    "        print(f\"----------------- {func.__name__} Start ---------------\")\n",
    "        \n",
    "        # Execute the quantization function and capture return values\n",
    "        # Expected return format: (success_flag, quantized_model)\n",
    "        flag, result = func(*args, **kwargs)\n",
    "        \n",
    "        # Log function execution completion\n",
    "        print(f\"----------------- {func.__name__} End -----------------\")\n",
    "        \n",
    "        # Implement fail-fast behavior: exit immediately on quantization failure\n",
    "        # This ensures early detection of quantization issues\n",
    "        if not flag:\n",
    "            exit()\n",
    "        \n",
    "        # Return original function results if successful\n",
    "        return flag, result\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf35f64",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ (Post-Training Quantization) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e151ddca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Keras(float_model):\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization (PTQ) using MCT on Keras model.\n",
    "    \n",
    "    PTQ is a quantization method that:\n",
    "    - Does not require model retraining\n",
    "    - Uses representative data for calibration\n",
    "    - Provides good accuracy with minimal computational overhead\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for basic PTQ quantization\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_internal_tpc = True                # Use MCT's built-in Target Platform Capabilities\n",
    "    use_mixed_precision = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PTQ\n",
    "    param_items = [\n",
    "        ['tpc_version', '1.0', 'The version of the TPC to use.'],\n",
    "        \n",
    "        # Quantization configuration parameters\n",
    "        ['activation_error_method', QuantizationErrorMethod.MSE, 'Error metric for activation quantization'],\n",
    "        ['weights_bias_correction', True, 'Enable bias correction for weights'],\n",
    "        ['z_threshold', float('inf'), 'Threshold for zero-point quantization'],\n",
    "        ['linear_collapsing', True, 'Enable linear layer collapsing optimization'],\n",
    "        ['residual_collapsing', True, 'Enable residual connection collapsing'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Keras.tflite', 'Path to save the quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_internal_tpc, use_mixed_precision, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f6eeb",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ + Mixed Precision Quantization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c082471a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Keras_mixed_precision(float_model):\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization with Mixed Precision (PTQ + mixed_precision) on Keras model.\n",
    "    \n",
    "    Mixed Precision Quantization:\n",
    "    - Uses different bit-widths for different layers\n",
    "    - Optimizes model size while maintaining accuracy\n",
    "    - Automatically selects optimal precision for each layer\n",
    "    - Uses resource constraints to guide precision allocation\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PTQ with mixed precision\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_internal_tpc = True                # Use MCT's built-in Target Platform Capabilities\n",
    "    use_mixed_precision = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        ['tpc_version', '1.0', 'The version of the TPC to use.'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based sensitivity scores for layer importance'],\n",
    "        \n",
    "        # Resource constraint configuration\n",
    "        ['weights_compression_ratio', 0.75, 'Target compression ratio for model weights (75% of original size)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Keras_mixed_precision.tflite', 'Path to save the mixed precision quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute mixed precision quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_internal_tpc, use_mixed_precision, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f2cba",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ (Gradient-based PTQ) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d82c0070",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Keras(float_model):\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization (GPTQ) on Keras model.\n",
    "    \n",
    "    GPTQ is an advanced quantization method that:\n",
    "    - Uses gradient information to optimize quantization parameters\n",
    "    - Fine-tunes the model during quantization process\n",
    "    - Generally provides better accuracy than standard PTQ\n",
    "    - Requires slightly more computational resources than PTQ\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for GPTQ quantization\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_internal_tpc = True                # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_mixed_precision = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for GPTQ\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Keras.tflite', 'Path to save the GPTQ quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute GPTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_internal_tpc, use_mixed_precision, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d77b5",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ + Mixed Precision Quantization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12e06b4e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Keras_mixed_precision(float_model):\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization with Mixed Precision (GPTQ + mixed_precision).\n",
    "    \n",
    "    This combines the benefits of both techniques:\n",
    "    - GPTQ: Gradient-based optimization for better quantization accuracy\n",
    "    - Mixed Precision: Optimal bit-width allocation for size/accuracy trade-off\n",
    "    \n",
    "    This is the most advanced quantization method available, providing:\n",
    "    - Best possible accuracy preservation\n",
    "    - Optimal model size reduction\n",
    "    - Automatic precision selection per layer\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for GPTQ with mixed precision\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_internal_tpc = True                # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_mixed_precision = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for GPTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default)'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision sensitivity analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based scores for layer importance ranking'],\n",
    "        \n",
    "        # Resource constraint configuration\n",
    "        ['weights_compression_ratio', 0.75, 'Target compression ratio for model weights (75% reduction)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Keras_mixed_precision.tflite', 'Path to save the GPTQ+mixed_precision quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute advanced GPTQ+mixed_precision quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_internal_tpc, use_mixed_precision, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fc4ce",
   "metadata": {},
   "source": [
    "### Run model Post-Training Quantization\n",
    "Lastly, we quantize our model using MCTWrapper API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b959792-f339-43a9-9573-a95052b28e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting quantization experiments with different methods...\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained MobileNetV2 model as the base model for quantization experiments\n",
    "# This model serves as the reference floating-point model for all quantization methods\n",
    "float_model = MobileNetV2()\n",
    "\n",
    "# Execute comprehensive quantization method comparison using MCT Wrapper functionality\n",
    "# Each method represents different trade-offs between accuracy, model size, and computation time\n",
    "print(\"Starting quantization experiments with different methods...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bce5ebd0-aa83-4bb7-9e23-5ae3b95858dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Keras Start ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Statistics Collection: 5it [00:13,  2.62s/it]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 105/105 [00:08<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6akw4o6x/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6akw4o6x/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Keras End -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 07:37:07.968689: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-11-06 07:37:07.968728: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-11-06 07:37:07.969430: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp6akw4o6x\n",
      "2025-11-06 07:37:07.976394: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-11-06 07:37:07.976406: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp6akw4o6x\n",
      "2025-11-06 07:37:07.991779: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
      "2025-11-06 07:37:07.994463: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-11-06 07:37:08.084052: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp6akw4o6x\n",
      "2025-11-06 07:37:08.114992: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 145562 microseconds.\n",
      "2025-11-06 07:37:08.164956: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Basic Post-Training Quantization (PTQ)\n",
    "# - Standard 8-bit quantization without advanced optimization techniquesed\n",
    "flag, quantized_model = PTQ_Keras(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9990c4d4-0ffe-4069-ba9b-6d14e9722edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Keras_mixed_precision Start ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Statistics Collection: 5it [00:13,  2.62s/it]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 105/105 [00:26<00:00,  4.03it/s]\n",
      "53it [00:22,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/ubuntu/.local/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/bca1c515b45e4c878019f8094542eba7-pulp.mps -sec 60 -timeMode elapsed -branch -printingOptions all -solution /tmp/bca1c515b45e4c878019f8094542eba7-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 112 COLUMNS\n",
      "At line 961 RHS\n",
      "At line 1069 BOUNDS\n",
      "At line 1229 ENDATA\n",
      "Problem MODEL has 107 rows, 212 columns and 477 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "seconds was changed from 1e+100 to 60\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 0.0457804 - 0.00 seconds\n",
      "Cgl0004I processed model has 54 rows, 159 columns (159 integer (159 of which binary)) and 265 elements\n",
      "Cbc0038I Initial state - 2 integers unsatisfied sum - 0.589583\n",
      "Cbc0038I Pass   1: suminf.    0.14150 (2) obj. 0.0621947 iterations 2\n",
      "Cbc0038I Solution found of 0.300313\n",
      "Cbc0038I Before mini branch and bound, 155 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 2 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.300313 to 0.0498835 (0.00 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.0494642\n",
      "Cbc0038I Reduced cost fixing fixed 94 variables on major pass 2\n",
      "Cbc0038I Pass   2: suminf.    0.58958 (2) obj. 0.0457804 iterations 3\n",
      "Cbc0038I Pass   3: suminf.    0.14414 (2) obj. 0.0494642 iterations 3\n",
      "Cbc0038I Pass   4: suminf.    0.14414 (2) obj. 0.0494642 iterations 4\n",
      "Cbc0038I Pass   5: suminf.    0.14414 (2) obj. 0.0494642 iterations 0\n",
      "Cbc0038I Pass   6: suminf.    0.14414 (2) obj. 0.0494642 iterations 6\n",
      "Cbc0038I Pass   7: suminf.    0.14414 (2) obj. 0.0494642 iterations 4\n",
      "Cbc0038I Pass   8: suminf.    0.45290 (2) obj. 0.0494642 iterations 3\n",
      "Cbc0038I Pass   9: suminf.    0.45290 (2) obj. 0.0494642 iterations 0\n",
      "Cbc0038I Pass  10: suminf.    0.47708 (2) obj. 0.0463513 iterations 1\n",
      "Cbc0038I Pass  11: suminf.    0.46021 (2) obj. 0.0482915 iterations 2\n",
      "Cbc0038I Pass  12: suminf.    0.46021 (2) obj. 0.0482915 iterations 0\n",
      "Cbc0038I Pass  13: suminf.    0.86332 (2) obj. 0.0494642 iterations 1\n",
      "Cbc0038I Pass  14: suminf.    0.45281 (4) obj. 0.0494642 iterations 4\n",
      "Cbc0038I Pass  15: suminf.    0.45281 (4) obj. 0.0494642 iterations 4\n",
      "Cbc0038I Pass  16: suminf.    0.45290 (2) obj. 0.0494642 iterations 2\n",
      "Cbc0038I Pass  17: suminf.    0.47708 (2) obj. 0.0463513 iterations 1\n",
      "Cbc0038I Pass  18: suminf.    0.57615 (4) obj. 0.0494642 iterations 5\n",
      "Cbc0038I Pass  19: suminf.    0.94265 (2) obj. 0.0494642 iterations 3\n",
      "Cbc0038I Pass  20: suminf.    0.45458 (2) obj. 0.0480443 iterations 1\n",
      "Cbc0038I Pass  21: suminf.    0.94265 (2) obj. 0.0494642 iterations 1\n",
      "Cbc0038I Pass  22: suminf.    0.57636 (2) obj. 0.0494642 iterations 1\n",
      "Cbc0038I Pass  23: suminf.    0.38708 (2) obj. 0.0489136 iterations 1\n",
      "Cbc0038I Pass  24: suminf.    0.57636 (2) obj. 0.0494642 iterations 1\n",
      "Cbc0038I Pass  25: suminf.    0.80671 (4) obj. 0.0494642 iterations 2\n",
      "Cbc0038I Pass  26: suminf.    0.49626 (2) obj. 0.0494642 iterations 4\n",
      "Cbc0038I Pass  27: suminf.    0.45290 (2) obj. 0.0494642 iterations 4\n",
      "Cbc0038I Pass  28: suminf.    0.45290 (2) obj. 0.0494642 iterations 0\n",
      "Cbc0038I Pass  29: suminf.    0.47708 (2) obj. 0.0463513 iterations 1\n",
      "Cbc0038I Pass  30: suminf.    1.31103 (4) obj. 0.0494642 iterations 2\n",
      "Cbc0038I Pass  31: suminf.    1.01196 (4) obj. 0.0494642 iterations 1\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 141 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 9 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.0498835 to 0.0488804 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.0482524\n",
      "Cbc0038I Reduced cost fixing fixed 98 variables on major pass 3\n",
      "Cbc0038I Pass  31: suminf.    0.58958 (2) obj. 0.0457804 iterations 0\n",
      "Cbc0038I Pass  32: suminf.    0.56069 (2) obj. 0.0482524 iterations 3\n",
      "Cbc0038I Pass  33: suminf.    0.56069 (2) obj. 0.0482524 iterations 7\n",
      "Cbc0038I Pass  34: suminf.    0.56069 (2) obj. 0.0482524 iterations 0\n",
      "Cbc0038I Pass  35: suminf.    0.56069 (2) obj. 0.0482524 iterations 1\n",
      "Cbc0038I Pass  36: suminf.    0.58958 (2) obj. 0.0457804 iterations 1\n",
      "Cbc0038I Pass  37: suminf.    0.75553 (2) obj. 0.0482524 iterations 4\n",
      "Cbc0038I Pass  38: suminf.    0.57271 (2) obj. 0.0477205 iterations 1\n",
      "Cbc0038I Pass  39: suminf.    0.75553 (2) obj. 0.0482524 iterations 1\n",
      "Cbc0038I Pass  40: suminf.    1.02747 (4) obj. 0.0482524 iterations 5\n",
      "Cbc0038I Pass  41: suminf.    1.02747 (4) obj. 0.0482524 iterations 6\n",
      "Cbc0038I Pass  42: suminf.    0.58958 (2) obj. 0.0457804 iterations 2\n",
      "Cbc0038I Pass  43: suminf.    0.56069 (2) obj. 0.0482524 iterations 1\n",
      "Cbc0038I Pass  44: suminf.    0.75553 (2) obj. 0.0482524 iterations 4\n",
      "Cbc0038I Pass  45: suminf.    0.57271 (2) obj. 0.0477205 iterations 1\n",
      "Cbc0038I Pass  46: suminf.    0.75553 (2) obj. 0.0482524 iterations 1\n",
      "Cbc0038I Pass  47: suminf.    0.98845 (4) obj. 0.0482524 iterations 4\n",
      "Cbc0038I Pass  48: suminf.    1.20062 (4) obj. 0.0482524 iterations 1\n",
      "Cbc0038I Pass  49: suminf.    1.20062 (4) obj. 0.0482524 iterations 0\n",
      "Cbc0038I Pass  50: suminf.    0.58958 (2) obj. 0.0457804 iterations 2\n",
      "Cbc0038I Pass  51: suminf.    0.56069 (2) obj. 0.0482524 iterations 1\n",
      "Cbc0038I Pass  52: suminf.    0.56069 (2) obj. 0.0482524 iterations 0\n",
      "Cbc0038I Pass  53: suminf.    0.56069 (2) obj. 0.0482524 iterations 0\n",
      "Cbc0038I Pass  54: suminf.    0.86945 (2) obj. 0.0482524 iterations 1\n",
      "Cbc0038I Pass  55: suminf.    0.86945 (2) obj. 0.0482524 iterations 0\n",
      "Cbc0038I Pass  56: suminf.    0.47708 (2) obj. 0.0463513 iterations 1\n",
      "Cbc0038I Pass  57: suminf.    0.47644 (4) obj. 0.0482524 iterations 1\n",
      "Cbc0038I Pass  58: suminf.    0.47644 (4) obj. 0.0482524 iterations 0\n",
      "Cbc0038I Pass  59: suminf.    0.58310 (4) obj. 0.0482524 iterations 3\n",
      "Cbc0038I Pass  60: suminf.    0.47644 (4) obj. 0.0482524 iterations 1\n",
      "Cbc0038I Rounding solution of 0.0477973 is better than previous of 0.0488804\n",
      "\n",
      "Cbc0038I Before mini branch and bound, 145 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 6 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.0471852\n",
      "Cbc0038I Reduced cost fixing fixed 102 variables on major pass 4\n",
      "Cbc0038I Pass  60: suminf.    0.58958 (2) obj. 0.0457804 iterations 0\n",
      "Cbc0038I Pass  61: suminf.    0.92752 (2) obj. 0.0471852 iterations 3\n",
      "Cbc0038I Pass  62: suminf.    0.70619 (2) obj. 0.0471852 iterations 5\n",
      "Cbc0038I Pass  63: suminf.    0.52208 (2) obj. 0.0466496 iterations 1\n",
      "Cbc0038I Pass  64: suminf.    0.70619 (2) obj. 0.0471852 iterations 1\n",
      "Cbc0038I Pass  65: suminf.    0.54039 (4) obj. 0.0471852 iterations 6\n",
      "Cbc0038I Pass  66: suminf.    0.49368 (4) obj. 0.0471852 iterations 11\n",
      "Cbc0038I Pass  67: suminf.    0.76372 (2) obj. 0.0471852 iterations 2\n",
      "Cbc0038I Pass  68: suminf.    0.47708 (2) obj. 0.0463513 iterations 1\n",
      "Cbc0038I Pass  69: suminf.    0.65759 (2) obj. 0.0471852 iterations 3\n",
      "Cbc0038I Pass  70: suminf.    0.47708 (2) obj. 0.0463513 iterations 2\n",
      "Cbc0038I Pass  71: suminf.    0.97160 (4) obj. 0.0471852 iterations 3\n",
      "Cbc0038I Pass  72: suminf.    0.47708 (2) obj. 0.0465753 iterations 4\n",
      "Cbc0038I Pass  73: suminf.    0.65759 (2) obj. 0.0471852 iterations 1\n",
      "Cbc0038I Pass  74: suminf.    0.92344 (2) obj. 0.0471852 iterations 2\n",
      "Cbc0038I Pass  75: suminf.    0.58958 (2) obj. 0.0457804 iterations 2\n",
      "Cbc0038I Pass  76: suminf.    0.92752 (2) obj. 0.0471852 iterations 1\n",
      "Cbc0038I Pass  77: suminf.    0.92752 (2) obj. 0.0471852 iterations 0\n",
      "Cbc0038I Pass  78: suminf.    0.92752 (2) obj. 0.0471852 iterations 0\n",
      "Cbc0038I Pass  79: suminf.    0.92752 (2) obj. 0.0471852 iterations 1\n",
      "Cbc0038I Pass  80: suminf.    0.58958 (2) obj. 0.0457804 iterations 2\n",
      "Cbc0038I Pass  81: suminf.    0.92752 (2) obj. 0.0471852 iterations 2\n",
      "Cbc0038I Pass  82: suminf.    0.52208 (2) obj. 0.0466496 iterations 2\n",
      "Cbc0038I Pass  83: suminf.    0.52208 (2) obj. 0.0466496 iterations 0\n",
      "Cbc0038I Pass  84: suminf.    0.70619 (2) obj. 0.0471852 iterations 1\n",
      "Cbc0038I Pass  85: suminf.    0.60805 (2) obj. 0.0471852 iterations 1\n",
      "Cbc0038I Pass  86: suminf.    0.70619 (2) obj. 0.0471852 iterations 1\n",
      "Cbc0038I Pass  87: suminf.    0.70619 (2) obj. 0.0471852 iterations 0\n",
      "Cbc0038I Pass  88: suminf.    0.52208 (2) obj. 0.0466496 iterations 1\n",
      "Cbc0038I Pass  89: suminf.    0.52208 (2) obj. 0.0466496 iterations 0\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 151 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 4 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.01 seconds)\n",
      "Cbc0038I After 0.01 seconds - Feasibility pump exiting with objective of 0.0477973 - took 0.01 seconds\n",
      "Cbc0012I Integer solution of 0.047797272 found by feasibility pump after 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 2 columns\n",
      "Cbc0006I The LP relaxation is infeasible or too expensive\n",
      "Cbc0013I At root node, 0 cuts changed objective from 0.045780359 to 0.045780359 in 1 passes\n",
      "Cbc0014I Cut generator 0 (Probing) - 1 row cuts average 0.0 elements, 3 column cuts (3 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 1 (Gomory) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 2 (Knapsack) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 3 (Clique) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 4 (MixedIntegerRounding2) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 5 (FlowCover) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 6 (TwoMirCuts) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 7 (ZeroHalf) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0001I Search completed - best objective 0.04779727228720419, took 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0035I Maximum depth 0, 99 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from 0.0457804 to 0.0457804\n",
      "Probing was tried 1 times and created 4 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                0.04779727\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               0\n",
      "Time (CPU seconds):             0.01\n",
      "Time (Wallclock seconds):       0.01\n",
      "\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.01   (Wallclock seconds):       0.01\n",
      "\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpymthg94f/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpymthg94f/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Keras_mixed_precision End -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 07:38:17.750681: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-11-06 07:38:17.750717: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-11-06 07:38:17.750855: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpymthg94f\n",
      "2025-11-06 07:38:17.756046: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-11-06 07:38:17.756062: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpymthg94f\n",
      "2025-11-06 07:38:17.771202: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-11-06 07:38:17.863602: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpymthg94f\n",
      "2025-11-06 07:38:17.899600: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 148745 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Method 2: PTQ with Mixed Precision Quantization\n",
    "# - Uses different bit-widths for different layers based on sensitivity analysis\n",
    "flag, quantized_model2 = PTQ_Keras_mixed_precision(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b952197-11c2-4373-842e-ce3b264ca5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Keras Start ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Statistics Collection: 5it [00:13,  2.75s/it]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 105/105 [00:09<00:00, 10.81it/s]\n",
      "Estimating representative dataset size: 5it [00:00, 31.81it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 100/100 [19:41<00:00, 11.82s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 100/100 [19:35<00:00, 11.76s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 100/100 [12:48<00:00,  7.68s/it]\n",
      "Running GPTQ optimization:   0%|                                                      | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:05<00:20,  5.01s/it]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:05<00:07,  2.36s/it]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:06<00:03,  1.51s/it]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:06<00:01,  1.12s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.11it/s]\u001b[A\n",
      "Running GPTQ optimization:  20%|█████████▏                                    | 1/5 [00:10<00:41, 10.27s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:02,  1.69it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:01<00:01,  1.83it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:01<00:01,  1.81it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:02<00:00,  1.80it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.85it/s]\u001b[A\n",
      "Running GPTQ optimization:  40%|██████████████████▍                           | 2/5 [00:13<00:17,  5.85s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:02,  1.73it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:01<00:01,  1.75it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:01<00:01,  1.76it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:02<00:00,  1.83it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.89it/s]\u001b[A\n",
      "Running GPTQ optimization:  60%|███████████████████████████▌                  | 3/5 [00:15<00:08,  4.42s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:02,  1.78it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:01<00:01,  1.89it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:01<00:01,  1.88it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:02<00:00,  1.91it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.87it/s]\u001b[A\n",
      "Running GPTQ optimization:  80%|████████████████████████████████████▊         | 4/5 [00:18<00:03,  3.73s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:02,  1.85it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:01<00:01,  1.92it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:01<00:01,  1.90it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:02<00:00,  1.93it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.93it/s]\u001b[A\n",
      "Running GPTQ optimization: 100%|██████████████████████████████████████████████| 5/5 [00:21<00:00,  4.21s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpc4davibj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpc4davibj/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Keras End -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 08:31:19.418708: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-11-06 08:31:19.418748: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-11-06 08:31:19.418879: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpc4davibj\n",
      "2025-11-06 08:31:19.424929: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-11-06 08:31:19.424948: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpc4davibj\n",
      "2025-11-06 08:31:19.441242: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-11-06 08:31:19.538790: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpc4davibj\n",
      "2025-11-06 08:31:19.578268: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 159390 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Gradient-based Post-Training Quantization (GPTQ)\n",
    "# - Uses gradient information to fine-tune quantization parameters during conversion\n",
    "flag, quantized_model3 = GPTQ_Keras(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "075a2012-9423-4eaa-84c7-365a5daafa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Keras_mixed_precision Start ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Statistics Collection: 5it [00:13,  2.76s/it]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 105/105 [00:28<00:00,  3.70it/s]\n",
      "53it [00:22,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/ubuntu/.local/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/ce5f1a4cdda34ce2ad0392304eca0326-pulp.mps -sec 60 -timeMode elapsed -branch -printingOptions all -solution /tmp/ce5f1a4cdda34ce2ad0392304eca0326-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 112 COLUMNS\n",
      "At line 961 RHS\n",
      "At line 1069 BOUNDS\n",
      "At line 1229 ENDATA\n",
      "Problem MODEL has 107 rows, 212 columns and 477 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "seconds was changed from 1e+100 to 60\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 0.0603818 - 0.00 seconds\n",
      "Cgl0004I processed model has 54 rows, 159 columns (159 integer (159 of which binary)) and 265 elements\n",
      "Cbc0038I Initial state - 2 integers unsatisfied sum - 0.589583\n",
      "Cbc0038I Pass   1: suminf.    0.14150 (2) obj. 0.0800306 iterations 2\n",
      "Cbc0038I Solution found of 0.372231\n",
      "Cbc0038I Before mini branch and bound, 155 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 2 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.372231 to 0.0665973 (0.00 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.0659668\n",
      "Cbc0038I Reduced cost fixing fixed 93 variables on major pass 2\n",
      "Cbc0038I Pass   2: suminf.    0.58958 (2) obj. 0.0603818 iterations 3\n",
      "Cbc0038I Pass   3: suminf.    0.14308 (2) obj. 0.0659668 iterations 3\n",
      "Cbc0038I Pass   4: suminf.    0.14308 (2) obj. 0.0659668 iterations 4\n",
      "Cbc0038I Pass   5: suminf.    0.14308 (2) obj. 0.0659668 iterations 0\n",
      "Cbc0038I Pass   6: suminf.    0.14308 (2) obj. 0.0659668 iterations 6\n",
      "Cbc0038I Pass   7: suminf.    0.14308 (2) obj. 0.0659668 iterations 4\n",
      "Cbc0038I Pass   8: suminf.    0.36295 (2) obj. 0.0659668 iterations 3\n",
      "Cbc0038I Pass   9: suminf.    0.36295 (2) obj. 0.0659668 iterations 0\n",
      "Cbc0038I Pass  10: suminf.    0.47708 (2) obj. 0.0608549 iterations 1\n",
      "Cbc0038I Pass  11: suminf.    0.14021 (2) obj. 0.0643372 iterations 3\n",
      "Cbc0038I Pass  12: suminf.    0.14021 (2) obj. 0.0643372 iterations 0\n",
      "Cbc0038I Pass  13: suminf.    0.50998 (2) obj. 0.0659668 iterations 1\n",
      "Cbc0038I Pass  14: suminf.    0.56899 (4) obj. 0.0659668 iterations 4\n",
      "Cbc0038I Pass  15: suminf.    0.56899 (4) obj. 0.0659668 iterations 3\n",
      "Cbc0038I Pass  16: suminf.    0.99600 (2) obj. 0.0659668 iterations 2\n",
      "Cbc0038I Pass  17: suminf.    0.15708 (2) obj. 0.0622697 iterations 1\n",
      "Cbc0038I Pass  18: suminf.    0.58785 (4) obj. 0.0659668 iterations 5\n",
      "Cbc0038I Pass  19: suminf.    0.96966 (2) obj. 0.0659668 iterations 4\n",
      "Cbc0038I Pass  20: suminf.    0.45458 (2) obj. 0.0636969 iterations 1\n",
      "Cbc0038I Pass  21: suminf.    0.96966 (2) obj. 0.0659668 iterations 1\n",
      "Cbc0038I Pass  22: suminf.    0.59138 (2) obj. 0.0659668 iterations 2\n",
      "Cbc0038I Pass  23: suminf.    0.38708 (2) obj. 0.0638861 iterations 2\n",
      "Cbc0038I Pass  24: suminf.    0.85923 (2) obj. 0.0659668 iterations 1\n",
      "Cbc0038I Pass  25: suminf.    0.52008 (4) obj. 0.0659668 iterations 2\n",
      "Cbc0038I Pass  26: suminf.    0.42637 (4) obj. 0.0659668 iterations 1\n",
      "Cbc0038I Pass  27: suminf.    0.61991 (2) obj. 0.0659668 iterations 2\n",
      "Cbc0038I Pass  28: suminf.    0.08958 (2) obj. 0.0636297 iterations 1\n",
      "Cbc0038I Pass  29: suminf.    0.55990 (2) obj. 0.0640257 iterations 2\n",
      "Cbc0038I Pass  30: suminf.    0.08958 (2) obj. 0.0636297 iterations 1\n",
      "Cbc0038I Pass  31: suminf.    0.09375 (2) obj. 0.0659668 iterations 7\n",
      "Cbc0038I Rounding solution of 0.0657233 is better than previous of 0.0665973\n",
      "\n",
      "Cbc0038I Before mini branch and bound, 137 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 11 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.0657233 to 0.0635836 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.0629352\n",
      "Cbc0038I Reduced cost fixing fixed 98 variables on major pass 3\n",
      "Cbc0038I Pass  31: suminf.    0.58958 (2) obj. 0.0603818 iterations 0\n",
      "Cbc0038I Pass  32: suminf.    0.83099 (2) obj. 0.0629352 iterations 3\n",
      "Cbc0038I Pass  33: suminf.    0.83099 (2) obj. 0.0629352 iterations 8\n",
      "Cbc0038I Pass  34: suminf.    0.83099 (2) obj. 0.0629352 iterations 0\n",
      "Cbc0038I Pass  35: suminf.    0.94142 (2) obj. 0.0629352 iterations 9\n",
      "Cbc0038I Pass  36: suminf.    0.94142 (2) obj. 0.0629352 iterations 0\n",
      "Cbc0038I Pass  37: suminf.    0.52208 (2) obj. 0.0605709 iterations 1\n",
      "Cbc0038I Pass  38: suminf.    0.52208 (2) obj. 0.060808 iterations 2\n",
      "Cbc0038I Pass  39: suminf.    0.52208 (2) obj. 0.0605709 iterations 1\n",
      "Cbc0038I Pass  40: suminf.    0.40958 (2) obj. 0.0610441 iterations 2\n",
      "Cbc0038I Pass  41: suminf.    0.40958 (2) obj. 0.0610441 iterations 0\n",
      "Cbc0038I Pass  42: suminf.    0.83871 (2) obj. 0.0629352 iterations 1\n",
      "Cbc0038I Pass  43: suminf.    0.94142 (2) obj. 0.0629352 iterations 4\n",
      "Cbc0038I Pass  44: suminf.    0.94142 (2) obj. 0.0629352 iterations 0\n",
      "Cbc0038I Pass  45: suminf.    0.94142 (2) obj. 0.0629352 iterations 2\n",
      "Cbc0038I Pass  46: suminf.    0.94142 (2) obj. 0.0629352 iterations 0\n",
      "Cbc0038I Pass  47: suminf.    0.52208 (2) obj. 0.0605709 iterations 1\n",
      "Cbc0038I Pass  48: suminf.    0.63912 (4) obj. 0.0629352 iterations 4\n",
      "Cbc0038I Pass  49: suminf.    0.50958 (2) obj. 0.0628556 iterations 3\n",
      "Cbc0038I Pass  50: suminf.    0.52765 (2) obj. 0.0629352 iterations 1\n",
      "Cbc0038I Pass  51: suminf.    0.50958 (2) obj. 0.0628556 iterations 1\n",
      "Cbc0038I Pass  52: suminf.    0.57271 (2) obj. 0.0624492 iterations 4\n",
      "Cbc0038I Pass  53: suminf.    0.57271 (2) obj. 0.0624492 iterations 0\n",
      "Cbc0038I Pass  54: suminf.    0.68299 (2) obj. 0.0629352 iterations 1\n",
      "Cbc0038I Pass  55: suminf.    0.46312 (2) obj. 0.0629352 iterations 1\n",
      "Cbc0038I Pass  56: suminf.    0.46021 (2) obj. 0.0629224 iterations 1\n",
      "Cbc0038I Pass  57: suminf.    0.46312 (2) obj. 0.0629352 iterations 1\n",
      "Cbc0038I Pass  58: suminf.    0.46021 (2) obj. 0.0629224 iterations 1\n",
      "Cbc0038I Pass  59: suminf.    0.57271 (2) obj. 0.0627093 iterations 5\n",
      "Cbc0038I Pass  60: suminf.    0.50521 (2) obj. 0.0628679 iterations 2\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 147 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 6 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.01 seconds)\n",
      "Cbc0038I After 0.01 seconds - Feasibility pump exiting with objective of 0.0635836 - took 0.01 seconds\n",
      "Cbc0012I Integer solution of 0.063583604 found by feasibility pump after 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 3 columns\n",
      "Cbc0006I The LP relaxation is infeasible or too expensive\n",
      "Cbc0013I At root node, 0 cuts changed objective from 0.06038176 to 0.06038176 in 1 passes\n",
      "Cbc0014I Cut generator 0 (Probing) - 1 row cuts average 0.0 elements, 4 column cuts (4 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 1 (Gomory) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 2 (Knapsack) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 3 (Clique) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 4 (MixedIntegerRounding2) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 5 (FlowCover) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 6 (TwoMirCuts) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 7 (ZeroHalf) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0001I Search completed - best objective 0.06358360399781421, took 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0035I Maximum depth 0, 95 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from 0.0603818 to 0.0603818\n",
      "Probing was tried 1 times and created 5 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                0.06358360\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               0\n",
      "Time (CPU seconds):             0.01\n",
      "Time (Wallclock seconds):       0.01\n",
      "\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.01   (Wallclock seconds):       0.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 100/100 [20:31<00:00, 12.31s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 100/100 [20:20<00:00, 12.21s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 100/100 [12:37<00:00,  7.57s/it]\n",
      "Running GPTQ optimization:   0%|                                                      | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:05<00:22,  5.54s/it]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:06<00:07,  2.60s/it]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:06<00:03,  1.65s/it]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:07<00:01,  1.21s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.04it/s]\u001b[A\n",
      "Running GPTQ optimization:  20%|█████████▏                                    | 1/5 [00:07<00:30,  7.67s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:02,  1.76it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:01<00:01,  1.86it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:01<00:01,  1.91it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:02<00:00,  1.94it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.94it/s]\u001b[A\n",
      "Running GPTQ optimization:  40%|██████████████████▍                           | 2/5 [00:10<00:14,  4.70s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:02,  1.87it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:01<00:01,  1.92it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:01<00:01,  1.92it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:02<00:00,  1.93it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.92it/s]\u001b[A\n",
      "Running GPTQ optimization:  60%|███████████████████████████▌                  | 3/5 [00:12<00:07,  3.75s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:02,  1.85it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:01<00:01,  1.88it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:01<00:01,  1.91it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:02<00:00,  1.93it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.94it/s]\u001b[A\n",
      "Running GPTQ optimization:  80%|████████████████████████████████████▊         | 4/5 [00:15<00:03,  3.30s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:02,  1.86it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:01<00:01,  1.94it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:01<00:01,  1.96it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:02<00:00,  1.97it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.97it/s]\u001b[A\n",
      "Running GPTQ optimization: 100%|██████████████████████████████████████████████| 5/5 [00:18<00:00,  3.61s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpcjk7c1iv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpcjk7c1iv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Keras_mixed_precision End -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 09:26:25.535563: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-11-06 09:26:25.535641: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-11-06 09:26:25.536032: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpcjk7c1iv\n",
      "2025-11-06 09:26:25.541989: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-11-06 09:26:25.542008: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpcjk7c1iv\n",
      "2025-11-06 09:26:25.558172: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-11-06 09:26:25.656187: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpcjk7c1iv\n",
      "2025-11-06 09:26:25.691559: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 155493 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Method 4: GPTQ with Mixed Precision Quantization\n",
    "# - Combines gradient-based optimization with mixed precision techniques\n",
    "flag, quantized_model4 = GPTQ_Keras_mixed_precision(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19453943-75ba-4707-bef9-e137ccb91094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All quantization methods completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"All quantization methods completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b34232",
   "metadata": {},
   "source": [
    "## Models evaluation\n",
    "In order to evaluate our models, we first need to load the validation dataset. As before, please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ce90826-e5c5-4be0-8b54-96795eab47eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model evaluation phase...\n",
      "Found 50000 files belonging to 1000 classes.\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation and Accuracy Comparison\n",
    "print(\"Starting model evaluation phase...\")\n",
    "\n",
    "# Prepare validation dataset for accuracy assessment\n",
    "val_dataset = get_dataset(batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e01931d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Original Model Evaluation ===\n",
      "1000/1000 [==============================] - 167s 166ms/step - loss: 1.2200 - accuracy: 0.7185\n",
      "Float model's Top 1 accuracy on the Imagenet validation set: 71.85%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate original floating-point model accuracy\n",
    "print(\"\\n=== Original Model Evaluation ===\")\n",
    "float_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "float_accuracy = float_model.evaluate(val_dataset)\n",
    "print(f\"Float model's Top 1 accuracy on the Imagenet validation set: {(float_accuracy[1] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88cabfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PTQ Model Evaluation ===\n",
      "1000/1000 [==============================] - 156s 156ms/step - loss: 1.3538 - accuracy: 0.7164\n",
      "PTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: 71.64%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate PTQ quantized model accuracy\n",
    "print(\"\\n=== PTQ Model Evaluation ===\")\n",
    "quantized_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model.evaluate(val_dataset)\n",
    "print(f\"PTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47a42b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PTQ + Mixed Precision Model Evaluation ===\n",
      "1000/1000 [==============================] - 158s 157ms/step - loss: 1.3545 - accuracy: 0.7159\n",
      "PTQ_Keras_mixed_precision Quantized model's Top 1 accuracy on the Imagenet validation set: 71.59%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate PTQ + Mixed Precision model accuracy\n",
    "print(\"\\n=== PTQ + Mixed Precision Model Evaluation ===\")\n",
    "quantized_model2.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model2.evaluate(val_dataset)\n",
    "print(f\"PTQ_Keras_mixed_precision Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aad8438d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPTQ Model Evaluation ===\n",
      "1000/1000 [==============================] - 158s 156ms/step - loss: 1.3649 - accuracy: 0.7137\n",
      "GPTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: 71.37%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate GPTQ quantized model accuracy\n",
    "print(\"\\n=== GPTQ Model Evaluation ===\")\n",
    "quantized_model3.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model3.evaluate(val_dataset)\n",
    "print(f\"GPTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46335127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPTQ + Mixed Precision Model Evaluation ===\n",
      "1000/1000 [==============================] - 158s 157ms/step - loss: 1.3859 - accuracy: 0.7107\n",
      "GPTQ_Keras_mixed_precision Quantized model's Top 1 accuracy on the Imagenet validation set: 71.07%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate GPTQ + Mixed Precision model accuracy\n",
    "print(\"\\n=== GPTQ + Mixed Precision Model Evaluation ===\")\n",
    "quantized_model4.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model4.evaluate(val_dataset)\n",
    "print(f\"GPTQ_Keras_mixed_precision Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fisish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71f63d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCTWrapper with a few lines of code.\n",
    "\n",
    "MCT can deliver competitive results across a wide range of tasks and network architectures. For more details, [check out the paper:](https://arxiv.org/abs/2109.09113).\n",
    "\n",
    "## Copyrights\n",
    "\n",
    "Copyright 2025 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

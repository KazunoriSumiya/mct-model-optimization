{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ca09f9",
   "metadata": {},
   "source": [
    "# Model Compression Toolkit (MCT) Wrapper API Comprehensive Quantization Comparison(tensorflow)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/SonySemiconductorSolutions/mct-model-optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_mct_wrapper.ipynb)\n",
    "\n",
    "## Overview \n",
    "This notebook provides a comprehensive demonstration of the MCT (Model Compression Toolkit) Wrapper API functionality, showcasing five different quantization methods on a MobileNetV2 model. The tutorial systematically compares the implementation, performance characteristics, and accuracy trade-offs of each quantization approach: PTQ (Post-Training Quantization), PTQ with Mixed Precision, GPTQ (Gradient-based PTQ), GPTQ with Mixed Precision, and LQ-PTQ (Low-bit Quantizer PTQ). Each method utilizes the unified MCTWrapper interface for consistent implementation and comparison.\n",
    "\n",
    "## Summary\n",
    "1. **Environment Setup**: Import required libraries and configure MCT with MobileNetV2 model\n",
    "2. **Dataset Preparation**: Load and prepare ImageNet validation dataset with representative data generation\n",
    "3. **PTQ Implementation**: Execute basic Post-Training Quantization with 8-bit precision and bias correction\n",
    "4. **PTQ + Mixed Precision**: Apply intelligent bit-width allocation based on layer sensitivity analysis (75% compression ratio)\n",
    "5. **GPTQ Implementation**: Perform gradient-based optimization with 5-epoch fine-tuning for enhanced accuracy\n",
    "6. **GPTQ + Mixed Precision**: Combine gradient optimization with mixed precision for optimal accuracy-compression trade-off\n",
    "7. **Performance Evaluation**: Comprehensive accuracy assessment and comparison across all quantization methods\n",
    "8. **Results Analysis**: Compare model sizes, inference accuracy, and quantization trade-offs\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cea7170-2624-4139-a659-8e27caf5ad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 08:46:04.196917: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-31 08:46:04.235814: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-31 08:46:04.419130: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-31 08:46:04.419162: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-31 08:46:04.420457: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-31 08:46:04.522363: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-31 08:46:04.523710: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-31 08:46:05.385588: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for deep learning and file handling\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2 \n",
    "from pathlib import Path\n",
    "from typing import Callable, Generator, List, Tuple, Any\n",
    "\n",
    "# Alternative pip install commands (commented out for local development)\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e138ad45-68d2-4c34-95ac-1a327ce54406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MCT core\n",
    "#import importlib\n",
    "#if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "#    !pip install model_compression_toolkit\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/wrapper/sonyfork/mct-model-optimization')\n",
    "\n",
    "import model_compression_toolkit as mct\n",
    "from model_compression_toolkit.core import QuantizationErrorMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce48a1",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f14d92-5116-40b7-9090-f378bdf3cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and setup ImageNet validation dataset if not already present\n",
    "if not os.path.isdir('imagenet'):\n",
    "    # Create directory and download required ImageNet files\n",
    "    os.system('mkdir imagenet')\n",
    "    os.system('wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz')\n",
    "    os.system('wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar')\n",
    "\n",
    "    # Move downloaded files to imagenet directory\n",
    "    os.system('mv ILSVRC2012_devkit_t12.tar.gz imagenet/')\n",
    "    os.system('mv ILSVRC2012_img_val.tar imagenet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e473150",
   "metadata": {
    "lines_to_next_cell": 1,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup ImageNet validation directory structure if not exists\n",
    "# This creates the directory structure expected by TensorFlow's image_dataset_from_directory\n",
    "# Check if ImageNet validation directory already exists\n",
    "if not os.path.isdir('imagenet/val'):\n",
    "    import subprocess\n",
    "    \n",
    "    # Clone MCT repository temporarily to access setup scripts\n",
    "    # This provides access to ImageNet data preparation utilities\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/sony/model_optimization.git', 'temp_mct'])\n",
    "    \n",
    "    # Make ImageNet preparation script executable with proper permissions\n",
    "    os.system('chmod +x ../../../resources/scripts/prepare_imagenet.sh')\n",
    "\n",
    "    # Run the preparation script to organize ImageNet data into proper directory structure\n",
    "    # This script handles data extraction and organization for TensorFlow compatibility\n",
    "    subprocess.run(['../../../resources/scripts/prepare_imagenet.sh'])\n",
    "\n",
    "def imagenet_preprocess_input(images: tf.Tensor, labels: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Apply MobileNetV2-specific preprocessing to input images.\n",
    "    \n",
    "    This function normalizes pixel values according to MobileNetV2 requirements,\n",
    "    ensuring consistent input format for the model.\n",
    "    \n",
    "    Args:\n",
    "        images: Input image tensor\n",
    "        labels: Corresponding label tensor\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of preprocessed images and unchanged labels\n",
    "    \"\"\"\n",
    "    return tf.keras.applications.mobilenet_v2.preprocess_input(images), labels\n",
    "\n",
    "def get_dataset(batch_size: int, shuffle: bool):\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory='./imagenet/val',\n",
    "        batch_size=batch_size,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=shuffle,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear'\n",
    "    )\n",
    "    dataset = dataset.map(lambda x, y: imagenet_preprocess_input(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5897999a-781d-472f-a494-2aabd758e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 1000 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 08:46:14.058074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-10-31 08:46:14.063086: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters for representative dataset generation\n",
    "# These parameters control the calibration dataset used for quantization\n",
    "batch_size = 5  # Number of images per batch for quantization calibration\n",
    "n_iter = 2      # Number of iterations to generate representative data\n",
    "                # Total calibration samples = batch_size * n_iter = 10 images\n",
    "\n",
    "# Create dataset instance for representative data generation\n",
    "# Use shuffled data to ensure diverse representative samples\n",
    "dataset = get_dataset(batch_size, shuffle=True)\n",
    "\n",
    "# Generator function for representative dataset used in quantization calibration\n",
    "def representative_dataset_gen():\n",
    "    \"\"\"\n",
    "    Generator function for representative dataset used in quantization calibration.\n",
    "\n",
    "    This function provides a small subset of data that MCT uses for:\n",
    "    - Calibrating quantization parameters across all model layers\n",
    "    - Determining optimal activation value ranges for each layer\n",
    "    - Computing quantization thresholds based on actual data distribution\n",
    "    - Minimizing quantization error through data-driven parameter selection\n",
    "    \n",
    "    Yields:\n",
    "        List containing numpy arrays of image batches in MCT-expected format\n",
    "    \"\"\"\n",
    "    for _ in range(n_iter):\n",
    "        # Extract one batch from the dataset and convert to numpy format\n",
    "       yield [dataset.take(1).get_single_element()[0].numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa8e319",
   "metadata": {},
   "source": [
    "## Model Post-Training quantization using MCTWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd8db92-9151-42d6-b3fa-311f7d528df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator to provide consistent logging and error handling for quantization functions\n",
    "def decorator(func):\n",
    "    \"\"\"\n",
    "    Wrapper decorator that provides standardized execution logging and error handling.\n",
    "    \n",
    "    This decorator enhances quantization functions by:\n",
    "    - Providing clear start/end execution markers for debugging\n",
    "    - Handling success/failure status from quantization operations\n",
    "    - Implementing fail-fast behavior on quantization errors\n",
    "    - Ensuring consistent logging format across all quantization methods\n",
    "    \n",
    "    Usage:\n",
    "        @decorator\n",
    "        def quantization_function(model):\n",
    "            # quantization implementation\n",
    "            return flag, quantized_model\n",
    "    \n",
    "    Args:\n",
    "        func: Function to be decorated (typically a quantization function)\n",
    "    \n",
    "    Returns:\n",
    "        Wrapped function with enhanced logging and error handling capabilities\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Log function execution start with clear delimiter\n",
    "        print(f\"----------------- {func.__name__} Start ---------------\")\n",
    "        \n",
    "        # Execute the quantization function and capture return values\n",
    "        # Expected return format: (success_flag, quantized_model)\n",
    "        flag, result = func(*args, **kwargs)\n",
    "        \n",
    "        # Log function execution completion\n",
    "        print(f\"----------------- {func.__name__} End -----------------\")\n",
    "        \n",
    "        # Implement fail-fast behavior: exit immediately on quantization failure\n",
    "        # This ensures early detection of quantization issues\n",
    "        if not flag:\n",
    "            exit()\n",
    "        \n",
    "        # Return original function results if successful\n",
    "        return flag, result\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf35f64",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ (Post-Training Quantization) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e151ddca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Keras(float_model):\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization (PTQ) using MCT on Keras model.\n",
    "    \n",
    "    PTQ is a quantization method that:\n",
    "    - Does not require model retraining\n",
    "    - Uses representative data for calibration\n",
    "    - Provides good accuracy with minimal computational overhead\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for basic PTQ quantization\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_internal_tpc = True                # Use MCT's built-in Target Platform Capabilities\n",
    "    use_mixed_precision = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PTQ\n",
    "    param_items = [\n",
    "        ['tpc_version', '1.0', 'The version of the TPC to use.'],\n",
    "        \n",
    "        # Quantization configuration parameters\n",
    "        ['activation_error_method', QuantizationErrorMethod.MSE, 'Error metric for activation quantization'],\n",
    "        ['weights_bias_correction', True, 'Enable bias correction for weights'],\n",
    "        ['z_threshold', float('inf'), 'Threshold for zero-point quantization'],\n",
    "        ['linear_collapsing', True, 'Enable linear layer collapsing optimization'],\n",
    "        ['residual_collapsing', True, 'Enable residual connection collapsing'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Keras.tflite', 'Path to save the quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_internal_tpc, use_mixed_precision, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f6eeb",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ + Mixed Precision Quantization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c082471a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Keras_mixed_precision(float_model):\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization with Mixed Precision (PTQ + mixed_precision) on Keras model.\n",
    "    \n",
    "    Mixed Precision Quantization:\n",
    "    - Uses different bit-widths for different layers\n",
    "    - Optimizes model size while maintaining accuracy\n",
    "    - Automatically selects optimal precision for each layer\n",
    "    - Uses resource constraints to guide precision allocation\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PTQ with mixed precision\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_internal_tpc = True                # Use MCT's built-in Target Platform Capabilities\n",
    "    use_mixed_precision = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        ['tpc_version', '1.0', 'The version of the TPC to use.'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based sensitivity scores for layer importance'],\n",
    "        \n",
    "        # Resource constraint configuration\n",
    "        ['weights_compression_ratio', 0.75, 'Target compression ratio for model weights (75% of original size)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Keras_mixed_precision.tflite', 'Path to save the mixed precision quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute mixed precision quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_internal_tpc, use_mixed_precision, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f2cba",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ (Gradient-based PTQ) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d82c0070",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Keras(float_model):\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization (GPTQ) on Keras model.\n",
    "    \n",
    "    GPTQ is an advanced quantization method that:\n",
    "    - Uses gradient information to optimize quantization parameters\n",
    "    - Fine-tunes the model during quantization process\n",
    "    - Generally provides better accuracy than standard PTQ\n",
    "    - Requires slightly more computational resources than PTQ\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for GPTQ quantization\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_internal_tpc = True                # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_mixed_precision = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for GPTQ\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Keras.tflite', 'Path to save the GPTQ quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute GPTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_internal_tpc, use_mixed_precision, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d77b5",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ + Mixed Precision Quantization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e06b4e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Keras_mixed_precision(float_model):\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization with Mixed Precision (GPTQ + mixed_precision).\n",
    "    \n",
    "    This combines the benefits of both techniques:\n",
    "    - GPTQ: Gradient-based optimization for better quantization accuracy\n",
    "    - Mixed Precision: Optimal bit-width allocation for size/accuracy trade-off\n",
    "    \n",
    "    This is the most advanced quantization method available, providing:\n",
    "    - Best possible accuracy preservation\n",
    "    - Optimal model size reduction\n",
    "    - Automatic precision selection per layer\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for GPTQ with mixed precision\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_internal_tpc = True                # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_mixed_precision = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for GPTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default)'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision sensitivity analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based scores for layer importance ranking'],\n",
    "        \n",
    "        # Resource constraint configuration\n",
    "        ['weights_compression_ratio', 0.75, 'Target compression ratio for model weights (75% reduction)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Keras_mixed_precision.tflite', 'Path to save the GPTQ+mixed_precision quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute advanced GPTQ+mixed_precision quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_internal_tpc, use_mixed_precision, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fc4ce",
   "metadata": {},
   "source": [
    "### Run model Post-Training Quantization\n",
    "Lastly, we quantize our model using MCTWrapper API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b959792-f339-43a9-9573-a95052b28e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting quantization experiments with different methods...\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained MobileNetV2 model as the base model for quantization experiments\n",
    "# This model serves as the reference floating-point model for all quantization methods\n",
    "float_model = MobileNetV2()\n",
    "\n",
    "# Execute comprehensive quantization method comparison using MCT Wrapper functionality\n",
    "# Each method represents different trade-offs between accuracy, model size, and computation time\n",
    "print(\"Starting quantization experiments with different methods...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bce5ebd0-aa83-4bb7-9e23-5ae3b95858dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Keras Start ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Statistics Collection: 2it [00:01,  1.18it/s]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 105/105 [00:09<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp7xdcy5yr/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp7xdcy5yr/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Keras End -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 08:46:32.517592: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-10-31 08:46:32.517625: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-10-31 08:46:32.518240: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp7xdcy5yr\n",
      "2025-10-31 08:46:32.525428: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-10-31 08:46:32.525442: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp7xdcy5yr\n",
      "2025-10-31 08:46:32.541575: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
      "2025-10-31 08:46:32.544377: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-10-31 08:46:32.646410: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp7xdcy5yr\n",
      "2025-10-31 08:46:32.681541: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 163301 microseconds.\n",
      "2025-10-31 08:46:32.738425: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Basic Post-Training Quantization (PTQ)\n",
    "# - Standard 8-bit quantization without advanced optimization techniquesed\n",
    "flag, quantized_model = PTQ_Keras(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9990c4d4-0ffe-4069-ba9b-6d14e9722edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Keras_mixed_precision Start ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Statistics Collection: 2it [00:01,  1.20it/s]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 105/105 [00:25<00:00,  4.13it/s]\n",
      "53it [00:22,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/ubuntu/.local/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/b5f1aa121e8d4d99961e81e2116922b0-pulp.mps -sec 60 -timeMode elapsed -branch -printingOptions all -solution /tmp/b5f1aa121e8d4d99961e81e2116922b0-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 112 COLUMNS\n",
      "At line 961 RHS\n",
      "At line 1069 BOUNDS\n",
      "At line 1229 ENDATA\n",
      "Problem MODEL has 107 rows, 212 columns and 477 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "seconds was changed from 1e+100 to 60\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 0.0383051 - 0.00 seconds\n",
      "Cgl0004I processed model has 54 rows, 159 columns (159 integer (159 of which binary)) and 265 elements\n",
      "Cbc0038I Initial state - 2 integers unsatisfied sum - 0.589583\n",
      "Cbc0038I Pass   1: suminf.    0.14150 (2) obj. 0.0527664 iterations 2\n",
      "Cbc0038I Solution found of 0.276364\n",
      "Cbc0038I Before mini branch and bound, 155 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 2 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.276364 to 0.0444358 (0.00 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.0438138\n",
      "Cbc0038I Reduced cost fixing fixed 92 variables on major pass 2\n",
      "Cbc0038I Pass   2: suminf.    0.58958 (2) obj. 0.0383051 iterations 3\n",
      "Cbc0038I Pass   3: suminf.    0.14311 (2) obj. 0.0438138 iterations 2\n",
      "Cbc0038I Pass   4: suminf.    0.26125 (2) obj. 0.0438138 iterations 5\n",
      "Cbc0038I Pass   5: suminf.    0.26125 (2) obj. 0.0438138 iterations 0\n",
      "Cbc0038I Pass   6: suminf.    0.26125 (2) obj. 0.0438138 iterations 6\n",
      "Cbc0038I Pass   7: suminf.    0.26125 (2) obj. 0.0438138 iterations 0\n",
      "Cbc0038I Pass   8: suminf.    0.26125 (2) obj. 0.0438138 iterations 1\n",
      "Cbc0038I Pass   9: suminf.    0.38647 (2) obj. 0.0438138 iterations 4\n",
      "Cbc0038I Pass  10: suminf.    0.38647 (2) obj. 0.0438138 iterations 0\n",
      "Cbc0038I Pass  11: suminf.    0.47708 (2) obj. 0.0383974 iterations 2\n",
      "Cbc0038I Pass  12: suminf.    0.27684 (2) obj. 0.0438138 iterations 1\n",
      "Cbc0038I Pass  13: suminf.    0.27684 (2) obj. 0.0438138 iterations 1\n",
      "Cbc0038I Pass  14: suminf.    0.27684 (2) obj. 0.0438138 iterations 0\n",
      "Cbc0038I Pass  15: suminf.    0.27684 (2) obj. 0.0438138 iterations 3\n",
      "Cbc0038I Pass  16: suminf.    0.29824 (4) obj. 0.0438138 iterations 6\n",
      "Cbc0038I Pass  17: suminf.    0.29824 (4) obj. 0.0438138 iterations 4\n",
      "Cbc0038I Pass  18: suminf.    0.90943 (2) obj. 0.0438138 iterations 2\n",
      "Cbc0038I Pass  19: suminf.    0.08958 (2) obj. 0.0394627 iterations 1\n",
      "Cbc0038I Pass  20: suminf.    0.00958 (2) obj. 0.0419291 iterations 1\n",
      "Cbc0038I Pass  21: suminf.    0.00958 (2) obj. 0.0419291 iterations 0\n",
      "Cbc0038I Pass  22: suminf.    0.44315 (2) obj. 0.0438138 iterations 1\n",
      "Cbc0038I Pass  23: suminf.    0.82193 (2) obj. 0.0438138 iterations 5\n",
      "Cbc0038I Pass  24: suminf.    0.82193 (2) obj. 0.0438138 iterations 0\n",
      "Cbc0038I Pass  25: suminf.    0.38708 (2) obj. 0.0400276 iterations 2\n",
      "Cbc0038I Pass  26: suminf.    0.74189 (2) obj. 0.0438138 iterations 1\n",
      "Cbc0038I Pass  27: suminf.    0.73146 (2) obj. 0.0438138 iterations 3\n",
      "Cbc0038I Pass  28: suminf.    0.41958 (2) obj. 0.0424581 iterations 1\n",
      "Cbc0038I Pass  29: suminf.    0.73146 (2) obj. 0.0438138 iterations 1\n",
      "Cbc0038I Pass  30: suminf.    0.94936 (4) obj. 0.0438138 iterations 1\n",
      "Cbc0038I Pass  31: suminf.    0.25916 (2) obj. 0.0438138 iterations 4\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 139 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 10 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.0444358 to 0.0431425 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.042167\n",
      "Cbc0038I Reduced cost fixing fixed 94 variables on major pass 3\n",
      "Cbc0038I Pass  31: suminf.    0.58958 (2) obj. 0.0383051 iterations 0\n",
      "Cbc0038I Pass  32: suminf.    0.52196 (2) obj. 0.042167 iterations 2\n",
      "Cbc0038I Pass  33: suminf.    0.52196 (2) obj. 0.042167 iterations 8\n",
      "Cbc0038I Pass  34: suminf.    0.52196 (2) obj. 0.042167 iterations 0\n",
      "Cbc0038I Pass  35: suminf.    0.01369 (2) obj. 0.042167 iterations 11\n",
      "Cbc0038I Solution found of 0.0421035\n",
      "Cbc0038I Before mini branch and bound, 151 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 4 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.0421035 to 0.041337 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.0404204\n",
      "Cbc0038I Reduced cost fixing fixed 98 variables on major pass 4\n",
      "Cbc0038I Pass  36: suminf.    0.58958 (2) obj. 0.0383051 iterations 0\n",
      "Cbc0038I Pass  37: suminf.    0.92377 (2) obj. 0.0404204 iterations 2\n",
      "Cbc0038I Pass  38: suminf.    0.92377 (2) obj. 0.0404204 iterations 4\n",
      "Cbc0038I Pass  39: suminf.    0.92377 (2) obj. 0.0404204 iterations 0\n",
      "Cbc0038I Pass  40: suminf.    0.92377 (2) obj. 0.0404204 iterations 5\n",
      "Cbc0038I Pass  41: suminf.    0.80088 (4) obj. 0.0404204 iterations 9\n",
      "Cbc0038I Pass  42: suminf.    0.62794 (4) obj. 0.0404204 iterations 7\n",
      "Cbc0038I Pass  43: suminf.    0.89990 (2) obj. 0.0404204 iterations 2\n",
      "Cbc0038I Pass  44: suminf.    0.52208 (2) obj. 0.0387782 iterations 1\n",
      "Cbc0038I Pass  45: suminf.    0.52208 (2) obj. 0.0387782 iterations 0\n",
      "Cbc0038I Pass  46: suminf.    0.52208 (2) obj. 0.0387782 iterations 0\n",
      "Cbc0038I Pass  47: suminf.    0.58958 (2) obj. 0.0384793 iterations 4\n",
      "Cbc0038I Pass  48: suminf.    0.58958 (2) obj. 0.0383051 iterations 1\n",
      "Cbc0038I Pass  49: suminf.    0.92377 (2) obj. 0.0404204 iterations 1\n",
      "Cbc0038I Pass  50: suminf.    0.92377 (2) obj. 0.0404204 iterations 1\n",
      "Cbc0038I Pass  51: suminf.    0.92377 (2) obj. 0.0404204 iterations 0\n",
      "Cbc0038I Pass  52: suminf.    0.92377 (2) obj. 0.0404204 iterations 0\n",
      "Cbc0038I Pass  53: suminf.    0.92377 (2) obj. 0.0404204 iterations 0\n",
      "Cbc0038I Pass  54: suminf.    0.88254 (2) obj. 0.0404204 iterations 2\n",
      "Cbc0038I Pass  55: suminf.    0.47708 (2) obj. 0.0383974 iterations 2\n",
      "Cbc0038I Pass  56: suminf.    0.94250 (2) obj. 0.0404204 iterations 1\n",
      "Cbc0038I Pass  57: suminf.    0.75666 (4) obj. 0.0404204 iterations 4\n",
      "Cbc0038I Pass  58: suminf.    0.62185 (4) obj. 0.0404204 iterations 1\n",
      "Cbc0038I Pass  59: suminf.    0.76617 (2) obj. 0.0404204 iterations 2\n",
      "Cbc0038I Pass  60: suminf.    0.40958 (2) obj. 0.0388704 iterations 1\n",
      "Cbc0038I Pass  61: suminf.    0.40958 (2) obj. 0.0388704 iterations 0\n",
      "Cbc0038I Pass  62: suminf.    0.40958 (2) obj. 0.0388704 iterations 0\n",
      "Cbc0038I Pass  63: suminf.    0.40958 (2) obj. 0.0389914 iterations 2\n",
      "Cbc0038I Pass  64: suminf.    0.83256 (4) obj. 0.0404204 iterations 3\n",
      "Cbc0038I Pass  65: suminf.    0.38708 (2) obj. 0.0400276 iterations 1\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 147 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 6 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.01 seconds)\n",
      "Cbc0038I After 0.01 seconds - Feasibility pump exiting with objective of 0.041337 - took 0.01 seconds\n",
      "Cbc0012I Integer solution of 0.041336994 found by feasibility pump after 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0012I Integer solution of 0.040541379 found by DiveCoefficient after 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 3 columns\n",
      "Cbc0006I The LP relaxation is infeasible or too expensive\n",
      "Cbc0013I At root node, 0 cuts changed objective from 0.038305131 to 0.038305131 in 1 passes\n",
      "Cbc0014I Cut generator 0 (Probing) - 1 row cuts average 0.0 elements, 2 column cuts (2 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 1 (Gomory) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 2 (Knapsack) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 3 (Clique) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 4 (MixedIntegerRounding2) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 5 (FlowCover) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 6 (TwoMirCuts) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 7 (ZeroHalf) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0001I Search completed - best objective 0.0405413786179438, took 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0035I Maximum depth 0, 98 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from 0.0383051 to 0.0383051\n",
      "Probing was tried 1 times and created 3 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                0.04054138\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               0\n",
      "Time (CPU seconds):             0.01\n",
      "Time (Wallclock seconds):       0.01\n",
      "\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.01   (Wallclock seconds):       0.01\n",
      "\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp8emk5ahg/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8emk5ahg/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Keras_mixed_precision End -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 08:47:30.017307: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-10-31 08:47:30.017473: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-10-31 08:47:30.017590: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp8emk5ahg\n",
      "2025-10-31 08:47:30.023289: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-10-31 08:47:30.023312: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp8emk5ahg\n",
      "2025-10-31 08:47:30.037463: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-10-31 08:47:30.124746: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp8emk5ahg\n",
      "2025-10-31 08:47:30.160757: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 143167 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Method 2: PTQ with Mixed Precision Quantization\n",
    "# - Uses different bit-widths for different layers based on sensitivity analysis\n",
    "flag, quantized_model2 = PTQ_Keras_mixed_precision(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b952197-11c2-4373-842e-ce3b264ca5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Keras Start ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Statistics Collection: 2it [00:01,  1.14it/s]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 105/105 [00:09<00:00, 11.65it/s]\n",
      "Estimating representative dataset size: 2it [00:00, 75.36it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 100/100 [09:25<00:00,  5.66s/it]\n",
      "Running GPTQ optimization:   0%|                                                      | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 1/2 [00:04<00:04,  4.56s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.02s/it]\u001b[A\n",
      "Running GPTQ optimization:  20%|█████████▏                                    | 1/5 [00:04<00:19,  4.80s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 1/2 [00:00<00:00,  3.50it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.78it/s]\u001b[A\n",
      "Running GPTQ optimization:  40%|██████████████████▍                           | 2/5 [00:05<00:06,  2.30s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 1/2 [00:00<00:00,  3.48it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.70it/s]\u001b[A\n",
      "Running GPTQ optimization:  60%|███████████████████████████▌                  | 3/5 [00:05<00:02,  1.50s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 1/2 [00:00<00:00,  3.43it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.77it/s]\u001b[A\n",
      "Running GPTQ optimization:  80%|████████████████████████████████████▊         | 4/5 [00:06<00:01,  1.12s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 1/2 [00:00<00:00,  3.62it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.00it/s]\u001b[A\n",
      "Running GPTQ optimization: 100%|██████████████████████████████████████████████| 5/5 [00:06<00:00,  1.39s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmphpy5r3ow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmphpy5r3ow/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Keras End -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 08:57:23.154546: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-10-31 08:57:23.154591: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-10-31 08:57:23.154717: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmphpy5r3ow\n",
      "2025-10-31 08:57:23.160674: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-10-31 08:57:23.160696: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmphpy5r3ow\n",
      "2025-10-31 08:57:23.176997: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-10-31 08:57:23.274922: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmphpy5r3ow\n",
      "2025-10-31 08:57:23.315084: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 160367 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Gradient-based Post-Training Quantization (GPTQ)\n",
    "# - Uses gradient information to fine-tune quantization parameters during conversion\n",
    "flag, quantized_model3 = GPTQ_Keras(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "075a2012-9423-4eaa-84c7-365a5daafa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Keras_mixed_precision Start ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Statistics Collection: 2it [00:01,  1.21it/s]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 105/105 [00:26<00:00,  3.91it/s]\n",
      "53it [00:22,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/ubuntu/.local/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/db7fa09d229843d3a68a02712b04141b-pulp.mps -sec 60 -timeMode elapsed -branch -printingOptions all -solution /tmp/db7fa09d229843d3a68a02712b04141b-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 112 COLUMNS\n",
      "At line 961 RHS\n",
      "At line 1069 BOUNDS\n",
      "At line 1229 ENDATA\n",
      "Problem MODEL has 107 rows, 212 columns and 477 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "seconds was changed from 1e+100 to 60\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 0.0258184 - 0.00 seconds\n",
      "Cgl0004I processed model has 54 rows, 159 columns (159 integer (159 of which binary)) and 265 elements\n",
      "Cbc0038I Initial state - 2 integers unsatisfied sum - 0.589583\n",
      "Cbc0038I Pass   1: suminf.    0.14150 (2) obj. 0.0507359 iterations 2\n",
      "Cbc0038I Solution found of 0.38846\n",
      "Cbc0038I Before mini branch and bound, 155 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 2 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.38846 to 0.0277218 (0.00 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.0275225\n",
      "Cbc0038I Reduced cost fixing fixed 100 variables on major pass 2\n",
      "Cbc0038I Pass   2: suminf.    0.58958 (2) obj. 0.0258184 iterations 3\n",
      "Cbc0038I Pass   3: suminf.    0.14771 (2) obj. 0.0275225 iterations 3\n",
      "Cbc0038I Pass   4: suminf.    0.14771 (2) obj. 0.0275225 iterations 4\n",
      "Cbc0038I Pass   5: suminf.    0.14771 (2) obj. 0.0275225 iterations 0\n",
      "Cbc0038I Pass   6: suminf.    0.14771 (2) obj. 0.0275225 iterations 8\n",
      "Cbc0038I Pass   7: suminf.    0.14771 (2) obj. 0.0275225 iterations 4\n",
      "Cbc0038I Pass   8: suminf.    0.46742 (2) obj. 0.0275225 iterations 3\n",
      "Cbc0038I Pass   9: suminf.    0.46742 (2) obj. 0.0275225 iterations 0\n",
      "Cbc0038I Pass  10: suminf.    0.47708 (2) obj. 0.0260981 iterations 1\n",
      "Cbc0038I Pass  11: suminf.    0.47708 (2) obj. 0.0260981 iterations 1\n",
      "Cbc0038I Pass  12: suminf.    0.47708 (2) obj. 0.0260981 iterations 0\n",
      "Cbc0038I Pass  13: suminf.    0.47708 (2) obj. 0.0260981 iterations 2\n",
      "Cbc0038I Pass  14: suminf.    0.40621 (4) obj. 0.0275225 iterations 3\n",
      "Cbc0038I Pass  15: suminf.    0.15708 (2) obj. 0.0274057 iterations 1\n",
      "Cbc0038I Pass  16: suminf.    0.24361 (2) obj. 0.0275225 iterations 1\n",
      "Cbc0038I Pass  17: suminf.    0.24361 (2) obj. 0.0275225 iterations 5\n",
      "Cbc0038I Pass  18: suminf.    0.15708 (2) obj. 0.0274057 iterations 2\n",
      "Cbc0038I Pass  19: suminf.    0.39133 (2) obj. 0.0275225 iterations 3\n",
      "Cbc0038I Pass  20: suminf.    0.26958 (2) obj. 0.0271261 iterations 2\n",
      "Cbc0038I Pass  21: suminf.    0.56332 (2) obj. 0.0275225 iterations 1\n",
      "Cbc0038I Pass  22: suminf.    0.56332 (2) obj. 0.0275225 iterations 2\n",
      "Cbc0038I Pass  23: suminf.    0.49958 (2) obj. 0.0273756 iterations 6\n",
      "Cbc0038I Pass  24: suminf.    0.49958 (2) obj. 0.0270793 iterations 1\n",
      "Cbc0038I Pass  25: suminf.    0.82796 (2) obj. 0.0275225 iterations 1\n",
      "Cbc0038I Pass  26: suminf.    0.52208 (2) obj. 0.0264621 iterations 3\n",
      "Cbc0038I Pass  27: suminf.    0.52208 (2) obj. 0.0261525 iterations 1\n",
      "Cbc0038I Pass  28: suminf.    0.46273 (2) obj. 0.0275225 iterations 1\n",
      "Cbc0038I Pass  29: suminf.    0.46273 (2) obj. 0.0275225 iterations 1\n",
      "Cbc0038I Pass  30: suminf.    0.46273 (2) obj. 0.0275225 iterations 0\n",
      "Cbc0038I Pass  31: suminf.    0.14771 (2) obj. 0.0275225 iterations 1\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 147 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 54 rows 159 columns, reduced to 1 rows 6 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.00 seconds)\n",
      "Cbc0038I After 0.00 seconds - Feasibility pump exiting with objective of 0.0277218 - took 0.00 seconds\n",
      "Cbc0012I Integer solution of 0.027721811 found by feasibility pump after 0 iterations and 0 nodes (0.00 seconds)\n",
      "Cbc0006I The LP relaxation is infeasible or too expensive\n",
      "Cbc0013I At root node, 0 cuts changed objective from 0.025818424 to 0.025818424 in 1 passes\n",
      "Cbc0014I Cut generator 0 (Probing) - 1 row cuts average 0.0 elements, 5 column cuts (5 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 1 (Gomory) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 2 (Knapsack) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 3 (Clique) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 4 (MixedIntegerRounding2) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 5 (FlowCover) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 6 (TwoMirCuts) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 7 (ZeroHalf) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0001I Search completed - best objective 0.02772181094939893, took 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0035I Maximum depth 0, 96 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from 0.0258184 to 0.0258184\n",
      "Probing was tried 1 times and created 6 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                0.02772181\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               0\n",
      "Time (CPU seconds):             0.01\n",
      "Time (Wallclock seconds):       0.01\n",
      "\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.01   (Wallclock seconds):       0.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 100/100 [09:43<00:00,  5.84s/it]\n",
      "Running GPTQ optimization:   0%|                                                      | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 1/2 [00:04<00:04,  4.36s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  1.94s/it]\u001b[A\n",
      "Running GPTQ optimization:  20%|█████████▏                                    | 1/5 [00:04<00:18,  4.62s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 1/2 [00:00<00:00,  3.43it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.78it/s]\u001b[A\n",
      "Running GPTQ optimization:  40%|██████████████████▍                           | 2/5 [00:05<00:06,  2.22s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 1/2 [00:00<00:00,  3.56it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.75it/s]\u001b[A\n",
      "Running GPTQ optimization:  60%|███████████████████████████▌                  | 3/5 [00:05<00:02,  1.46s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 1/2 [00:00<00:00,  3.49it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.86it/s]\u001b[A\n",
      "Running GPTQ optimization:  80%|████████████████████████████████████▊         | 4/5 [00:06<00:01,  1.09s/it]\u001b[A\n",
      "  0%|                                                                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 1/2 [00:00<00:00,  3.62it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.94it/s]\u001b[A\n",
      "Running GPTQ optimization: 100%|██████████████████████████████████████████████| 5/5 [00:06<00:00,  1.35s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmph7rh4ek7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmph7rh4ek7/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Keras_mixed_precision End -----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 09:08:15.356270: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-10-31 09:08:15.356316: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-10-31 09:08:15.356430: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmph7rh4ek7\n",
      "2025-10-31 09:08:15.362247: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-10-31 09:08:15.362263: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmph7rh4ek7\n",
      "2025-10-31 09:08:15.376570: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-10-31 09:08:15.463795: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmph7rh4ek7\n",
      "2025-10-31 09:08:15.499840: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 143410 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Method 4: GPTQ with Mixed Precision Quantization\n",
    "# - Combines gradient-based optimization with mixed precision techniques\n",
    "flag, quantized_model4 = GPTQ_Keras_mixed_precision(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19453943-75ba-4707-bef9-e137ccb91094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All quantization methods completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"All quantization methods completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b34232",
   "metadata": {},
   "source": [
    "## Models evaluation\n",
    "In order to evaluate our models, we first need to load the validation dataset. As before, please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ce90826-e5c5-4be0-8b54-96795eab47eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model evaluation phase...\n",
      "Found 50000 files belonging to 1000 classes.\n",
      "\n",
      "=== Original Model Evaluation ===\n",
      "1000/1000 [==============================] - 167s 166ms/step - loss: 1.2200 - accuracy: 0.7185\n",
      "Float model's Top 1 accuracy on the Imagenet validation set: 71.85%\n",
      "\n",
      "=== PTQ Model Evaluation ===\n",
      "1000/1000 [==============================] - 156s 156ms/step - loss: 1.3560 - accuracy: 0.7167\n",
      "PTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: 71.67%\n",
      "\n",
      "=== PTQ + Mixed Precision Model Evaluation ===\n",
      "1000/1000 [==============================] - 158s 157ms/step - loss: 1.3589 - accuracy: 0.7153\n",
      "PTQ_Keras_mixed_precision Quantized model's Top 1 accuracy on the Imagenet validation set: 71.53%\n",
      "\n",
      "=== GPTQ Model Evaluation ===\n",
      "1000/1000 [==============================] - 157s 157ms/step - loss: 1.3716 - accuracy: 0.7137\n",
      "GPTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: 71.37%\n",
      "\n",
      "=== GPTQ + Mixed Precision Model Evaluation ===\n",
      "1000/1000 [==============================] - 159s 158ms/step - loss: 1.3820 - accuracy: 0.7108\n",
      "GPTQ_Keras_mixed_precision Quantized model's Top 1 accuracy on the Imagenet validation set: 71.08%\n",
      "Fisish\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation and Accuracy Comparison\n",
    "print(\"Starting model evaluation phase...\")\n",
    "\n",
    "# Prepare validation dataset for accuracy assessment\n",
    "val_dataset = get_dataset(batch_size=50, shuffle=False)\n",
    "\n",
    "# Evaluate original floating-point model accuracy\n",
    "print(\"\\n=== Original Model Evaluation ===\")\n",
    "float_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "float_accuracy = float_model.evaluate(val_dataset)\n",
    "print(f\"Float model's Top 1 accuracy on the Imagenet validation set: {(float_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# Evaluate PTQ quantized model accuracy\n",
    "print(\"\\n=== PTQ Model Evaluation ===\")\n",
    "quantized_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model.evaluate(val_dataset)\n",
    "print(f\"PTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# Evaluate PTQ + Mixed Precision model accuracy\n",
    "print(\"\\n=== PTQ + Mixed Precision Model Evaluation ===\")\n",
    "quantized_model2.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model2.evaluate(val_dataset)\n",
    "print(f\"PTQ_Keras_mixed_precision Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# Evaluate GPTQ quantized model accuracy\n",
    "print(\"\\n=== GPTQ Model Evaluation ===\")\n",
    "quantized_model3.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model3.evaluate(val_dataset)\n",
    "print(f\"GPTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# Evaluate GPTQ + Mixed Precision model accuracy\n",
    "print(\"\\n=== GPTQ + Mixed Precision Model Evaluation ===\")\n",
    "quantized_model4.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model4.evaluate(val_dataset)\n",
    "print(f\"GPTQ_Keras_mixed_precision Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "print(\"Fisish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71f63d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCTWrapper with a few lines of code.\n",
    "\n",
    "MCT can deliver competitive results across a wide range of tasks and network architectures. For more details, [check out the paper:](https://arxiv.org/abs/2109.09113).\n",
    "\n",
    "## Copyrights\n",
    "\n",
    "Copyright 2025 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

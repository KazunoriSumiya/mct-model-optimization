{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ca09f9",
   "metadata": {},
   "source": [
    "# Model Compression Toolkit (MCT) Wrapper API Comprehensive Quantization Comparison\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_wrapper.ipynb)\n",
    "\n",
    "## Overview \n",
    "This notebook provides a comprehensive demonstration of the MCT (Model Compression Toolkit) Wrapper API functionality, showcasing five different quantization methods on a MobileNetV2 model. The tutorial systematically compares the implementation, performance characteristics, and accuracy trade-offs of each quantization approach: PTQ (Post-Training Quantization), PTQ with Mixed Precision, GPTQ (Gradient-based PTQ), GPTQ with Mixed Precision, and LQ-PTQ (Low-bit Quantizer PTQ). Each method utilizes the unified MCTWrapper interface for consistent implementation and comparison.\n",
    "\n",
    "## Summary\n",
    "1. **Environment Setup**: Import required libraries and configure MCT with MobileNetV2 model\n",
    "2. **Dataset Preparation**: Load and prepare ImageNet validation dataset with representative data generation\n",
    "3. **PTQ Implementation**: Execute basic Post-Training Quantization with 8-bit precision and bias correction\n",
    "4. **PTQ + Mixed Precision**: Apply intelligent bit-width allocation based on layer sensitivity analysis (75% compression ratio)\n",
    "5. **GPTQ Implementation**: Perform gradient-based optimization with 5-epoch fine-tuning for enhanced accuracy\n",
    "6. **GPTQ + Mixed Precision**: Combine gradient optimization with mixed precision for optimal accuracy-compression trade-off\n",
    "7. **LQ-PTQ Implementation**: Execute ultra-low bit quantization (2-4 bits) with specialized converter requirements\n",
    "8. **Performance Evaluation**: Comprehensive accuracy assessment and comparison across all quantization methods\n",
    "9. **Results Analysis**: Compare model sizes, inference accuracy, and quantization trade-offs\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea7170-2624-4139-a659-8e27caf5ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for deep learning and file handling\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2 \n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Generator, Any, Callable\n",
    "\n",
    "# Alternative pip install commands (commented out for local development)\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138ad45-68d2-4c34-95ac-1a327ce54406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MCT core\n",
    "#import importlib\n",
    "#if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "#    !pip install model_compression_toolkit\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/wrapper/sonyfork/mct-model-optimization')\n",
    "\n",
    "import model_compression_toolkit as mct\n",
    "from model_compression_toolkit.core import QuantizationErrorMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce48a1",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e473150",
   "metadata": {
    "lines_to_next_cell": 1,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download and extract ImageNet validation dataset if not already present\n",
    "# This setup is required for model quantization and evaluation\n",
    "if not os.path.isdir('imagenet'):\n",
    "    # Create directory and download dataset files\n",
    "    os.system('mkdir imagenet')\n",
    "    os.system('wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz')\n",
    "    os.system('wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar')\n",
    "    # Extract downloaded archives\n",
    "    os.system('cd imagenet && tar -xzf ILSVRC2012_devkit_t12.tar.gz &&       mkdir ILSVRC2012_img_val && tar -xf ILSVRC2012_img_val.tar -C ILSVRC2012_img_val')\n",
    "\n",
    "# Define paths for dataset organization\n",
    "root = Path('./imagenet')\n",
    "imgs_dir = root / 'ILSVRC2012_img_val'\n",
    "target_dir = root /'val'\n",
    "\n",
    "def extract_labels() -> List[str]:\n",
    "    \"\"\"Extract ground truth labels from ImageNet metadata\"\"\"\n",
    "    os.system('pip install -q scipy')\n",
    "    import scipy\n",
    "    mat = scipy.io.loadmat(root / 'ILSVRC2012_devkit_t12/data/meta.mat', squeeze_me=True)\n",
    "    cls_to_nid = {s[0]: s[1] for i, s in enumerate(mat['synsets']) if s[4] == 0} \n",
    "    with open(root / 'ILSVRC2012_devkit_t12/data/ILSVRC2012_validation_ground_truth.txt', 'r') as f:\n",
    "        return [cls_to_nid[int(cls)] for cls in f.readlines()]\n",
    "\n",
    "# Organize images into class-specific directories for tf.data.Dataset compatibility\n",
    "if not target_dir.exists():\n",
    "    labels = extract_labels()\n",
    "    for lbl in set(labels):\n",
    "        os.makedirs(target_dir / lbl)\n",
    "    for img_file, lbl in zip(sorted(os.listdir(imgs_dir)), labels):\n",
    "        shutil.move(imgs_dir / img_file, target_dir / lbl)\n",
    "\n",
    "# Preprocessing function for MobileNetV2\n",
    "def imagenet_preprocess_input(images: tf.Tensor, labels: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"Apply MobileNetV2-specific preprocessing to input images\"\"\"\n",
    "    return tf.keras.applications.mobilenet_v2.preprocess_input(images), labels\n",
    "\n",
    "def get_dataset(batch_size: int, shuffle: bool) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Create a tf.data.Dataset from ImageNet validation images\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of images per batch\n",
    "        shuffle: Whether to shuffle the dataset\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed and optimized tf.data.Dataset\n",
    "    \"\"\"\n",
    "    # Load images from directory structure with automatic labeling\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory='./imagenet/val',\n",
    "        batch_size=batch_size,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=shuffle,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear')\n",
    "    dataset = dataset.map(lambda x, y: (imagenet_preprocess_input(x, y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eed912",
   "metadata": {},
   "source": [
    "### Representative dataset construction\n",
    "We show how to create a generator for the representative dataset, which is required for post-training quantization.\n",
    "\n",
    "The representative dataset is used for collecting statistics on the inference outputs of all layers in the model.\n",
    " \n",
    "In order to decide on the size of the representative dataset, we configure the batch size and the number of calibration iterations.\n",
    "This gives us the total number of samples that will be used during PTQ (batch_size x n_iter).\n",
    "In this example we set `batch_size = 50` and `n_iter = 10`, resulting in a total of 500 representative images.\n",
    "\n",
    "Please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897999a-781d-472f-a494-2aabd758e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters for representative dataset generation\n",
    "batch_size: int = 5  # Number of images per batch for quantization calibration\n",
    "n_iter: int = 2      # Number of iterations to generate representative data\n",
    "\n",
    "# Create dataset instance for representative data generation\n",
    "dataset = get_dataset(batch_size, shuffle=True)\n",
    "\n",
    "# Generator for representative dataset used in quantization\n",
    "def representative_dataset_gen() -> Generator[List[Any], None, None]:\n",
    "    \"\"\"\n",
    "    Generator function for representative dataset used in quantization calibration.\n",
    "    \n",
    "    This function provides a small subset of data that MCT uses to:\n",
    "    - Calibrate quantization parameters\n",
    "    - Determine optimal activation ranges\n",
    "    - Configure quantization thresholds\n",
    "    \n",
    "    Yields:\n",
    "        List containing numpy arrays of image batches\n",
    "    \"\"\"\n",
    "    for _ in range(n_iter):\n",
    "        # Extract one batch and convert to numpy format required by MCT\n",
    "        yield [dataset.take(1).get_single_element()[0].numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa8e319",
   "metadata": {},
   "source": [
    "## Model Post-Training quantization using MCTWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8db92-9151-42d6-b3fa-311f7d528df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator to provide consistent logging and error handling for quantization functions\n",
    "def decorator(func: Callable[[keras.Model], Tuple[bool, keras.Model]]) -> Callable[[keras.Model], Tuple[bool, keras.Model]]:\n",
    "    \"\"\"\n",
    "    Wrapper decorator that:\n",
    "    - Logs function start and end execution\n",
    "    - Handles success/failure status from quantization functions\n",
    "    - Exits program if quantization fails\n",
    "    \n",
    "    Args:\n",
    "        func: Function to be decorated (quantization function)\n",
    "    \n",
    "    Returns:\n",
    "        Wrapped function with logging and error handling\n",
    "    \"\"\"\n",
    "    def wrapper(*args: Any, **kwargs: Any) -> Tuple[bool, keras.Model]:\n",
    "        print(f\"----------------- {func.__name__} Start ---------------\")\n",
    "        flag, result = func(*args, **kwargs)\n",
    "        print(f\"----------------- {func.__name__} End -----------------\")\n",
    "        if not flag:exit()\n",
    "        return flag, result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf35f64",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ (Post-Training Quantization) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151ddca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Keras(float_model: keras.Model) -> Tuple[bool, keras.Model]:\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization (PTQ) using MCT on Keras model.\n",
    "    \n",
    "    PTQ is a quantization method that:\n",
    "    - Does not require model retraining\n",
    "    - Uses representative data for calibration\n",
    "    - Provides good accuracy with minimal computational overhead\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for basic PTQ quantization\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_MCT_TPC = True                # Use MCT's built-in Target Platform Capabilities\n",
    "    use_MixP = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PTQ\n",
    "    param_items = [\n",
    "        ['tpc_version', '1.0', 'The version of the TPC to use.'],\n",
    "        \n",
    "        # Quantization configuration parameters\n",
    "        ['activation_error_method', QuantizationErrorMethod.MSE, 'Error metric for activation quantization'],\n",
    "        ['weights_bias_correction', True, 'Enable bias correction for weights'],\n",
    "        ['z_threshold', float('inf'), 'Threshold for zero-point quantization'],\n",
    "        ['linear_collapsing', True, 'Enable linear layer collapsing optimization'],\n",
    "        ['residual_collapsing', True, 'Enable residual connection collapsing'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Keras.tflite', 'Path to save the quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mctwrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f6eeb",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ + Mixed Precision Quantization (MixP) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082471a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Keras_MixP(float_model: keras.Model) -> Tuple[bool, keras.Model]:\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization with Mixed Precision (PTQ + MixP) on Keras model.\n",
    "    \n",
    "    Mixed Precision Quantization:\n",
    "    - Uses different bit-widths for different layers\n",
    "    - Optimizes model size while maintaining accuracy\n",
    "    - Automatically selects optimal precision for each layer\n",
    "    - Uses resource constraints to guide precision allocation\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PTQ with mixed precision\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_MCT_TPC = True                # Use MCT's built-in Target Platform Capabilities\n",
    "    use_MixP = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        ['tpc_version', '1.0', 'The version of the TPC to use.'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based sensitivity scores for layer importance'],\n",
    "        \n",
    "        # Resource constraint configuration\n",
    "        ['weights_compression_ratio', 0.75, 'Target compression ratio for model weights (75% of original size)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Keras_MixP.tflite', 'Path to save the mixed precision quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute mixed precision quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mctwrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f2cba",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ (Gradient-based PTQ) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c0070",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Keras(float_model: keras.Model) -> Tuple[bool, keras.Model]:\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization (GPTQ) on Keras model.\n",
    "    \n",
    "    GPTQ is an advanced quantization method that:\n",
    "    - Uses gradient information to optimize quantization parameters\n",
    "    - Fine-tunes the model during quantization process\n",
    "    - Generally provides better accuracy than standard PTQ\n",
    "    - Requires slightly more computational resources than PTQ\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for GPTQ quantization\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_MCT_TPC = False               # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_MixP = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for GPTQ\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Keras.tflite', 'Path to save the GPTQ quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute GPTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mctwrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d77b5",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ + Mixed Precision Quantization (MixP) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e06b4e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Keras_MixP(float_model: keras.Model) -> Tuple[bool, keras.Model]:\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization with Mixed Precision (GPTQ + MixP).\n",
    "    \n",
    "    This combines the benefits of both techniques:\n",
    "    - GPTQ: Gradient-based optimization for better quantization accuracy\n",
    "    - Mixed Precision: Optimal bit-width allocation for size/accuracy trade-off\n",
    "    \n",
    "    This is the most advanced quantization method available, providing:\n",
    "    - Best possible accuracy preservation\n",
    "    - Optimal model size reduction\n",
    "    - Automatic precision selection per layer\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for GPTQ with mixed precision\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_MCT_TPC = False               # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_MixP = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for GPTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default)'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision sensitivity analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based scores for layer importance ranking'],\n",
    "        \n",
    "        # Resource constraint configuration\n",
    "        ['weights_compression_ratio', 0.75, 'Target compression ratio for model weights (75% reduction)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Keras_MixP.tflite', 'Path to save the GPTQ+MixP quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute advanced GPTQ+MixP quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mctwrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8fb733",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run LQPTQ (Low-bit Quantizer PTQ) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator\n",
    "def LQPTQ_Keras(float_model: keras.Model) -> Tuple[bool, keras.Model]:\n",
    "    \"\"\"\n",
    "    Perform Low-bit Quantizer Post-Training Quantization (LQ-PTQ) on Keras model.\n",
    "    \n",
    "    LQ-PTQ is a specialized quantization method that:\n",
    "    - Targets very low bit-width quantization (e.g., 2-4 bits)\n",
    "    - Uses advanced techniques for ultra-low precision\n",
    "    - Requires specific converter versions for deployment\n",
    "    - Currently only supports TensorFlow/Keras framework\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for LQ-PTQ quantization\n",
    "    method = 'LQPTQ'                  # Low-bit Quantizer Post-Training Quantization\n",
    "    framework = 'tensorflow'          # Target framework (TensorFlow only for LQ-PTQ)\n",
    "    use_MCT_TPC = False               # Use external Target Platform Capabilities\n",
    "    use_MixP = False                  # Mixed precision not applicable for LQ-PTQ\n",
    "\n",
    "    # Parameter configuration for LQ-PTQ\n",
    "    param_items = [\n",
    "        # LQ-PTQ specific training parameters\n",
    "        ['learning_rate', 0.0001, 'Learning rate for low-bit quantization optimization'],\n",
    "        ['converter_ver', 'v3.14', 'Converter version for deployment compatibility'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_LQPTQ_Keras.tflite', 'Path to save the LQ-PTQ quantized model']\n",
    "    ]\n",
    "\n",
    "    # LQ-PTQ requires a different representative dataset format (single batch, not generator)\n",
    "    representative_dataset = dataset.take(1).get_single_element()[0].numpy()\n",
    "    \n",
    "    # Execute LQ-PTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mctwrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fc4ce",
   "metadata": {},
   "source": [
    "### Run model Post-Training Quantization\n",
    "Lastly, we quantize our model using MCTWrapper API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b959792-f339-43a9-9573-a95052b28e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2 model as the base model for quantization experiments\n",
    "float_model = MobileNetV2()\n",
    "\n",
    "# Execute comprehensive quantization method comparison using MCT Wrapper functionality\n",
    "print(\"Starting quantization experiments with different methods...\")\n",
    "\n",
    "# Method 1: Basic Post-Training Quantization (PTQ)\n",
    "# - Standard 8-bit quantization without optimization\n",
    "# - Fastest method with good baseline performance\n",
    "flag, quantized_model = PTQ_Keras(float_model)\n",
    "\n",
    "# Method 2: PTQ with Mixed Precision Quantization\n",
    "# - Uses different bit-widths for different layers based on sensitivity analysis\n",
    "# - Optimizes model size while maintaining accuracy through intelligent bit allocation\n",
    "flag, quantized_model2 = PTQ_Keras_MixP(float_model)\n",
    "\n",
    "# Method 3: Gradient-based Post-Training Quantization (GPTQ)\n",
    "# - Uses gradient information to fine-tune quantization parameters\n",
    "# - Provides better accuracy than basic PTQ through learned optimization\n",
    "flag, quantized_model3 = GPTQ_Keras(float_model)\n",
    "\n",
    "# Method 4: GPTQ with Mixed Precision Quantization\n",
    "# - Combines gradient-based optimization with mixed precision\n",
    "# - Delivers the best accuracy-compression trade-off available\n",
    "flag, quantized_model4 = GPTQ_Keras_MixP(float_model)\n",
    "\n",
    "# Method 5: Low-bit Quantization Post-Training Quantization (LQ-PTQ)\n",
    "# - Experimental ultra-low precision quantization (commented out - requires specific setup)\n",
    "#flag, quantized_model5 = LQPTQ_Keras(float_model)\n",
    "\n",
    "print(\"All quantization methods completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b34232",
   "metadata": {},
   "source": [
    "## Models evaluation\n",
    "In order to evaluate our models, we first need to load the validation dataset. As before, please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c0ae4-79a5-403c-ad83-a2d7c9604ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Accuracy Comparison\n",
    "print(\"Starting model evaluation phase...\")\n",
    "\n",
    "# Prepare validation dataset for accuracy assessment\n",
    "val_dataset = get_dataset(batch_size=50, shuffle=False)\n",
    "\n",
    "# Evaluate original floating-point model accuracy\n",
    "print(\"\\n=== Original Model Evaluation ===\")\n",
    "float_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "float_accuracy = float_model.evaluate(val_dataset)\n",
    "print(f\"Float model's Top 1 accuracy on the Imagenet validation set: {(float_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# Evaluate PTQ quantized model accuracy\n",
    "print(\"\\n=== PTQ Model Evaluation ===\")\n",
    "quantized_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model.evaluate(val_dataset)\n",
    "print(f\"PTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# Evaluate PTQ + Mixed Precision model accuracy\n",
    "print(\"\\n=== PTQ + Mixed Precision Model Evaluation ===\")\n",
    "quantized_model2.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model2.evaluate(val_dataset)\n",
    "print(f\"PTQ_Keras_MixP Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# Evaluate GPTQ quantized model accuracy\n",
    "print(\"\\n=== GPTQ Model Evaluation ===\")\n",
    "quantized_model3.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model3.evaluate(val_dataset)\n",
    "print(f\"GPTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# Evaluate GPTQ + Mixed Precision model accuracy\n",
    "print(\"\\n=== GPTQ + Mixed Precision Model Evaluation ===\")\n",
    "quantized_model4.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model4.evaluate(val_dataset)\n",
    "print(f\"GPTQ_Keras_MixP Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# LQ-PTQ model evaluation (commented out)\n",
    "#print(\"\\n=== LQ-PTQ Model Evaluation ===\")\n",
    "#quantized_model5.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "#quantized_accuracy = quantized_model5.evaluate(val_dataset)\n",
    "#print(f\"Quantized model5's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "print(\"Fisish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf0aa8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCTWrapper with a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71f63d",
   "metadata": {},
   "source": [
    "Copyright 2024 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

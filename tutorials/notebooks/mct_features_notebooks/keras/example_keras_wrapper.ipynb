{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9ca09f9",
   "metadata": {},
   "source": [
    "# Model Compression Toolkit (MCT) Wrapper API Comprehensive Quantization Comparison(tensorflow)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/SonySemiconductorSolutions/mct-model-optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_wrapper.ipynb)\n",
    "\n",
    "## Overview \n",
    "This notebook provides a comprehensive demonstration of the MCT (Model Compression Toolkit) Wrapper API functionality, showcasing five different quantization methods on a MobileNetV2 model. The tutorial systematically compares the implementation, performance characteristics, and accuracy trade-offs of each quantization approach: PTQ (Post-Training Quantization), PTQ with Mixed Precision, GPTQ (Gradient-based PTQ), GPTQ with Mixed Precision, and LQ-PTQ (Low-bit Quantizer PTQ). Each method utilizes the unified MCTWrapper interface for consistent implementation and comparison.\n",
    "\n",
    "## Summary\n",
    "1. **Environment Setup**: Import required libraries and configure MCT with MobileNetV2 model\n",
    "2. **Dataset Preparation**: Load and prepare ImageNet validation dataset with representative data generation\n",
    "3. **PTQ Implementation**: Execute basic Post-Training Quantization with 8-bit precision and bias correction\n",
    "4. **PTQ + Mixed Precision**: Apply intelligent bit-width allocation based on layer sensitivity analysis (75% compression ratio)\n",
    "5. **GPTQ Implementation**: Perform gradient-based optimization with 5-epoch fine-tuning for enhanced accuracy\n",
    "6. **GPTQ + Mixed Precision**: Combine gradient optimization with mixed precision for optimal accuracy-compression trade-off\n",
    "7. **LQ-PTQ Implementation**: Execute ultra-low bit quantization (2-4 bits) with specialized converter requirements\n",
    "8. **Performance Evaluation**: Comprehensive accuracy assessment and comparison across all quantization methods\n",
    "9. **Results Analysis**: Compare model sizes, inference accuracy, and quantization trade-offs\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea7170-2624-4139-a659-8e27caf5ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for deep learning and file handling\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2 \n",
    "from pathlib import Path\n",
    "from typing import Callable, Generator, List, Tuple, Any\n",
    "\n",
    "# Alternative pip install commands (commented out for local development)\n",
    "!pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138ad45-68d2-4c34-95ac-1a327ce54406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MCT core\n",
    "#import importlib\n",
    "#if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "#    !pip install model_compression_toolkit\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/wrapper/sonyfork/mct-model-optimization')\n",
    "\n",
    "import model_compression_toolkit as mct\n",
    "from model_compression_toolkit.core import QuantizationErrorMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce48a1",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f14d92-5116-40b7-9090-f378bdf3cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and setup ImageNet validation dataset if not already present\n",
    "if not os.path.isdir('imagenet'):\n",
    "    # Create directory and download required ImageNet files\n",
    "    os.system('mkdir imagenet')\n",
    "    os.system('wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz')\n",
    "    os.system('wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar')\n",
    "\n",
    "    # Move downloaded files to imagenet directory\n",
    "    os.system('mv ILSVRC2012_devkit_t12.tar.gz imagenet/')\n",
    "    os.system('mv ILSVRC2012_img_val.tar imagenet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e473150",
   "metadata": {
    "lines_to_next_cell": 1,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup ImageNet validation directory structure if not exists\n",
    "# This creates the directory structure expected by TensorFlow's image_dataset_from_directory\n",
    "# Check if ImageNet validation directory already exists\n",
    "if not os.path.isdir('imagenet/val'):\n",
    "    import subprocess\n",
    "    \n",
    "    # Clone MCT repository temporarily to access setup scripts\n",
    "    # This provides access to ImageNet data preparation utilities\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/sony/model_optimization.git', 'temp_mct'])\n",
    "    \n",
    "    # Make ImageNet preparation script executable with proper permissions\n",
    "    os.system('chmod +x ../../../resources/scripts/prepare_imagenet.sh')\n",
    "\n",
    "    # Run the preparation script to organize ImageNet data into proper directory structure\n",
    "    # This script handles data extraction and organization for TensorFlow compatibility\n",
    "    subprocess.run(['../../../resources/scripts/prepare_imagenet.sh'])\n",
    "\n",
    "def imagenet_preprocess_input(images: tf.Tensor, labels: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Apply MobileNetV2-specific preprocessing to input images.\n",
    "    \n",
    "    This function normalizes pixel values according to MobileNetV2 requirements,\n",
    "    ensuring consistent input format for the model.\n",
    "    \n",
    "    Args:\n",
    "        images: Input image tensor\n",
    "        labels: Corresponding label tensor\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of preprocessed images and unchanged labels\n",
    "    \"\"\"\n",
    "    return tf.keras.applications.mobilenet_v2.preprocess_input(images), labels\n",
    "\n",
    "def get_dataset(batch_size: int, shuffle: bool):\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory='./imagenet/val',\n",
    "        batch_size=batch_size,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=shuffle,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear'\n",
    "    )\n",
    "    dataset = dataset.map(lambda x, y: imagenet_preprocess_input(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897999a-781d-472f-a494-2aabd758e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters for representative dataset generation\n",
    "# These parameters control the calibration dataset used for quantization\n",
    "batch_size = 5  # Number of images per batch for quantization calibration\n",
    "n_iter = 2      # Number of iterations to generate representative data\n",
    "                # Total calibration samples = batch_size * n_iter = 10 images\n",
    "\n",
    "# Create dataset instance for representative data generation\n",
    "# Use shuffled data to ensure diverse representative samples\n",
    "dataset = get_dataset(batch_size, shuffle=True)\n",
    "\n",
    "# Generator function for representative dataset used in quantization calibration\n",
    "def representative_dataset_gen():\n",
    "    \"\"\"\n",
    "    Generator function for representative dataset used in quantization calibration.\n",
    "\n",
    "    This function provides a small subset of data that MCT uses for:\n",
    "    - Calibrating quantization parameters across all model layers\n",
    "    - Determining optimal activation value ranges for each layer\n",
    "    - Computing quantization thresholds based on actual data distribution\n",
    "    - Minimizing quantization error through data-driven parameter selection\n",
    "    \n",
    "    Yields:\n",
    "        List containing numpy arrays of image batches in MCT-expected format\n",
    "    \"\"\"\n",
    "    for _ in range(n_iter):\n",
    "        # Extract one batch from the dataset and convert to numpy format\n",
    "       yield [dataset.take(1).get_single_element()[0].numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa8e319",
   "metadata": {},
   "source": [
    "## Model Post-Training quantization using MCTWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8db92-9151-42d6-b3fa-311f7d528df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator to provide consistent logging and error handling for quantization functions\n",
    "def decorator(func):\n",
    "    \"\"\"\n",
    "    Wrapper decorator that provides standardized execution logging and error handling.\n",
    "    \n",
    "    This decorator enhances quantization functions by:\n",
    "    - Providing clear start/end execution markers for debugging\n",
    "    - Handling success/failure status from quantization operations\n",
    "    - Implementing fail-fast behavior on quantization errors\n",
    "    - Ensuring consistent logging format across all quantization methods\n",
    "    \n",
    "    Usage:\n",
    "        @decorator\n",
    "        def quantization_function(model):\n",
    "            # quantization implementation\n",
    "            return flag, quantized_model\n",
    "    \n",
    "    Args:\n",
    "        func: Function to be decorated (typically a quantization function)\n",
    "    \n",
    "    Returns:\n",
    "        Wrapped function with enhanced logging and error handling capabilities\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Log function execution start with clear delimiter\n",
    "        print(f\"----------------- {func.__name__} Start ---------------\")\n",
    "        \n",
    "        # Execute the quantization function and capture return values\n",
    "        # Expected return format: (success_flag, quantized_model)\n",
    "        flag, result = func(*args, **kwargs)\n",
    "        \n",
    "        # Log function execution completion\n",
    "        print(f\"----------------- {func.__name__} End -----------------\")\n",
    "        \n",
    "        # Implement fail-fast behavior: exit immediately on quantization failure\n",
    "        # This ensures early detection of quantization issues\n",
    "        if not flag:\n",
    "            exit()\n",
    "        \n",
    "        # Return original function results if successful\n",
    "        return flag, result\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf35f64",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ (Post-Training Quantization) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151ddca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Keras(float_model):\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization (PTQ) using MCT on Keras model.\n",
    "    \n",
    "    PTQ is a quantization method that:\n",
    "    - Does not require model retraining\n",
    "    - Uses representative data for calibration\n",
    "    - Provides good accuracy with minimal computational overhead\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for basic PTQ quantization\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_MCT_TPC = True                # Use MCT's built-in Target Platform Capabilities\n",
    "    use_MixP = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PTQ\n",
    "    param_items = [\n",
    "        ['tpc_version', '1.0', 'The version of the TPC to use.'],\n",
    "        \n",
    "        # Quantization configuration parameters\n",
    "        ['activation_error_method', QuantizationErrorMethod.MSE, 'Error metric for activation quantization'],\n",
    "        ['weights_bias_correction', True, 'Enable bias correction for weights'],\n",
    "        ['z_threshold', float('inf'), 'Threshold for zero-point quantization'],\n",
    "        ['linear_collapsing', True, 'Enable linear layer collapsing optimization'],\n",
    "        ['residual_collapsing', True, 'Enable residual connection collapsing'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Keras.tflite', 'Path to save the quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f6eeb",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ + Mixed Precision Quantization (MixP) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082471a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Keras_MixP(float_model):\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization with Mixed Precision (PTQ + MixP) on Keras model.\n",
    "    \n",
    "    Mixed Precision Quantization:\n",
    "    - Uses different bit-widths for different layers\n",
    "    - Optimizes model size while maintaining accuracy\n",
    "    - Automatically selects optimal precision for each layer\n",
    "    - Uses resource constraints to guide precision allocation\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PTQ with mixed precision\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_MCT_TPC = True                # Use MCT's built-in Target Platform Capabilities\n",
    "    use_MixP = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        ['tpc_version', '1.0', 'The version of the TPC to use.'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based sensitivity scores for layer importance'],\n",
    "        \n",
    "        # Resource constraint configuration\n",
    "        ['weights_compression_ratio', 0.75, 'Target compression ratio for model weights (75% of original size)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Keras_MixP.tflite', 'Path to save the mixed precision quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute mixed precision quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f2cba",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ (Gradient-based PTQ) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c0070",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Keras(float_model):\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization (GPTQ) on Keras model.\n",
    "    \n",
    "    GPTQ is an advanced quantization method that:\n",
    "    - Uses gradient information to optimize quantization parameters\n",
    "    - Fine-tunes the model during quantization process\n",
    "    - Generally provides better accuracy than standard PTQ\n",
    "    - Requires slightly more computational resources than PTQ\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for GPTQ quantization\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_MCT_TPC = False               # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_MixP = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for GPTQ\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Keras.tflite', 'Path to save the GPTQ quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute GPTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d77b5",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ + Mixed Precision Quantization (MixP) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e06b4e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Keras_MixP(float_model):\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization with Mixed Precision (GPTQ + MixP).\n",
    "    \n",
    "    This combines the benefits of both techniques:\n",
    "    - GPTQ: Gradient-based optimization for better quantization accuracy\n",
    "    - Mixed Precision: Optimal bit-width allocation for size/accuracy trade-off\n",
    "    \n",
    "    This is the most advanced quantization method available, providing:\n",
    "    - Best possible accuracy preservation\n",
    "    - Optimal model size reduction\n",
    "    - Automatic precision selection per layer\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for GPTQ with mixed precision\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'tensorflow'          # Target framework (Keras/TensorFlow)\n",
    "    use_MCT_TPC = False               # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_MixP = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for GPTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default)'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision sensitivity analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based scores for layer importance ranking'],\n",
    "        \n",
    "        # Resource constraint configuration\n",
    "        ['weights_compression_ratio', 0.75, 'Target compression ratio for model weights (75% reduction)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Keras_MixP.tflite', 'Path to save the GPTQ+MixP quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute advanced GPTQ+MixP quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8fb733",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run LQPTQ (Low-bit Quantizer PTQ) with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator\n",
    "def LQPTQ_Keras(float_model):\n",
    "    \"\"\"\n",
    "    Perform Low-bit Quantizer Post-Training Quantization (LQ-PTQ) on Keras model.\n",
    "    \n",
    "    LQ-PTQ is a specialized quantization method that:\n",
    "    - Targets very low bit-width quantization (e.g., 2-4 bits)\n",
    "    - Uses advanced techniques for ultra-low precision\n",
    "    - Requires specific converter versions for deployment\n",
    "    - Currently only supports TensorFlow/Keras framework\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point Keras model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for LQ-PTQ quantization\n",
    "    method = 'LQPTQ'                  # Low-bit Quantizer Post-Training Quantization\n",
    "    framework = 'tensorflow'          # Target framework (TensorFlow only for LQ-PTQ)\n",
    "    use_MCT_TPC = False               # Use external Target Platform Capabilities\n",
    "    use_MixP = False                  # Mixed precision not applicable for LQ-PTQ\n",
    "\n",
    "    # Parameter configuration for LQ-PTQ\n",
    "    param_items = [\n",
    "        # LQ-PTQ specific training parameters\n",
    "        ['learning_rate', 0.0001, 'Learning rate for low-bit quantization optimization'],\n",
    "        ['converter_ver', 'v3.14', 'Converter version for deployment compatibility'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_LQPTQ_Keras.tflite', 'Path to save the LQ-PTQ quantized model']\n",
    "    ]\n",
    "\n",
    "    # LQ-PTQ requires a different representative dataset format (single batch, not generator)\n",
    "    representative_dataset = dataset.take(1).get_single_element()[0].numpy()\n",
    "    \n",
    "    # Execute LQ-PTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314fc4ce",
   "metadata": {},
   "source": [
    "### Run model Post-Training Quantization\n",
    "Lastly, we quantize our model using MCTWrapper API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b959792-f339-43a9-9573-a95052b28e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2 model as the base model for quantization experiments\n",
    "# This model serves as the reference floating-point model for all quantization methods\n",
    "float_model = MobileNetV2()\n",
    "\n",
    "# Execute comprehensive quantization method comparison using MCT Wrapper functionality\n",
    "# Each method represents different trade-offs between accuracy, model size, and computation time\n",
    "print(\"Starting quantization experiments with different methods...\")\n",
    "\n",
    "# Method 1: Basic Post-Training Quantization (PTQ)\n",
    "# - Standard 8-bit quantization without advanced optimization techniquesed\n",
    "flag, quantized_model = PTQ_Keras(float_model)\n",
    "\n",
    "# Method 2: PTQ with Mixed Precision Quantization\n",
    "# - Uses different bit-widths for different layers based on sensitivity analysis\n",
    "flag, quantized_model2 = PTQ_Keras_MixP(float_model)\n",
    "\n",
    "# Method 3: Gradient-based Post-Training Quantization (GPTQ)\n",
    "# - Uses gradient information to fine-tune quantization parameters during conversion\n",
    "flag, quantized_model3 = GPTQ_Keras(float_model)\n",
    "\n",
    "# Method 4: GPTQ with Mixed Precision Quantization\n",
    "# - Combines gradient-based optimization with mixed precision techniques\n",
    "flag, quantized_model4 = GPTQ_Keras_MixP(float_model)\n",
    "\n",
    "# Method 5: Low-bit Quantization Post-Training Quantization (LQ-PTQ)\n",
    "# - Experimental ultra-low precision quantization (2-4 bits per weight)\n",
    "#flag, quantized_model5 = LQPTQ_Keras(float_model)\n",
    "\n",
    "print(\"All quantization methods completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b34232",
   "metadata": {},
   "source": [
    "## Models evaluation\n",
    "In order to evaluate our models, we first need to load the validation dataset. As before, please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce90826-e5c5-4be0-8b54-96795eab47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Accuracy Comparison\n",
    "print(\"Starting model evaluation phase...\")\n",
    "\n",
    "# Prepare validation dataset for accuracy assessment\n",
    "val_dataset = get_dataset(batch_size=50, shuffle=False)\n",
    "\n",
    "# Evaluate original floating-point model accuracy\n",
    "print(\"\\n=== Original Model Evaluation ===\")\n",
    "float_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "float_accuracy = float_model.evaluate(val_dataset)\n",
    "print(f\"Float model's Top 1 accuracy on the Imagenet validation set: {(float_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# Evaluate PTQ quantized model accuracy\n",
    "print(\"\\n=== PTQ Model Evaluation ===\")\n",
    "quantized_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model.evaluate(val_dataset)\n",
    "print(f\"PTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# Evaluate PTQ + Mixed Precision model accuracy\n",
    "print(\"\\n=== PTQ + Mixed Precision Model Evaluation ===\")\n",
    "quantized_model2.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model2.evaluate(val_dataset)\n",
    "print(f\"PTQ_Keras_MixP Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# Evaluate GPTQ quantized model accuracy\n",
    "print(\"\\n=== GPTQ Model Evaluation ===\")\n",
    "quantized_model3.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model3.evaluate(val_dataset)\n",
    "print(f\"GPTQ_Keras Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# Evaluate GPTQ + Mixed Precision model accuracy\n",
    "print(\"\\n=== GPTQ + Mixed Precision Model Evaluation ===\")\n",
    "quantized_model4.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "quantized_accuracy = quantized_model4.evaluate(val_dataset)\n",
    "print(f\"GPTQ_Keras_MixP Quantized model's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "# LQ-PTQ model evaluation (commented out)\n",
    "#print(\"\\n=== LQ-PTQ Model Evaluation ===\")\n",
    "#quantized_model5.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "#quantized_accuracy = quantized_model5.evaluate(val_dataset)\n",
    "#print(f\"Quantized model5's Top 1 accuracy on the Imagenet validation set: {(quantized_accuracy[1] * 100):.2f}%\")\n",
    "\n",
    "print(\"Fisish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e71f63d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCTWrapper with a few lines of code.\n",
    "\n",
    "MCT can deliver competitive results across a wide range of tasks and network architectures. For more details, [check out the paper:](https://arxiv.org/abs/2109.09113).\n",
    "\n",
    "## Copyrights\n",
    "\n",
    "Copyright 2024 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

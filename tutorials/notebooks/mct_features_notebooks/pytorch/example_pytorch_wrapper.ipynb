{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a91956",
   "metadata": {},
   "source": [
    "# Model Compression Toolkit (MCT) Wrapper API Comprehensive Quantization Comparison\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_wrapper.ipynb)\n",
    "\n",
    "## Overview \n",
    "This notebook provides a comprehensive demonstration of the MCT (Model Compression Toolkit) Wrapper API functionality, showcasing five different quantization methods on a MobileNetV2 model. The tutorial systematically compares the implementation, performance characteristics, and accuracy trade-offs of each quantization approach: PTQ (Post-Training Quantization), PTQ with Mixed Precision, GPTQ (Gradient-based PTQ), GPTQ with Mixed Precision. Each method utilizes the unified MCTWrapper interface for consistent implementation and comparison.\n",
    "\n",
    "## Summary\n",
    "1. **Environment Setup**: Import required libraries and configure MCT with MobileNetV2 model\n",
    "2. **Dataset Preparation**: Load and prepare ImageNet validation dataset with representative data generation\n",
    "3. **PTQ Implementation**: Execute basic Post-Training Quantization with 8-bit precision and bias correction\n",
    "4. **PTQ + Mixed Precision**: Apply intelligent bit-width allocation based on layer sensitivity analysis (75% compression ratio)\n",
    "5. **GPTQ Implementation**: Perform gradient-based optimization with 5-epoch fine-tuning for enhanced accuracy\n",
    "6. **GPTQ + Mixed Precision**: Combine gradient optimization with mixed precision for optimal accuracy-compression trade-off\n",
    "7. **Performance Evaluation**: Comprehensive accuracy assessment and comparison across all quantization methods\n",
    "8. **Results Analysis**: Compare model sizes, inference accuracy, and quantization trade-offs\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ff6fa5-a840-4aa3-b9b2-b95187ceb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for PyTorch deep learning and data handling\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.datasets import ImageNet\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Generator, Any, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509ebef7-d1e3-4d24-aaa8-1fa40330072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 15:30:34.749157: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-16 15:30:34.750837: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-16 15:30:34.773264: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-16 15:30:34.773293: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-16 15:30:34.773325: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-16 15:30:34.779477: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-16 15:30:35.267805: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Configure system path to include MCT library for local development\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/wrapper/sonyfork/mct-model-optimization')\n",
    "\n",
    "#pip install -q tensorflow\n",
    "#import importlib\n",
    "#if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "#   !pip install model_compression_toolkit\n",
    "\n",
    "# Import Model Compression Toolkit (MCT) core functionality for PyTorch\n",
    "import model_compression_toolkit as mct\n",
    "from model_compression_toolkit.core import QuantizationErrorMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd23b21",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6451112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and setup ImageNet validation dataset if not already present\n",
    "if not os.path.isdir('imagenet'):\n",
    "    # Create directory and download required ImageNet files\n",
    "    os.system('mkdir imagenet')\n",
    "    os.system('wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz')\n",
    "    os.system('wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b43e6",
   "metadata": {},
   "source": [
    "### Representative dataset construction\n",
    "We show how to create a generator for the representative dataset, which is required for post-training quantization.\n",
    "\n",
    "The representative dataset is used for collecting statistics on the inference outputs of all layers in the model.\n",
    " \n",
    "In order to decide on the size of the representative dataset, we configure the batch size and the number of calibration iterations.\n",
    "This gives us the total number of samples that will be used during PTQ (batch_size x n_iter).\n",
    "In this example we set `batch_size = 50` and `n_iter = 10`, resulting in a total of 500 representative images.\n",
    "\n",
    "Please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9566bb3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2 weights and configure dataset transforms\n",
    "weights = MobileNet_V2_Weights.IMAGENET1K_V2  # Use ImageNet V2 pre-trained weights\n",
    "# Create ImageNet validation dataset with automatic preprocessing transforms\n",
    "dataset = ImageNet(root='./imagenet', split='val', transform=weights.transforms())\n",
    "\n",
    "# Configuration parameters for representative dataset generation\n",
    "default_batch_size: int = 10  # Batch size for quantization calibration data\n",
    "n_iter: int = 5               # Number of iterations to generate representative batches\n",
    "# Create DataLoader with shuffling for representative data diversity\n",
    "dataloader = DataLoader(dataset, batch_size=default_batch_size, shuffle=True)\n",
    "\n",
    "def representative_dataset_gen() -> Generator[List[torch.Tensor], None, None]:\n",
    "    \"\"\"\n",
    "    Generator function for representative dataset used in PyTorch quantization.\n",
    "    \n",
    "    This function provides calibration data that MCT uses to:\n",
    "    - Determine optimal quantization parameters for PyTorch models\n",
    "    - Calibrate activation ranges and thresholds\n",
    "    - Configure layer-specific quantization settings\n",
    "    \n",
    "    Yields:\n",
    "        List containing PyTorch tensors for model calibration\n",
    "    \"\"\"\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    for _ in range(n_iter):\n",
    "        # Extract image batch (ignore labels) and yield as list for MCT compatibility\n",
    "        yield [next(dataloader_iter)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d31427",
   "metadata": {},
   "source": [
    "## Model Evaluation Function\n",
    "Define a comprehensive evaluation function for PyTorch models that provides accurate performance measurement on the validation dataset with GPU acceleration support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956978a3-8817-46f0-8a2c-11c25a6fc063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: torch.nn.Module, testloader: DataLoader, mode: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate PyTorch model accuracy using a DataLoader with GPU acceleration.\n",
    "    \n",
    "    This function performs complete accuracy evaluation by:\n",
    "    - Moving model and data to available device (GPU/CPU)\n",
    "    - Running inference in evaluation mode (no gradient computation)\n",
    "    - Computing Top-1 accuracy across the entire validation set\n",
    "    - Providing progress tracking during evaluation\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to evaluate (float or quantized)\n",
    "        testloader: DataLoader containing validation dataset\n",
    "        mode: String identifier for logging (e.g., 'Float', 'PTQ_Pytorch')\n",
    "    \n",
    "    Returns:\n",
    "        float: Top-1 accuracy percentage\n",
    "    \"\"\"\n",
    "    # Determine best available device for inference\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Perform inference without gradient computation for efficiency\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass to get predictions\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate and display accuracy\n",
    "    val_acc = (100 * correct / total)\n",
    "    print(mode + ' Accuracy: %.2f%%' % val_acc)\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0ee68",
   "metadata": {},
   "source": [
    "## Model Post-Training quantization using MCTWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a04842-364c-466a-95d5-0c58c25730fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator to provide consistent logging and error handling for quantization functions\n",
    "def decorator(func: Callable[[torch.nn.Module], Tuple[bool, torch.nn.Module]]) -> Callable[[torch.nn.Module], Tuple[bool, torch.nn.Module]]:\n",
    "    \"\"\"\n",
    "    Wrapper decorator that provides:\n",
    "    - Consistent start/end logging for quantization operations\n",
    "    - Automatic error handling and program termination on failure\n",
    "    - Success/failure status tracking for all quantization methods\n",
    "    \n",
    "    Args:\n",
    "        func: Quantization function to be decorated\n",
    "    \n",
    "    Returns:\n",
    "        Wrapped function with enhanced logging and error handling\n",
    "    \"\"\"\n",
    "    def wrapper(*args: Any, **kwargs: Any) -> Tuple[bool, torch.nn.Module]:\n",
    "        print(f\"----------------- {func.__name__} Start ---------------\")\n",
    "        flag, result = func(*args, **kwargs)\n",
    "        print(f\"----------------- {func.__name__} End -----------------\")\n",
    "        if not flag:exit()\n",
    "        return flag, result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556d1d9",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ (Post-Training Quantization) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe46712",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Pytorch(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization (PTQ) on PyTorch model.\n",
    "    \n",
    "    PTQ for PyTorch provides:\n",
    "    - Fast quantization without model retraining\n",
    "    - Standard 8-bit integer quantization\n",
    "    - Efficient calibration using representative data\n",
    "    - Direct ONNX export for deployment\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch PTQ quantization\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    use_MCT_TPC = False               # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_MixP = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch PTQ\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # Quantization configuration parameters\n",
    "        ['activation_error_method', QuantizationErrorMethod.MSE, 'Error metric for activation quantization'],\n",
    "        ['weights_bias_correction', True, 'Enable bias correction for weights'],\n",
    "        ['z_threshold', float('inf'), 'Threshold for zero-point quantization'],\n",
    "        ['linear_collapsing', True, 'Enable linear layer collapsing optimization'],\n",
    "        ['residual_collapsing', True, 'Enable residual connection collapsing'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Pytorch.onnx', 'Path to save quantized model as ONNX']\n",
    "    ]\n",
    "\n",
    "    # Execute PyTorch PTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mctwrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6df571",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ + Mixed Precision Quantization (MixP) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8227d00b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Pytorch_MixP(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization with Mixed Precision (PTQ + MixP) on PyTorch model.\n",
    "    \n",
    "    Mixed Precision PTQ for PyTorch offers:\n",
    "    - Automatic bit-width selection per layer\n",
    "    - Optimal size/accuracy trade-off\n",
    "    - Resource-constrained quantization\n",
    "    - Advanced sensitivity analysis for PyTorch models\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch PTQ with mixed precision\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    use_MCT_TPC = False               # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_MixP = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch PTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision sensitivity analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based scores for layer importance ranking'],\n",
    "        \n",
    "        # Resource constraint configuration (more aggressive for PyTorch)\n",
    "        ['weights_compression_ratio', 0.5, 'Target compression ratio for model weights (50% reduction)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Pytorch_MixP.onnx', 'Path to save mixed precision quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute PyTorch mixed precision PTQ using MCTWrapper\n",
    "    wrapper = mct.wrapper.mctwrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d6a40",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ (Gradient-based PTQ) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85c9da67",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Pytorch(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization (GPTQ) on PyTorch model.\n",
    "    \n",
    "    GPTQ for PyTorch provides:\n",
    "    - Advanced gradient-based quantization optimization\n",
    "    - Fine-tuning during quantization process\n",
    "    - Superior accuracy preservation compared to PTQ\n",
    "    - Optimized parameter updates using representative data\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch GPTQ quantization\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    use_MCT_TPC = False               # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_MixP = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch GPTQ\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default Adam)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Pytorch.onnx', 'Path to save GPTQ quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute PyTorch GPTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mctwrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e757e27",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ + Mixed Precision Quantization (MixP) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2005ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Pytorch_MixP(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization with Mixed Precision (GPTQ + MixP).\n",
    "    \n",
    "    This advanced method combines:\n",
    "    - GPTQ: Gradient-based optimization for optimal quantization parameters\n",
    "    - Mixed Precision: Automatic bit-width selection for each layer\n",
    "    \n",
    "    Provides the best quantization results for PyTorch models with:\n",
    "    - Maximum accuracy preservation\n",
    "    - Optimal model size reduction\n",
    "    - Layer-wise precision optimization\n",
    "    - Advanced gradient-based calibration\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch GPTQ with mixed precision\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    use_MCT_TPC = False               # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_MixP = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch GPTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default Adam)'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision sensitivity analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based scores for layer importance'],\n",
    "        \n",
    "        # Resource constraint configuration\n",
    "        ['weights_compression_ratio', 0.5, 'Target compression ratio for model weights (50% reduction)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Pytorch_MixP.onnx', 'Path to save GPTQ+MixP quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute advanced PyTorch GPTQ+MixP quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mctwrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07704042",
   "metadata": {},
   "source": [
    "### Run model Post-Training Quantization\n",
    "Lastly, we quantize our model using MCTWrapper API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6eee799-c5e8-4004-b120-fa23d08da74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2 model with ImageNet weights for quantization experiments\n",
    "float_model = mobilenet_v2(weights=weights)\n",
    "\n",
    "# Create DataLoader for validation/evaluation with larger batch size for efficiency\n",
    "val_dataloader = DataLoader(dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3d5d54c-0686-4c81-a24b-d8c1d84deea2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PyTorch quantization experiments with different methods...\n",
      "----------------- PTQ_Pytorch Start ---------------\n",
      "Warning: The key is not found in the default parameters and will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Model Compression Toolkit:DepthwiseConv2D is not in model.\n",
      "Statistics Collection: 5it [00:08,  1.62s/it]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 102/102 [00:09<00:00, 10.44it/s]\n",
      "WARNING:Model Compression Toolkit:Attribute 'metadata' not found in the model or its submodules.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/mct_quantizers/pytorch/quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  threshold = torch.tensor(threshold, dtype=torch.float32).to(get_working_device())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Pytorch End -----------------\n",
      "----------------- PTQ_Pytorch_MixP Start ---------------\n",
      "Warning: The key is not found in the default parameters and will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Model Compression Toolkit:DepthwiseConv2D is not in model.\n",
      "WARNING:Model Compression Toolkit:DepthwiseConv2D is not in model.\n",
      "Statistics Collection: 5it [00:08,  1.62s/it]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 102/102 [00:28<00:00,  3.55it/s]\n",
      "53it [00:12,  4.41it/s]\n",
      "WARNING:Model Compression Toolkit:Attribute 'metadata' not found in the model or its submodules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/ubuntu/.local/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/5e0cdfbdc03e4c6c9e0e70524afada8d-pulp.mps -sec 60 -timeMode elapsed -branch -printingOptions all -solution /tmp/5e0cdfbdc03e4c6c9e0e70524afada8d-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 112 COLUMNS\n",
      "At line 961 RHS\n",
      "At line 1069 BOUNDS\n",
      "At line 1229 ENDATA\n",
      "Problem MODEL has 107 rows, 212 columns and 477 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "seconds was changed from 1e+100 to 60\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 0.216393 - 0.00 seconds\n",
      "Cgl0004I processed model has 53 rows, 157 columns (157 integer (157 of which binary)) and 261 elements\n",
      "Cbc0038I Initial state - 1 integers unsatisfied sum - 0.0159\n",
      "Cbc0038I Pass   1: suminf.    0.01590 (1) obj. 0.216393 iterations 7\n",
      "Cbc0038I Solution found of 0.40679\n",
      "Cbc0038I Before mini branch and bound, 156 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.00 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.387742\n",
      "Cbc0038I Reduced cost fixing fixed 31 variables on major pass 2\n",
      "Cbc0038I Pass   2: suminf.    0.01590 (1) obj. 0.216393 iterations 0\n",
      "Cbc0038I Pass   3: suminf.    0.09846 (1) obj. 0.387742 iterations 13\n",
      "Cbc0038I Pass   4: suminf.    0.92671 (3) obj. 0.387742 iterations 16\n",
      "Cbc0038I Pass   5: suminf.    0.39749 (1) obj. 0.387742 iterations 3\n",
      "Cbc0038I Pass   6: suminf.    0.43490 (1) obj. 0.355314 iterations 8\n",
      "Cbc0038I Pass   7: suminf.    1.10145 (3) obj. 0.387742 iterations 8\n",
      "Cbc0038I Pass   8: suminf.    1.10145 (3) obj. 0.387742 iterations 0\n",
      "Cbc0038I Pass   9: suminf.    0.34479 (1) obj. 0.387742 iterations 9\n",
      "Cbc0038I Pass  10: suminf.    0.11490 (1) obj. 0.343263 iterations 8\n",
      "Cbc0038I Pass  11: suminf.    1.08789 (3) obj. 0.387742 iterations 8\n",
      "Cbc0038I Pass  12: suminf.    0.13515 (1) obj. 0.35042 iterations 4\n",
      "Cbc0038I Pass  13: suminf.    0.32805 (1) obj. 0.387742 iterations 10\n",
      "Cbc0038I Pass  14: suminf.    0.65899 (3) obj. 0.387742 iterations 9\n",
      "Cbc0038I Solution found of 0.369478\n",
      "Cbc0038I Before mini branch and bound, 117 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 2 rows 22 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.369478 to 0.220624 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.21977\n",
      "Cbc0038I Reduced cost fixing fixed 85 variables on major pass 3\n",
      "Cbc0038I Pass  15: suminf.    0.01590 (1) obj. 0.216393 iterations 0\n",
      "Cbc0038I Pass  16: suminf.    0.03335 (1) obj. 0.21977 iterations 9\n",
      "Cbc0038I Pass  17: suminf.    0.58493 (4) obj. 0.21977 iterations 4\n",
      "Cbc0038I Pass  18: suminf.    0.42714 (3) obj. 0.21977 iterations 1\n",
      "Cbc0038I Pass  19: suminf.    0.02687 (1) obj. 0.21977 iterations 3\n",
      "Cbc0038I Pass  20: suminf.    0.01590 (1) obj. 0.217646 iterations 2\n",
      "Cbc0038I Pass  21: suminf.    0.00428 (1) obj. 0.218871 iterations 2\n",
      "Cbc0038I Pass  22: suminf.    0.00428 (1) obj. 0.218871 iterations 0\n",
      "Cbc0038I Pass  23: suminf.    0.00892 (1) obj. 0.21977 iterations 2\n",
      "Cbc0038I Pass  24: suminf.    0.00968 (1) obj. 0.219613 iterations 6\n",
      "Cbc0038I Pass  25: suminf.    0.00968 (1) obj. 0.219613 iterations 0\n",
      "Cbc0038I Pass  26: suminf.    0.01048 (1) obj. 0.21977 iterations 2\n",
      "Cbc0038I Pass  27: suminf.    1.16643 (4) obj. 0.21977 iterations 10\n",
      "Cbc0038I Pass  28: suminf.    0.56379 (2) obj. 0.21977 iterations 3\n",
      "Cbc0038I Pass  29: suminf.    0.08130 (1) obj. 0.219059 iterations 3\n",
      "Cbc0038I Pass  30: suminf.    0.08497 (1) obj. 0.21977 iterations 2\n",
      "Cbc0038I Pass  31: suminf.    0.87963 (3) obj. 0.21977 iterations 5\n",
      "Cbc0038I Pass  32: suminf.    0.87963 (3) obj. 0.21977 iterations 0\n",
      "Cbc0038I Pass  33: suminf.    0.08592 (1) obj. 0.21977 iterations 3\n",
      "Cbc0038I Pass  34: suminf.    0.08198 (1) obj. 0.219007 iterations 2\n",
      "Cbc0038I Pass  35: suminf.    0.85391 (3) obj. 0.21977 iterations 3\n",
      "Cbc0038I Pass  36: suminf.    0.85391 (3) obj. 0.21977 iterations 0\n",
      "Cbc0038I Pass  37: suminf.    0.09264 (1) obj. 0.21977 iterations 3\n",
      "Cbc0038I Pass  38: suminf.    0.08588 (1) obj. 0.21846 iterations 2\n",
      "Cbc0038I Pass  39: suminf.    0.40285 (3) obj. 0.21977 iterations 4\n",
      "Cbc0038I Pass  40: suminf.    0.40285 (3) obj. 0.21977 iterations 0\n",
      "Cbc0038I Pass  41: suminf.    0.07963 (1) obj. 0.21977 iterations 3\n",
      "Cbc0038I Pass  42: suminf.    0.07440 (1) obj. 0.218757 iterations 2\n",
      "Cbc0038I Pass  43: suminf.    1.39654 (4) obj. 0.21977 iterations 4\n",
      "Cbc0038I Pass  44: suminf.    0.54783 (2) obj. 0.21977 iterations 3\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 132 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 13 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.220624 to 0.218275 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.217704\n",
      "Cbc0038I Reduced cost fixing fixed 92 variables on major pass 4\n",
      "Cbc0038I Pass  44: suminf.    0.01590 (1) obj. 0.216393 iterations 0\n",
      "Cbc0038I Pass  45: suminf.    0.02267 (1) obj. 0.217704 iterations 8\n",
      "Cbc0038I Pass  46: suminf.    0.36331 (2) obj. 0.217704 iterations 1\n",
      "Cbc0038I Pass  47: suminf.    0.52043 (4) obj. 0.217704 iterations 1\n",
      "Cbc0038I Pass  48: suminf.    0.02400 (1) obj. 0.217599 iterations 2\n",
      "Cbc0038I Pass  49: suminf.    0.02454 (1) obj. 0.217704 iterations 1\n",
      "Cbc0038I Pass  50: suminf.    0.64797 (3) obj. 0.217704 iterations 2\n",
      "Cbc0038I Pass  51: suminf.    0.64797 (3) obj. 0.217704 iterations 0\n",
      "Cbc0038I Pass  52: suminf.    0.01055 (1) obj. 0.217704 iterations 2\n",
      "Cbc0038I Pass  53: suminf.    0.00630 (1) obj. 0.216881 iterations 1\n",
      "Cbc0038I Pass  54: suminf.    0.50337 (4) obj. 0.217704 iterations 4\n",
      "Cbc0038I Pass  55: suminf.    0.50337 (4) obj. 0.217704 iterations 0\n",
      "Cbc0038I Pass  56: suminf.    0.12630 (1) obj. 0.217204 iterations 2\n",
      "Cbc0038I Pass  57: suminf.    0.12888 (1) obj. 0.217704 iterations 1\n",
      "Cbc0038I Pass  58: suminf.    0.85947 (3) obj. 0.217704 iterations 1\n",
      "Cbc0038I Pass  59: suminf.    0.85947 (3) obj. 0.217704 iterations 0\n",
      "Cbc0038I Pass  60: suminf.    0.10671 (3) obj. 0.217704 iterations 3\n",
      "Cbc0038I Pass  61: suminf.    0.00360 (1) obj. 0.21767 iterations 1\n",
      "Cbc0038I Pass  62: suminf.    0.00377 (1) obj. 0.217704 iterations 1\n",
      "Cbc0038I Pass  63: suminf.    0.64797 (3) obj. 0.217704 iterations 2\n",
      "Cbc0038I Pass  64: suminf.    0.64797 (3) obj. 0.217704 iterations 0\n",
      "Cbc0038I Pass  65: suminf.    0.01055 (1) obj. 0.217704 iterations 2\n",
      "Cbc0038I Pass  66: suminf.    0.00630 (1) obj. 0.216881 iterations 1\n",
      "Cbc0038I Pass  67: suminf.    1.37185 (4) obj. 0.217704 iterations 4\n",
      "Cbc0038I Pass  68: suminf.    1.16378 (4) obj. 0.217704 iterations 1\n",
      "Cbc0038I Pass  69: suminf.    0.09750 (1) obj. 0.217236 iterations 2\n",
      "Cbc0038I Pass  70: suminf.    0.09992 (1) obj. 0.217704 iterations 1\n",
      "Cbc0038I Pass  71: suminf.    0.82648 (3) obj. 0.217704 iterations 1\n",
      "Cbc0038I Pass  72: suminf.    0.82648 (3) obj. 0.217704 iterations 0\n",
      "Cbc0038I Pass  73: suminf.    0.82648 (3) obj. 0.217704 iterations 0\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 136 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 11 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.218275 to 0.218161 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.21731\n",
      "Cbc0038I Reduced cost fixing fixed 96 variables on major pass 5\n",
      "Cbc0038I Pass  73: suminf.    0.01590 (1) obj. 0.216393 iterations 0\n",
      "Cbc0038I Pass  74: suminf.    0.02064 (1) obj. 0.21731 iterations 8\n",
      "Cbc0038I Pass  75: suminf.    0.01590 (1) obj. 0.216393 iterations 1\n",
      "Cbc0038I Pass  76: suminf.    0.00630 (1) obj. 0.217035 iterations 1\n",
      "Cbc0038I Pass  77: suminf.    0.00630 (1) obj. 0.217035 iterations 0\n",
      "Cbc0038I Pass  78: suminf.    0.00772 (1) obj. 0.21731 iterations 1\n",
      "Cbc0038I Pass  79: suminf.    0.18893 (3) obj. 0.21731 iterations 2\n",
      "Cbc0038I Pass  80: suminf.    0.09270 (1) obj. 0.217272 iterations 1\n",
      "Cbc0038I Pass  81: suminf.    0.09290 (1) obj. 0.21731 iterations 1\n",
      "Cbc0038I Pass  82: suminf.    0.37683 (3) obj. 0.21731 iterations 2\n",
      "Cbc0038I Pass  83: suminf.    0.37683 (3) obj. 0.21731 iterations 0\n",
      "Cbc0038I Pass  84: suminf.    0.10581 (1) obj. 0.21731 iterations 2\n",
      "Cbc0038I Pass  85: suminf.    0.10230 (1) obj. 0.216631 iterations 1\n",
      "Cbc0038I Pass  86: suminf.    0.00630 (1) obj. 0.217035 iterations 2\n",
      "Cbc0038I Pass  87: suminf.    0.00630 (1) obj. 0.217035 iterations 0\n",
      "Cbc0038I Pass  88: suminf.    0.00772 (1) obj. 0.21731 iterations 1\n",
      "Cbc0038I Pass  89: suminf.    0.06473 (1) obj. 0.21731 iterations 3\n",
      "Cbc0038I Pass  90: suminf.    0.06390 (1) obj. 0.217149 iterations 1\n",
      "Cbc0038I Pass  91: suminf.    0.06473 (1) obj. 0.21731 iterations 1\n",
      "Cbc0038I Pass  92: suminf.    0.06473 (1) obj. 0.21731 iterations 0\n",
      "Cbc0038I Pass  93: suminf.    0.14173 (1) obj. 0.21731 iterations 2\n",
      "Cbc0038I Pass  94: suminf.    0.14070 (1) obj. 0.217111 iterations 1\n",
      "Cbc0038I Pass  95: suminf.    0.14173 (1) obj. 0.21731 iterations 1\n",
      "Cbc0038I Pass  96: suminf.    0.90832 (3) obj. 0.21731 iterations 2\n",
      "Cbc0038I Pass  97: suminf.    0.09270 (1) obj. 0.217118 iterations 1\n",
      "Cbc0038I Pass  98: suminf.    0.09369 (1) obj. 0.21731 iterations 1\n",
      "Cbc0038I Pass  99: suminf.    0.57943 (3) obj. 0.21731 iterations 1\n",
      "Cbc0038I Pass 100: suminf.    0.09270 (1) obj. 0.217118 iterations 1\n",
      "Cbc0038I Pass 101: suminf.    0.91657 (3) obj. 0.21731 iterations 3\n",
      "Cbc0038I Pass 102: suminf.    0.91657 (3) obj. 0.21731 iterations 0\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 144 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 7 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.01 seconds)\n",
      "Cbc0038I After 0.01 seconds - Feasibility pump exiting with objective of 0.218161 - took 0.01 seconds\n",
      "Cbc0012I Integer solution of 0.21816144 found by feasibility pump after 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 3 columns\n",
      "Cbc0031I 5 added rows had average density of 14.6\n",
      "Cbc0013I At root node, 5 cuts changed objective from 0.2163934 to 0.21806356 in 12 passes\n",
      "Cbc0014I Cut generator 0 (Probing) - 32 row cuts average 2.6 elements, 2 column cuts (2 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 1 (Gomory) - 11 row cuts average 18.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 2 (Knapsack) - 23 row cuts average 8.3 elements, 0 column cuts (0 active)  in 0.001 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 3 (Clique) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 4 (MixedIntegerRounding2) - 9 row cuts average 15.6 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 5 (FlowCover) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 6 (TwoMirCuts) - 56 row cuts average 17.7 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0010I After 0 nodes, 1 on tree, 0.21816144 best solution, best possible 0.21808381 (0.02 seconds)\n",
      "Cbc0001I Search completed - best objective 0.2181614401633923, took 86 iterations and 2 nodes (0.02 seconds)\n",
      "Cbc0032I Strong branching done 12 times (32 iterations), fathomed 0 nodes and fixed 2 variables\n",
      "Cbc0035I Maximum depth 0, 96 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from 0.216393 to 0.218084\n",
      "Probing was tried 15 times and created 43 cuts of which 0 were active after adding rounds of cuts (0.001 seconds)\n",
      "Gomory was tried 13 times and created 11 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 13 times and created 26 cuts of which 0 were active after adding rounds of cuts (0.001 seconds)\n",
      "Clique was tried 12 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 13 times and created 9 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 12 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 13 times and created 64 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                0.21816144\n",
      "Enumerated nodes:               2\n",
      "Total iterations:               86\n",
      "Time (CPU seconds):             0.01\n",
      "Time (Wallclock seconds):       0.02\n",
      "\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.01   (Wallclock seconds):       0.02\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/mct_quantizers/pytorch/quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  threshold = torch.tensor(threshold, dtype=torch.float32).to(get_working_device())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Pytorch_MixP End -----------------\n",
      "----------------- GPTQ_Pytorch Start ---------------\n",
      "Warning: The key is not found in the default parameters and will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Model Compression Toolkit:DepthwiseConv2D is not in model.\n",
      "Statistics Collection: 5it [00:07,  1.56s/it]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 102/102 [00:10<00:00, 10.11it/s]\n",
      "Estimating representative dataset size: 5it [00:00, 34.74it/s]\n",
      "Hessian random iterations: 100%|██████████████████████████████████████████| 100/100 [00:14<00:00,  7.03it/s]\n",
      "Hessian random iterations: 100%|██████████████████████████████████████████| 100/100 [00:07<00:00, 12.55it/s]\n",
      "Running GPTQ optimization:   0%|                                                      | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:01,  2.66it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  4.56it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  6.02it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  7.03it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.33it/s]\u001b[A\n",
      "Running GPTQ optimization:  20%|█████████▏                                    | 1/5 [00:00<00:03,  1.21it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  9.52it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  9.76it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  9.53it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  9.55it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.85it/s]\u001b[A\n",
      "Running GPTQ optimization:  40%|██████████████████▍                           | 2/5 [00:01<00:01,  1.51it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  9.21it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  9.37it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  9.41it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  9.30it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.26it/s]\u001b[A\n",
      "Running GPTQ optimization:  60%|███████████████████████████▌                  | 3/5 [00:01<00:01,  1.65it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  9.80it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  9.49it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  9.78it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.09it/s]\u001b[A\n",
      "Running GPTQ optimization:  80%|████████████████████████████████████▊         | 4/5 [00:02<00:00,  1.72it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  9.92it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  9.79it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  9.85it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.84it/s]\u001b[A\n",
      "Running GPTQ optimization: 100%|██████████████████████████████████████████████| 5/5 [00:02<00:00,  1.68it/s]\u001b[A\n",
      "WARNING:Model Compression Toolkit:Attribute 'metadata' not found in the model or its submodules.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/mct_quantizers/pytorch/quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  threshold = torch.tensor(threshold, dtype=torch.float32).to(get_working_device())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Pytorch End -----------------\n",
      "----------------- GPTQ_Pytorch_MixP Start ---------------\n",
      "Warning: The key is not found in the default parameters and will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Model Compression Toolkit:DepthwiseConv2D is not in model.\n",
      "WARNING:Model Compression Toolkit:DepthwiseConv2D is not in model.\n",
      "Statistics Collection: 5it [00:07,  1.46s/it]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 102/102 [00:28<00:00,  3.63it/s]\n",
      "53it [00:12,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/ubuntu/.local/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/f8da0f372d3942ae8e348976277ba8cf-pulp.mps -sec 60 -timeMode elapsed -branch -printingOptions all -solution /tmp/f8da0f372d3942ae8e348976277ba8cf-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 112 COLUMNS\n",
      "At line 961 RHS\n",
      "At line 1069 BOUNDS\n",
      "At line 1229 ENDATA\n",
      "Problem MODEL has 107 rows, 212 columns and 477 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "seconds was changed from 1e+100 to 60\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 0.199109 - 0.00 seconds\n",
      "Cgl0004I processed model has 53 rows, 157 columns (157 integer (157 of which binary)) and 261 elements\n",
      "Cbc0038I Initial state - 1 integers unsatisfied sum - 0.1359\n",
      "Cbc0038I Pass   1: suminf.    0.13590 (1) obj. 0.199109 iterations 7\n",
      "Cbc0038I Solution found of 0.370153\n",
      "Cbc0038I Before mini branch and bound, 156 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 0 rows 0 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.00 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.35304\n",
      "Cbc0038I Reduced cost fixing fixed 34 variables on major pass 2\n",
      "Cbc0038I Pass   2: suminf.    0.13590 (1) obj. 0.199109 iterations 0\n",
      "Cbc0038I Pass   3: suminf.    0.08646 (1) obj. 0.35304 iterations 14\n",
      "Cbc0038I Pass   4: suminf.    0.47989 (3) obj. 0.35304 iterations 16\n",
      "Cbc0038I Pass   5: suminf.    0.42892 (1) obj. 0.35304 iterations 5\n",
      "Cbc0038I Pass   6: suminf.    0.44510 (1) obj. 0.349838 iterations 10\n",
      "Cbc0038I Pass   7: suminf.    1.09068 (3) obj. 0.35304 iterations 12\n",
      "Cbc0038I Pass   8: suminf.    0.10626 (1) obj. 0.35304 iterations 9\n",
      "Cbc0038I Pass   9: suminf.    0.24190 (1) obj. 0.326191 iterations 9\n",
      "Cbc0038I Pass  10: suminf.    0.30838 (4) obj. 0.35304 iterations 18\n",
      "Cbc0038I Pass  11: suminf.    0.04119 (2) obj. 0.35304 iterations 8\n",
      "Cbc0038I Pass  12: suminf.    0.13470 (1) obj. 0.301825 iterations 10\n",
      "Cbc0038I Pass  13: suminf.    0.39344 (1) obj. 0.35304 iterations 10\n",
      "Cbc0038I Pass  14: suminf.    0.30721 (3) obj. 0.35304 iterations 9\n",
      "Cbc0038I Pass  15: suminf.    0.30721 (3) obj. 0.35304 iterations 0\n",
      "Cbc0038I Pass  16: suminf.    0.39195 (1) obj. 0.292011 iterations 5\n",
      "Cbc0038I Pass  17: suminf.    0.29974 (1) obj. 0.35304 iterations 10\n",
      "Cbc0038I Pass  18: suminf.    0.43891 (3) obj. 0.35304 iterations 8\n",
      "Cbc0038I Pass  19: suminf.    0.35415 (1) obj. 0.350791 iterations 5\n",
      "Cbc0038I Pass  20: suminf.    0.36551 (1) obj. 0.35304 iterations 9\n",
      "Cbc0038I Pass  21: suminf.    1.06374 (3) obj. 0.35304 iterations 6\n",
      "Cbc0038I Pass  22: suminf.    0.48420 (1) obj. 0.35304 iterations 10\n",
      "Cbc0038I Pass  23: suminf.    0.48915 (1) obj. 0.347764 iterations 6\n",
      "Cbc0038I Pass  24: suminf.    0.68606 (3) obj. 0.35304 iterations 9\n",
      "Cbc0038I Pass  25: suminf.    0.36630 (1) obj. 0.338188 iterations 4\n",
      "Cbc0038I Pass  26: suminf.    0.44133 (1) obj. 0.35304 iterations 8\n",
      "Cbc0038I Pass  27: suminf.    0.66032 (3) obj. 0.35304 iterations 10\n",
      "Cbc0038I Pass  28: suminf.    0.66032 (3) obj. 0.35304 iterations 1\n",
      "Cbc0038I Pass  29: suminf.    0.26143 (1) obj. 0.301937 iterations 5\n",
      "Cbc0038I Pass  30: suminf.    0.00326 (1) obj. 0.35304 iterations 5\n",
      "Cbc0038I Pass  31: suminf.    0.20961 (4) obj. 0.35304 iterations 16\n",
      "Cbc0038I Rounding solution of 0.353684 is better than previous of 0.370153\n",
      "\n",
      "Cbc0038I Before mini branch and bound, 93 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 4 rows 37 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.353684 to 0.202501 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.201815\n",
      "Cbc0038I Reduced cost fixing fixed 87 variables on major pass 3\n",
      "Cbc0038I Pass  31: suminf.    0.13590 (1) obj. 0.199109 iterations 0\n",
      "Cbc0038I Pass  32: suminf.    0.14957 (1) obj. 0.201815 iterations 9\n",
      "Cbc0038I Pass  33: suminf.    0.08790 (1) obj. 0.20023 iterations 4\n",
      "Cbc0038I Pass  34: suminf.    0.08790 (1) obj. 0.20023 iterations 0\n",
      "Cbc0038I Pass  35: suminf.    0.09591 (1) obj. 0.201815 iterations 2\n",
      "Cbc0038I Pass  36: suminf.    0.86481 (3) obj. 0.201815 iterations 3\n",
      "Cbc0038I Pass  37: suminf.    0.86481 (3) obj. 0.201815 iterations 1\n",
      "Cbc0038I Pass  38: suminf.    0.10220 (1) obj. 0.201815 iterations 3\n",
      "Cbc0038I Pass  39: suminf.    0.09548 (1) obj. 0.200483 iterations 2\n",
      "Cbc0038I Pass  40: suminf.    0.91809 (3) obj. 0.201815 iterations 2\n",
      "Cbc0038I Pass  41: suminf.    0.91809 (3) obj. 0.201815 iterations 0\n",
      "Cbc0038I Pass  42: suminf.    0.09081 (1) obj. 0.201815 iterations 3\n",
      "Cbc0038I Pass  43: suminf.    0.08588 (1) obj. 0.200838 iterations 2\n",
      "Cbc0038I Pass  44: suminf.    0.97571 (3) obj. 0.201815 iterations 2\n",
      "Cbc0038I Pass  45: suminf.    0.97571 (3) obj. 0.201815 iterations 0\n",
      "Cbc0038I Pass  46: suminf.    0.08677 (1) obj. 0.201815 iterations 3\n",
      "Cbc0038I Pass  47: suminf.    0.08318 (1) obj. 0.201103 iterations 2\n",
      "Cbc0038I Pass  48: suminf.    0.98293 (3) obj. 0.201815 iterations 5\n",
      "Cbc0038I Pass  49: suminf.    0.09330 (1) obj. 0.201671 iterations 2\n",
      "Cbc0038I Pass  50: suminf.    0.09403 (1) obj. 0.201815 iterations 2\n",
      "Cbc0038I Pass  51: suminf.    0.86886 (3) obj. 0.201815 iterations 4\n",
      "Cbc0038I Pass  52: suminf.    0.10290 (1) obj. 0.201316 iterations 1\n",
      "Cbc0038I Pass  53: suminf.    0.10542 (1) obj. 0.201815 iterations 2\n",
      "Cbc0038I Pass  54: suminf.    0.24576 (3) obj. 0.201815 iterations 3\n",
      "Cbc0038I Pass  55: suminf.    0.71923 (3) obj. 0.201815 iterations 5\n",
      "Cbc0038I Pass  56: suminf.    0.71923 (3) obj. 0.201815 iterations 0\n",
      "Cbc0038I Pass  57: suminf.    0.13668 (1) obj. 0.201815 iterations 3\n",
      "Cbc0038I Pass  58: suminf.    0.13170 (1) obj. 0.200829 iterations 2\n",
      "Cbc0038I Pass  59: suminf.    1.10303 (3) obj. 0.201815 iterations 2\n",
      "Cbc0038I Pass  60: suminf.    0.14130 (1) obj. 0.200551 iterations 1\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 132 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 13 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.202501 to 0.202169 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.201003\n",
      "Cbc0038I Reduced cost fixing fixed 90 variables on major pass 4\n",
      "Cbc0038I Pass  60: suminf.    0.13590 (1) obj. 0.199109 iterations 0\n",
      "Cbc0038I Pass  61: suminf.    0.14547 (1) obj. 0.201003 iterations 9\n",
      "Cbc0038I Pass  62: suminf.    0.21560 (1) obj. 0.201003 iterations 2\n",
      "Cbc0038I Pass  63: suminf.    0.21270 (1) obj. 0.200428 iterations 2\n",
      "Cbc0038I Pass  64: suminf.    0.21560 (1) obj. 0.201003 iterations 2\n",
      "Cbc0038I Pass  65: suminf.    0.89525 (3) obj. 0.201003 iterations 3\n",
      "Cbc0038I Pass  66: suminf.    0.89525 (3) obj. 0.201003 iterations 0\n",
      "Cbc0038I Pass  67: suminf.    0.22661 (1) obj. 0.201003 iterations 3\n",
      "Cbc0038I Pass  68: suminf.    0.22230 (1) obj. 0.200151 iterations 2\n",
      "Cbc0038I Pass  69: suminf.    0.18390 (1) obj. 0.200916 iterations 1\n",
      "Cbc0038I Pass  70: suminf.    0.18390 (1) obj. 0.200916 iterations 0\n",
      "Cbc0038I Pass  71: suminf.    0.18434 (1) obj. 0.201003 iterations 2\n",
      "Cbc0038I Pass  72: suminf.    0.67273 (3) obj. 0.201003 iterations 2\n",
      "Cbc0038I Pass  73: suminf.    0.52465 (3) obj. 0.201003 iterations 3\n",
      "Cbc0038I Pass  74: suminf.    0.52465 (3) obj. 0.201003 iterations 0\n",
      "Cbc0038I Pass  75: suminf.    0.09220 (1) obj. 0.201003 iterations 3\n",
      "Cbc0038I Pass  76: suminf.    0.08790 (1) obj. 0.200153 iterations 2\n",
      "Cbc0038I Pass  77: suminf.    0.34095 (3) obj. 0.201003 iterations 2\n",
      "Cbc0038I Pass  78: suminf.    0.34095 (3) obj. 0.201003 iterations 0\n",
      "Cbc0038I Pass  79: suminf.    0.10320 (1) obj. 0.201003 iterations 3\n",
      "Cbc0038I Pass  80: suminf.    0.09750 (1) obj. 0.199875 iterations 2\n",
      "Cbc0038I Pass  81: suminf.    0.31777 (3) obj. 0.201003 iterations 2\n",
      "Cbc0038I Pass  82: suminf.    0.18390 (1) obj. 0.200916 iterations 1\n",
      "Cbc0038I Pass  83: suminf.    0.18434 (1) obj. 0.201003 iterations 2\n",
      "Cbc0038I Pass  84: suminf.    0.28723 (3) obj. 0.201003 iterations 4\n",
      "Cbc0038I Pass  85: suminf.    0.25646 (3) obj. 0.201003 iterations 1\n",
      "Cbc0038I Pass  86: suminf.    0.13633 (1) obj. 0.201003 iterations 3\n",
      "Cbc0038I Pass  87: suminf.    0.13118 (1) obj. 0.199983 iterations 2\n",
      "Cbc0038I Pass  88: suminf.    0.84196 (3) obj. 0.201003 iterations 3\n",
      "Cbc0038I Pass  89: suminf.    0.21960 (1) obj. 0.200416 iterations 1\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 140 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 9 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.01 seconds)\n",
      "Cbc0038I After 0.01 seconds - Feasibility pump exiting with objective of 0.202169 - took 0.01 seconds\n",
      "Cbc0012I Integer solution of 0.20216887 found by feasibility pump after 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 4 columns\n",
      "Cbc0006I The LP relaxation is infeasible or too expensive\n",
      "Cbc0031I 1 added rows had average density of 15\n",
      "Cbc0013I At root node, 1 cuts changed objective from 0.19910924 to 0.20144138 in 2 passes\n",
      "Cbc0014I Cut generator 0 (Probing) - 1 row cuts average 0.0 elements, 2 column cuts (2 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 1 (Gomory) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 2 (Knapsack) - 1 row cuts average 19.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 3 (Clique) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 4 (MixedIntegerRounding2) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 5 (FlowCover) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 6 (TwoMirCuts) - 4 row cuts average 24.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0001I Search completed - best objective 0.2021688739112167, took 5 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0035I Maximum depth 0, 96 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from 0.199109 to 0.201441\n",
      "Probing was tried 2 times and created 3 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 1 times and created 1 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 1 times and created 4 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                0.20216887\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               5\n",
      "Time (CPU seconds):             0.01\n",
      "Time (Wallclock seconds):       0.01\n",
      "\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.01   (Wallclock seconds):       0.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hessian random iterations: 100%|██████████████████████████████████████████| 100/100 [00:14<00:00,  6.95it/s]\n",
      "Hessian random iterations: 100%|██████████████████████████████████████████| 100/100 [00:08<00:00, 12.28it/s]\n",
      "Running GPTQ optimization:   0%|                                                      | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  8.13it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  7.82it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  8.34it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  8.55it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.23it/s]\u001b[A\n",
      "Running GPTQ optimization:  20%|█████████▏                                    | 1/5 [00:00<00:02,  1.64it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  8.70it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  9.09it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  8.87it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  9.01it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.18it/s]\u001b[A\n",
      "Running GPTQ optimization:  40%|██████████████████▍                           | 2/5 [00:01<00:01,  1.73it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  8.85it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  8.84it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  7.81it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  8.23it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.50it/s]\u001b[A\n",
      "Running GPTQ optimization:  60%|███████████████████████████▌                  | 3/5 [00:01<00:01,  1.70it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  9.41it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  9.16it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  8.75it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  8.51it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.47it/s]\u001b[A\n",
      "Running GPTQ optimization:  80%|████████████████████████████████████▊         | 4/5 [00:02<00:00,  1.66it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  8.67it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  8.81it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  9.00it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  8.93it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.47it/s]\u001b[A\n",
      "Running GPTQ optimization: 100%|██████████████████████████████████████████████| 5/5 [00:02<00:00,  1.68it/s]\u001b[A\n",
      "WARNING:Model Compression Toolkit:Attribute 'metadata' not found in the model or its submodules.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/mct_quantizers/pytorch/quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  threshold = torch.tensor(threshold, dtype=torch.float32).to(get_working_device())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Pytorch_MixP End -----------------\n",
      "All PyTorch quantization methods completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Execute all PyTorch quantization methods on the same base model for comparison\n",
    "print(\"Starting PyTorch quantization experiments with different methods...\")\n",
    "\n",
    "# 1. Basic Post-Training Quantization for PyTorch\n",
    "flag, quantized_model = PTQ_Pytorch(float_model)\n",
    "\n",
    "# 2. PTQ with Mixed Precision (optimized size/accuracy trade-off for PyTorch)\n",
    "flag, quantized_model2 = PTQ_Pytorch_MixP(float_model)\n",
    "\n",
    "# 3. Gradient-based PTQ (improved accuracy through fine-tuning for PyTorch)\n",
    "flag, quantized_model3 = GPTQ_Pytorch(float_model)\n",
    "\n",
    "# 4. GPTQ with Mixed Precision (best accuracy with optimal compression for PyTorch)\n",
    "flag, quantized_model4 = GPTQ_Pytorch_MixP(float_model)\n",
    "\n",
    "print(\"All PyTorch quantization methods completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2172853",
   "metadata": {},
   "source": [
    "## Models evaluation\n",
    "In order to evaluate our models, we first need to load the validation dataset. As before, please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19492a4c-6ec7-4643-82c0-ce448ba0b8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PyTorch model evaluation phase...\n",
      "This evaluation will test all quantized models against the validation dataset\n",
      "\n",
      "=== Original PyTorch Model Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1000/1000 [04:12<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float Accuracy: 72.14%\n",
      "\n",
      "=== PyTorch PTQ Model Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1000/1000 [04:00<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTQ_Pytorch Accuracy: 71.86%\n",
      "\n",
      "=== PyTorch PTQ + Mixed Precision Model Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████▌                                                      | 199/1000 [00:47<03:10,  4.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Evaluate PTQ + Mixed Precision PyTorch model accuracy\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== PyTorch PTQ + Mixed Precision Model Evaluation ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized_model2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPTQ_Pytorch_MixP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Evaluate GPTQ quantized PyTorch model accuracy\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== PyTorch GPTQ Model Evaluation ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, testloader, mode)\u001b[0m\n\u001b[1;32m     30\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Forward pass to get predictions\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/wrapper/sonyfork/mct-model-optimization/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py:377\u001b[0m, in \u001b[0;36mPytorchModel.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    374\u001b[0m use_activation_quantization, activation_quantization_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_activation_quantization_fn(node)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# Run node operation and fetch outputs\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m out_tensors_of_n, out_tensors_of_n_float \u001b[38;5;241m=\u001b[39m \u001b[43m_run_operation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43mop_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43mquantize_node_activation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation_quantization_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m                                                          \u001b[49m\u001b[43muse_activation_quantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_activation_quantization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m node_to_output_tensors_dict\u001b[38;5;241m.\u001b[39mupdate({node: out_tensors_of_n})\n\u001b[1;32m    384\u001b[0m node_to_output_tensors_dict_float\u001b[38;5;241m.\u001b[39mupdate({node: out_tensors_of_n_float})\n",
      "File \u001b[0;32m~/wrapper/sonyfork/mct-model-optimization/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py:155\u001b[0m, in \u001b[0;36m_run_operation\u001b[0;34m(n, input_tensors, op_func, quantize_node_activation_fn, use_activation_quantization)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m         merged_inputs, functional_kwargs \u001b[38;5;241m=\u001b[39m _merge_inputs(n, input_tensors, op_call_args, functional_kwargs\u001b[38;5;241m.\u001b[39mcopy(),\n\u001b[1;32m    154\u001b[0m                                                          tensor_input_allocs\u001b[38;5;241m=\u001b[39m_tensor_input_allocs)\n\u001b[0;32m--> 155\u001b[0m         out_tensors_of_n_float \u001b[38;5;241m=\u001b[39m \u001b[43mop_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmerged_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunctional_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Add a fake quant node if the node has an activation threshold.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m out_tensors_of_n \u001b[38;5;241m=\u001b[39m out_tensors_of_n_float\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mct_quantizers/pytorch/quantize_wrapper.py:236\u001b[0m, in \u001b[0;36mPytorchQuantizationWrapper.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m         quantized_weight \u001b[38;5;241m=\u001b[39m quantizer(unquantized_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m         quantized_weight \u001b[38;5;241m=\u001b[39m \u001b[43mquantizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43munquantized_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     quantized_weights\u001b[38;5;241m.\u001b[39mupdate({name: quantized_weight})\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_quantize_weights(quantized_weights)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mct_quantizers/pytorch/quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py:139\u001b[0m, in \u001b[0;36mWeightsSymmetricInferableQuantizer.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mper_channel:\n\u001b[1;32m    138\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfake_quantize_per_channel_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchannel_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mquant_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_quantized_domain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mquant_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_quantized_domain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# PyTorch Model Evaluation and Accuracy Comparison\n",
    "print(\"Starting PyTorch model evaluation phase...\")\n",
    "print(\"This evaluation will test all quantized models against the validation dataset\")\n",
    "\n",
    "# Evaluate original floating-point PyTorch model accuracy\n",
    "print(\"\\n=== Original PyTorch Model Evaluation ===\")\n",
    "evaluate(float_model, val_dataloader, 'Float')\n",
    "\n",
    "# Evaluate PTQ quantized PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch PTQ Model Evaluation ===\")\n",
    "evaluate(quantized_model, val_dataloader, 'PTQ_Pytorch')\n",
    "\n",
    "# Evaluate PTQ + Mixed Precision PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch PTQ + Mixed Precision Model Evaluation ===\")\n",
    "evaluate(quantized_model2, val_dataloader, 'PTQ_Pytorch_MixP')\n",
    "\n",
    "# Evaluate GPTQ quantized PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch GPTQ Model Evaluation ===\")\n",
    "evaluate(quantized_model3, val_dataloader, 'GPTQ_Pytorch')\n",
    "\n",
    "# Evaluate GPTQ + Mixed Precision PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch GPTQ + Mixed Precision Model Evaluation ===\")\n",
    "evaluate(quantized_model4, val_dataloader, 'GPTQ_Pytorch_MixP')\n",
    "print(\"Fisish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3905b593",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCTWrapper with a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37baa56",
   "metadata": {},
   "source": [
    "Copyright 2024 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

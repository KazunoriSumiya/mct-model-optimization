{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a91956",
   "metadata": {},
   "source": [
    "# Model Compression Toolkit (MCT) Wrapper API Comprehensive Quantization Comparison (pytorch)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/SonySemiconductorSolutions/mct-model-optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_wrapper.ipynb)\n",
    "\n",
    "## Overview \n",
    "This notebook provides a comprehensive demonstration of the MCT (Model Compression Toolkit) Wrapper API functionality, showcasing five different quantization methods on a MobileNetV2 model. The tutorial systematically compares the implementation, performance characteristics, and accuracy trade-offs of each quantization approach: PTQ (Post-Training Quantization), PTQ with Mixed Precision, GPTQ (Gradient-based PTQ), GPTQ with Mixed Precision. Each method utilizes the unified MCTWrapper interface for consistent implementation and comparison.\n",
    "\n",
    "## Summary\n",
    "1. **Environment Setup**: Import required libraries and configure MCT with MobileNetV2 model\n",
    "2. **Dataset Preparation**: Load and prepare ImageNet validation dataset with representative data generation\n",
    "3. **PTQ Implementation**: Execute basic Post-Training Quantization with 8-bit precision and bias correction\n",
    "4. **PTQ + Mixed Precision**: Apply intelligent bit-width allocation based on layer sensitivity analysis (75% compression ratio)\n",
    "5. **GPTQ Implementation**: Perform gradient-based optimization with 5-epoch fine-tuning for enhanced accuracy\n",
    "6. **GPTQ + Mixed Precision**: Combine gradient optimization with mixed precision for optimal accuracy-compression trade-off\n",
    "7. **Performance Evaluation**: Comprehensive accuracy assessment and comparison across all quantization methods\n",
    "8. **Results Analysis**: Compare model sizes, inference accuracy, and quantization trade-offs\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff6fa5-a840-4aa3-b9b2-b95187ceb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for PyTorch deep learning and data handling\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.datasets import ImageNet\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Generator, Any, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ebef7-d1e3-4d24-aaa8-1fa40330072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure system path to include MCT library for local development\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/wrapper/sonyfork/mct-model-optimization')\n",
    "\n",
    "#pip install -q tensorflow\n",
    "#import importlib\n",
    "#if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "#   !pip install model_compression_toolkit\n",
    "\n",
    "# Import Model Compression Toolkit (MCT) core functionality for PyTorch\n",
    "import model_compression_toolkit as mct\n",
    "from model_compression_toolkit.core import QuantizationErrorMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd23b21",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6451112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and setup ImageNet validation dataset if not already present\n",
    "if not os.path.isdir('imagenet'):\n",
    "    # Create directory and download required ImageNet files\n",
    "    os.system('mkdir imagenet')\n",
    "    os.system('wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz')\n",
    "    os.system('wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b43e6",
   "metadata": {},
   "source": [
    "### Representative dataset construction\n",
    "We show how to create a generator for the representative dataset, which is required for post-training quantization.\n",
    "\n",
    "The representative dataset is used for collecting statistics on the inference outputs of all layers in the model.\n",
    " \n",
    "In order to decide on the size of the representative dataset, we configure the batch size and the number of calibration iterations.\n",
    "This gives us the total number of samples that will be used during PTQ (batch_size x n_iter).\n",
    "In this example we set `batch_size = 50` and `n_iter = 10`, resulting in a total of 500 representative images.\n",
    "\n",
    "Please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566bb3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2 weights and configure dataset transforms\n",
    "weights = MobileNet_V2_Weights.IMAGENET1K_V2  # Use ImageNet V2 pre-trained weights\n",
    "# Create ImageNet validation dataset with automatic preprocessing transforms\n",
    "dataset = ImageNet(root='./imagenet', split='val', transform=weights.transforms())\n",
    "\n",
    "# Configuration parameters for representative dataset generation\n",
    "default_batch_size: int = 10  # Batch size for quantization calibration data\n",
    "n_iter: int = 5               # Number of iterations to generate representative batches\n",
    "# Create DataLoader with shuffling for representative data diversity\n",
    "dataloader = DataLoader(dataset, batch_size=default_batch_size, shuffle=True)\n",
    "\n",
    "def representative_dataset_gen() -> Generator[List[torch.Tensor], None, None]:\n",
    "    \"\"\"\n",
    "    Generator function for representative dataset used in PyTorch quantization.\n",
    "    \n",
    "    This function provides calibration data that MCT uses to:\n",
    "    - Determine optimal quantization parameters for PyTorch models\n",
    "    - Calibrate activation ranges and thresholds\n",
    "    - Configure layer-specific quantization settings\n",
    "    \n",
    "    Yields:\n",
    "        List containing PyTorch tensors for model calibration\n",
    "    \"\"\"\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    for _ in range(n_iter):\n",
    "        # Extract image batch (ignore labels) and yield as list for MCT compatibility\n",
    "        yield [next(dataloader_iter)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d31427",
   "metadata": {},
   "source": [
    "## Model Evaluation Function\n",
    "Define a comprehensive evaluation function for PyTorch models that provides accurate performance measurement on the validation dataset with GPU acceleration support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956978a3-8817-46f0-8a2c-11c25a6fc063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: torch.nn.Module, testloader: DataLoader, mode: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate PyTorch model accuracy using a DataLoader with GPU acceleration.\n",
    "    \n",
    "    This function performs complete accuracy evaluation by:\n",
    "    - Moving model and data to available device (GPU/CPU)\n",
    "    - Running inference in evaluation mode (no gradient computation)\n",
    "    - Computing Top-1 accuracy across the entire validation set\n",
    "    - Providing progress tracking during evaluation\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to evaluate (float or quantized)\n",
    "        testloader: DataLoader containing validation dataset\n",
    "        mode: String identifier for logging (e.g., 'Float', 'PTQ_Pytorch')\n",
    "    \n",
    "    Returns:\n",
    "        float: Top-1 accuracy percentage\n",
    "    \"\"\"\n",
    "    # Determine best available device for inference\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Perform inference without gradient computation for efficiency\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass to get predictions\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate and display accuracy\n",
    "    val_acc = (100 * correct / total)\n",
    "    print(mode + ' Accuracy: %.2f%%' % val_acc)\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0ee68",
   "metadata": {},
   "source": [
    "## Model Post-Training quantization using MCTWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a04842-364c-466a-95d5-0c58c25730fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator to provide consistent logging and error handling for quantization functions\n",
    "def decorator(func: Callable[[torch.nn.Module], Tuple[bool, torch.nn.Module]]) -> Callable[[torch.nn.Module], Tuple[bool, torch.nn.Module]]:\n",
    "    \"\"\"\n",
    "    Wrapper decorator that provides:\n",
    "    - Consistent start/end logging for quantization operations\n",
    "    - Automatic error handling and program termination on failure\n",
    "    - Success/failure status tracking for all quantization methods\n",
    "    \n",
    "    Args:\n",
    "        func: Quantization function to be decorated\n",
    "    \n",
    "    Returns:\n",
    "        Wrapped function with enhanced logging and error handling\n",
    "    \"\"\"\n",
    "    def wrapper(*args: Any, **kwargs: Any) -> Tuple[bool, torch.nn.Module]:\n",
    "        print(f\"----------------- {func.__name__} Start ---------------\")\n",
    "        flag, result = func(*args, **kwargs)\n",
    "        print(f\"----------------- {func.__name__} End -----------------\")\n",
    "        if not flag:exit()\n",
    "        return flag, result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556d1d9",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ (Post-Training Quantization) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe46712",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Pytorch(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization (PTQ) on PyTorch model.\n",
    "    \n",
    "    PTQ for PyTorch provides:\n",
    "    - Fast quantization without model retraining\n",
    "    - Standard 8-bit integer quantization\n",
    "    - Efficient calibration using representative data\n",
    "    - Direct ONNX export for deployment\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch PTQ quantization\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    use_MCT_TPC = False               # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_MixP = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch PTQ\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # Quantization configuration parameters\n",
    "        ['activation_error_method', QuantizationErrorMethod.MSE, 'Error metric for activation quantization'],\n",
    "        ['weights_bias_correction', True, 'Enable bias correction for weights'],\n",
    "        ['z_threshold', float('inf'), 'Threshold for zero-point quantization'],\n",
    "        ['linear_collapsing', True, 'Enable linear layer collapsing optimization'],\n",
    "        ['residual_collapsing', True, 'Enable residual connection collapsing'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Pytorch.onnx', 'Path to save quantized model as ONNX']\n",
    "    ]\n",
    "\n",
    "    # Execute PyTorch PTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6df571",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ + Mixed Precision Quantization (MixP) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227d00b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Pytorch_MixP(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization with Mixed Precision (PTQ + MixP) on PyTorch model.\n",
    "    \n",
    "    Mixed Precision PTQ for PyTorch offers:\n",
    "    - Automatic bit-width selection per layer\n",
    "    - Optimal size/accuracy trade-off\n",
    "    - Resource-constrained quantization\n",
    "    - Advanced sensitivity analysis for PyTorch models\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch PTQ with mixed precision\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    use_MCT_TPC = False               # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_MixP = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch PTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision sensitivity analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based scores for layer importance ranking'],\n",
    "        \n",
    "        # Resource constraint configuration (more aggressive for PyTorch)\n",
    "        ['weights_compression_ratio', 0.5, 'Target compression ratio for model weights (50% reduction)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Pytorch_MixP.onnx', 'Path to save mixed precision quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute PyTorch mixed precision PTQ using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d6a40",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ (Gradient-based PTQ) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9da67",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Pytorch(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization (GPTQ) on PyTorch model.\n",
    "    \n",
    "    GPTQ for PyTorch provides:\n",
    "    - Advanced gradient-based quantization optimization\n",
    "    - Fine-tuning during quantization process\n",
    "    - Superior accuracy preservation compared to PTQ\n",
    "    - Optimized parameter updates using representative data\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch GPTQ quantization\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    use_MCT_TPC = False               # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_MixP = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch GPTQ\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default Adam)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Pytorch.onnx', 'Path to save GPTQ quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute PyTorch GPTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e757e27",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ + Mixed Precision Quantization (MixP) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Pytorch_MixP(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization with Mixed Precision (GPTQ + MixP).\n",
    "    \n",
    "    This advanced method combines:\n",
    "    - GPTQ: Gradient-based optimization for optimal quantization parameters\n",
    "    - Mixed Precision: Automatic bit-width selection for each layer\n",
    "    \n",
    "    Provides the best quantization results for PyTorch models with:\n",
    "    - Maximum accuracy preservation\n",
    "    - Optimal model size reduction\n",
    "    - Layer-wise precision optimization\n",
    "    - Advanced gradient-based calibration\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch GPTQ with mixed precision\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    use_MCT_TPC = False               # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_MixP = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch GPTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default Adam)'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision sensitivity analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based scores for layer importance'],\n",
    "        \n",
    "        # Resource constraint configuration\n",
    "        ['weights_compression_ratio', 0.5, 'Target compression ratio for model weights (50% reduction)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Pytorch_MixP.onnx', 'Path to save GPTQ+MixP quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute advanced PyTorch GPTQ+MixP quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_MCT_TPC, use_MixP, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07704042",
   "metadata": {},
   "source": [
    "### Run model Post-Training Quantization\n",
    "Lastly, we quantize our model using MCTWrapper API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eee799-c5e8-4004-b120-fa23d08da74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2 model with ImageNet weights for quantization experiments\n",
    "float_model = mobilenet_v2(weights=weights)\n",
    "\n",
    "# Create DataLoader for validation/evaluation with larger batch size for efficiency\n",
    "val_dataloader = DataLoader(dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5d54c-0686-4c81-a24b-d8c1d84deea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute all PyTorch quantization methods on the same base model for comparison\n",
    "print(\"Starting PyTorch quantization experiments with different methods...\")\n",
    "\n",
    "# 1. Basic Post-Training Quantization for PyTorch\n",
    "flag, quantized_model = PTQ_Pytorch(float_model)\n",
    "\n",
    "# 2. PTQ with Mixed Precision (optimized size/accuracy trade-off for PyTorch)\n",
    "flag, quantized_model2 = PTQ_Pytorch_MixP(float_model)\n",
    "\n",
    "# 3. Gradient-based PTQ (improved accuracy through fine-tuning for PyTorch)\n",
    "flag, quantized_model3 = GPTQ_Pytorch(float_model)\n",
    "\n",
    "# 4. GPTQ with Mixed Precision (best accuracy with optimal compression for PyTorch)\n",
    "flag, quantized_model4 = GPTQ_Pytorch_MixP(float_model)\n",
    "\n",
    "print(\"All PyTorch quantization methods completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2172853",
   "metadata": {},
   "source": [
    "## Models evaluation\n",
    "In order to evaluate our models, we first need to load the validation dataset. As before, please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19492a4c-6ec7-4643-82c0-ce448ba0b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Model Evaluation and Accuracy Comparison\n",
    "print(\"Starting PyTorch model evaluation phase...\")\n",
    "print(\"This evaluation will test all quantized models against the validation dataset\")\n",
    "\n",
    "# Evaluate original floating-point PyTorch model accuracy\n",
    "print(\"\\n=== Original PyTorch Model Evaluation ===\")\n",
    "evaluate(float_model, val_dataloader, 'Float')\n",
    "\n",
    "# Evaluate PTQ quantized PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch PTQ Model Evaluation ===\")\n",
    "evaluate(quantized_model, val_dataloader, 'PTQ_Pytorch')\n",
    "\n",
    "# Evaluate PTQ + Mixed Precision PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch PTQ + Mixed Precision Model Evaluation ===\")\n",
    "evaluate(quantized_model2, val_dataloader, 'PTQ_Pytorch_MixP')\n",
    "\n",
    "# Evaluate GPTQ quantized PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch GPTQ Model Evaluation ===\")\n",
    "evaluate(quantized_model3, val_dataloader, 'GPTQ_Pytorch')\n",
    "\n",
    "# Evaluate GPTQ + Mixed Precision PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch GPTQ + Mixed Precision Model Evaluation ===\")\n",
    "evaluate(quantized_model4, val_dataloader, 'GPTQ_Pytorch_MixP')\n",
    "print(\"Fisish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37baa56",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCTWrapper with a few lines of code.\n",
    "\n",
    "MCT can deliver competitive results across a wide range of tasks and network architectures. For more details, [check out the paper:](https://arxiv.org/abs/2109.09113).\n",
    "\n",
    "## Copyrights\n",
    "\n",
    "Copyright 2024 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

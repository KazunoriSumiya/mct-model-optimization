{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a91956",
   "metadata": {},
   "source": [
    "# Model Compression Toolkit (MCT) Wrapper API Comprehensive Quantization Comparison (pytorch)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/SonySemiconductorSolutions/mct-model-optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_mct_wrapper.ipynb)\n",
    "\n",
    "## Overview \n",
    "This notebook provides a comprehensive demonstration of the MCT (Model Compression Toolkit) Wrapper API functionality, showcasing five different quantization methods on a MobileNetV2 model. The tutorial systematically compares the implementation, performance characteristics, and accuracy trade-offs of each quantization approach: PTQ (Post-Training Quantization), PTQ with Mixed Precision, GPTQ (Gradient-based PTQ), GPTQ with Mixed Precision. Each method utilizes the unified MCTWrapper interface for consistent implementation and comparison.\n",
    "\n",
    "## Summary\n",
    "1. **Environment Setup**: Import required libraries and configure MCT with MobileNetV2 model\n",
    "2. **Dataset Preparation**: Load and prepare ImageNet validation dataset with representative data generation\n",
    "3. **PTQ Implementation**: Execute basic Post-Training Quantization with 8-bit precision and bias correction\n",
    "4. **PTQ + Mixed Precision**: Apply intelligent bit-width allocation based on layer sensitivity analysis (75% compression ratio)\n",
    "5. **GPTQ Implementation**: Perform gradient-based optimization with 5-epoch fine-tuning for enhanced accuracy\n",
    "6. **GPTQ + Mixed Precision**: Combine gradient optimization with mixed precision for optimal accuracy-compression trade-off\n",
    "7. **Performance Evaluation**: Comprehensive accuracy assessment and comparison across all quantization methods\n",
    "8. **Results Analysis**: Compare model sizes, inference accuracy, and quantization trade-offs\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ff6fa5-a840-4aa3-b9b2-b95187ceb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for PyTorch deep learning and data handling\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.datasets import ImageNet\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Generator, Any, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509ebef7-d1e3-4d24-aaa8-1fa40330072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 09:29:28.302532: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-31 09:29:28.353880: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-31 09:29:28.589654: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-31 09:29:28.589687: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-31 09:29:28.591655: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-31 09:29:28.722651: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-31 09:29:29.607421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Configure system path to include MCT library for local development\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu/wrapper/sonyfork/mct-model-optimization')\n",
    "\n",
    "#pip install -q tensorflow\n",
    "#import importlib\n",
    "#if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "#   !pip install model_compression_toolkit\n",
    "\n",
    "# Import Model Compression Toolkit (MCT) core functionality for PyTorch\n",
    "import model_compression_toolkit as mct\n",
    "from model_compression_toolkit.core import QuantizationErrorMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd23b21",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6451112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and setup ImageNet validation dataset if not already present\n",
    "if not os.path.isdir('imagenet'):\n",
    "    # Create directory and download required ImageNet files\n",
    "    os.system('mkdir imagenet')\n",
    "    os.system('wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz')\n",
    "    os.system('wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b43e6",
   "metadata": {},
   "source": [
    "### Representative dataset construction\n",
    "We show how to create a generator for the representative dataset, which is required for post-training quantization.\n",
    "\n",
    "The representative dataset is used for collecting statistics on the inference outputs of all layers in the model.\n",
    " \n",
    "In order to decide on the size of the representative dataset, we configure the batch size and the number of calibration iterations.\n",
    "This gives us the total number of samples that will be used during PTQ (batch_size x n_iter).\n",
    "In this example we set `batch_size = 50` and `n_iter = 10`, resulting in a total of 500 representative images.\n",
    "\n",
    "Please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9566bb3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2 weights and configure dataset transforms\n",
    "weights = MobileNet_V2_Weights.IMAGENET1K_V2  # Use ImageNet V2 pre-trained weights\n",
    "# Create ImageNet validation dataset with automatic preprocessing transforms\n",
    "dataset = ImageNet(root='./imagenet', split='val', transform=weights.transforms())\n",
    "\n",
    "# Configuration parameters for representative dataset generation\n",
    "default_batch_size: int = 10  # Batch size for quantization calibration data\n",
    "n_iter: int = 5               # Number of iterations to generate representative batches\n",
    "# Create DataLoader with shuffling for representative data diversity\n",
    "dataloader = DataLoader(dataset, batch_size=default_batch_size, shuffle=True)\n",
    "\n",
    "def representative_dataset_gen() -> Generator[List[torch.Tensor], None, None]:\n",
    "    \"\"\"\n",
    "    Generator function for representative dataset used in PyTorch quantization.\n",
    "    \n",
    "    This function provides calibration data that MCT uses to:\n",
    "    - Determine optimal quantization parameters for PyTorch models\n",
    "    - Calibrate activation ranges and thresholds\n",
    "    - Configure layer-specific quantization settings\n",
    "    \n",
    "    Yields:\n",
    "        List containing PyTorch tensors for model calibration\n",
    "    \"\"\"\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    for _ in range(n_iter):\n",
    "        # Extract image batch (ignore labels) and yield as list for MCT compatibility\n",
    "        yield [next(dataloader_iter)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d31427",
   "metadata": {},
   "source": [
    "## Model Evaluation Function\n",
    "Define a comprehensive evaluation function for PyTorch models that provides accurate performance measurement on the validation dataset with GPU acceleration support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956978a3-8817-46f0-8a2c-11c25a6fc063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: torch.nn.Module, testloader: DataLoader, mode: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate PyTorch model accuracy using a DataLoader with GPU acceleration.\n",
    "    \n",
    "    This function performs complete accuracy evaluation by:\n",
    "    - Moving model and data to available device (GPU/CPU)\n",
    "    - Running inference in evaluation mode (no gradient computation)\n",
    "    - Computing Top-1 accuracy across the entire validation set\n",
    "    - Providing progress tracking during evaluation\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to evaluate (float or quantized)\n",
    "        testloader: DataLoader containing validation dataset\n",
    "        mode: String identifier for logging (e.g., 'Float', 'PTQ_Pytorch')\n",
    "    \n",
    "    Returns:\n",
    "        float: Top-1 accuracy percentage\n",
    "    \"\"\"\n",
    "    # Determine best available device for inference\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Perform inference without gradient computation for efficiency\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass to get predictions\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate and display accuracy\n",
    "    val_acc = (100 * correct / total)\n",
    "    print(mode + ' Accuracy: %.2f%%' % val_acc)\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0ee68",
   "metadata": {},
   "source": [
    "## Model Post-Training quantization using MCTWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a04842-364c-466a-95d5-0c58c25730fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator to provide consistent logging and error handling for quantization functions\n",
    "def decorator(func: Callable[[torch.nn.Module], Tuple[bool, torch.nn.Module]]) -> Callable[[torch.nn.Module], Tuple[bool, torch.nn.Module]]:\n",
    "    \"\"\"\n",
    "    Wrapper decorator that provides:\n",
    "    - Consistent start/end logging for quantization operations\n",
    "    - Automatic error handling and program termination on failure\n",
    "    - Success/failure status tracking for all quantization methods\n",
    "    \n",
    "    Args:\n",
    "        func: Quantization function to be decorated\n",
    "    \n",
    "    Returns:\n",
    "        Wrapped function with enhanced logging and error handling\n",
    "    \"\"\"\n",
    "    def wrapper(*args: Any, **kwargs: Any) -> Tuple[bool, torch.nn.Module]:\n",
    "        print(f\"----------------- {func.__name__} Start ---------------\")\n",
    "        flag, result = func(*args, **kwargs)\n",
    "        print(f\"----------------- {func.__name__} End -----------------\")\n",
    "        if not flag:exit()\n",
    "        return flag, result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556d1d9",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ (Post-Training Quantization) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe46712",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Pytorch(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization (PTQ) on PyTorch model.\n",
    "    \n",
    "    PTQ for PyTorch provides:\n",
    "    - Fast quantization without model retraining\n",
    "    - Standard 8-bit integer quantization\n",
    "    - Efficient calibration using representative data\n",
    "    - Direct ONNX export for deployment\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch PTQ quantization\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    use_internal_tpc = True                # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_mixed_precision = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch PTQ\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # Quantization configuration parameters\n",
    "        ['activation_error_method', QuantizationErrorMethod.MSE, 'Error metric for activation quantization'],\n",
    "        ['weights_bias_correction', True, 'Enable bias correction for weights'],\n",
    "        ['z_threshold', float('inf'), 'Threshold for zero-point quantization'],\n",
    "        ['linear_collapsing', True, 'Enable linear layer collapsing optimization'],\n",
    "        ['residual_collapsing', True, 'Enable residual connection collapsing'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Pytorch.onnx', 'Path to save quantized model as ONNX']\n",
    "    ]\n",
    "\n",
    "    # Execute PyTorch PTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_internal_tpc, use_mixed_precision, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6df571",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run PTQ + Mixed Precision Quantization with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8227d00b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def PTQ_Pytorch_mixed_precision(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Post-Training Quantization with Mixed Precision (PTQ + mixed_precision) on PyTorch model.\n",
    "    \n",
    "    Mixed Precision PTQ for PyTorch offers:\n",
    "    - Automatic bit-width selection per layer\n",
    "    - Optimal size/accuracy trade-off\n",
    "    - Resource-constrained quantization\n",
    "    - Advanced sensitivity analysis for PyTorch models\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch PTQ with mixed precision\n",
    "    method = 'PTQ'                    # Post-Training Quantization method\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    use_internal_tpc = True                # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_mixed_precision = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch PTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision sensitivity analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based scores for layer importance ranking'],\n",
    "        \n",
    "        # Resource constraint configuration (more aggressive for PyTorch)\n",
    "        ['weights_compression_ratio', 0.5, 'Target compression ratio for model weights (50% reduction)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_PTQ_Pytorch_mixed_precision.onnx', 'Path to save mixed precision quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute PyTorch mixed precision PTQ using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_internal_tpc, use_mixed_precision, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d6a40",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ (Gradient-based PTQ) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85c9da67",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Pytorch(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization (GPTQ) on PyTorch model.\n",
    "    \n",
    "    GPTQ for PyTorch provides:\n",
    "    - Advanced gradient-based quantization optimization\n",
    "    - Fine-tuning during quantization process\n",
    "    - Superior accuracy preservation compared to PTQ\n",
    "    - Optimized parameter updates using representative data\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch GPTQ quantization\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    use_internal_tpc = True                # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_mixed_precision = False                  # Disable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch GPTQ\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default Adam)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Pytorch.onnx', 'Path to save GPTQ quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute PyTorch GPTQ quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_internal_tpc, use_mixed_precision, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e757e27",
   "metadata": {
    "cell_marker": "#########################################################################",
    "lines_to_next_cell": 0
   },
   "source": [
    "Run GPTQ + Mixed Precision Quantization with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2005ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator\n",
    "def GPTQ_Pytorch_mixed_precision(float_model: torch.nn.Module) -> Tuple[bool, torch.nn.Module]:\n",
    "    \"\"\"\n",
    "    Perform Gradient-based Post-Training Quantization with Mixed Precision (GPTQ + mixed_precision).\n",
    "    \n",
    "    This advanced method combines:\n",
    "    - GPTQ: Gradient-based optimization for optimal quantization parameters\n",
    "    - Mixed Precision: Automatic bit-width selection for each layer\n",
    "    \n",
    "    Provides the best quantization results for PyTorch models with:\n",
    "    - Maximum accuracy preservation\n",
    "    - Optimal model size reduction\n",
    "    - Layer-wise precision optimization\n",
    "    - Advanced gradient-based calibration\n",
    "    \n",
    "    Args:\n",
    "        float_model: Original floating-point PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success_flag, quantized_model)\n",
    "    \"\"\"\n",
    "    # Configuration for PyTorch GPTQ with mixed precision\n",
    "    method = 'GPTQ'                   # Gradient-based Post-Training Quantization\n",
    "    framework = 'pytorch'             # Target framework (PyTorch)\n",
    "    use_internal_tpc = True                # Use external EdgeMDT Target Platform Capabilities\n",
    "    use_mixed_precision = True                   # Enable mixed-precision quantization\n",
    "\n",
    "    # Parameter configuration for PyTorch GPTQ with Mixed Precision\n",
    "    param_items = [\n",
    "        # Platform configuration\n",
    "        ['target_platform_version', 'v1', 'Target platform capabilities version'],\n",
    "        \n",
    "        # GPTQ-specific training parameters\n",
    "        ['n_epochs', 5, 'Number of epochs for gradient-based fine-tuning'],\n",
    "        ['optimizer', None, 'Optimizer for fine-tuning (None = use default Adam)'],\n",
    "        \n",
    "        # Mixed precision configuration\n",
    "        ['num_of_images', 5, 'Number of images for mixed precision sensitivity analysis'],\n",
    "        ['use_hessian_based_scores', False, 'Use Hessian-based scores for layer importance'],\n",
    "        \n",
    "        # Resource constraint configuration\n",
    "        ['weights_compression_ratio', 0.5, 'Target compression ratio for model weights (50% reduction)'],\n",
    "        \n",
    "        # Output configuration\n",
    "        ['save_model_path', './qmodel_GPTQ_Pytorch_mixed_precision.onnx', 'Path to save GPTQ+mixed_precision quantized model']\n",
    "    ]\n",
    "\n",
    "    # Execute advanced PyTorch GPTQ+mixed_precision quantization using MCTWrapper\n",
    "    wrapper = mct.wrapper.mct_wrapper.MCTWrapper()\n",
    "    flag, quantized_model = wrapper.quantize_and_export(\n",
    "        float_model, method, framework, use_internal_tpc, use_mixed_precision, \n",
    "        representative_dataset_gen, param_items)\n",
    "    return flag, quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07704042",
   "metadata": {},
   "source": [
    "### Run model Post-Training Quantization\n",
    "Lastly, we quantize our model using MCTWrapper API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6eee799-c5e8-4004-b120-fa23d08da74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2 model with ImageNet weights for quantization experiments\n",
    "float_model = mobilenet_v2(weights=weights)\n",
    "\n",
    "# Create DataLoader for validation/evaluation with larger batch size for efficiency\n",
    "val_dataloader = DataLoader(dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3d5d54c-0686-4c81-a24b-d8c1d84deea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PyTorch quantization experiments with different methods...\n"
     ]
    }
   ],
   "source": [
    "# Execute all PyTorch quantization methods on the same base model for comparison\n",
    "print(\"Starting PyTorch quantization experiments with different methods...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41b29a22-dbbd-4aac-aba7-d3281324d31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Pytorch Start ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Model Compression Toolkit:DepthwiseConv2D is not in model.\n",
      "Statistics Collection: 5it [00:06,  1.36s/it]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 102/102 [00:09<00:00, 10.85it/s]\n",
      "WARNING:Model Compression Toolkit:Attribute 'metadata' not found in the model or its submodules.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/mct_quantizers/pytorch/quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  threshold = torch.tensor(threshold, dtype=torch.float32).to(get_working_device())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Pytorch End -----------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Basic Post-Training Quantization for PyTorch\n",
    "flag, quantized_model = PTQ_Pytorch(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14e47383-d512-47bb-8602-bbc284f3786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Pytorch_mixed_precision Start ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Model Compression Toolkit:DepthwiseConv2D is not in model.\n",
      "WARNING:Model Compression Toolkit:DepthwiseConv2D is not in model.\n",
      "Statistics Collection: 5it [00:07,  1.45s/it]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 102/102 [00:27<00:00,  3.68it/s]\n",
      "53it [00:13,  3.95it/s]\n",
      "WARNING:Model Compression Toolkit:Attribute 'metadata' not found in the model or its submodules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/ubuntu/.local/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/abc22c3cf97b4b12af83b432cf3e3a70-pulp.mps -sec 60 -timeMode elapsed -branch -printingOptions all -solution /tmp/abc22c3cf97b4b12af83b432cf3e3a70-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 112 COLUMNS\n",
      "At line 961 RHS\n",
      "At line 1069 BOUNDS\n",
      "At line 1229 ENDATA\n",
      "Problem MODEL has 107 rows, 212 columns and 477 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "seconds was changed from 1e+100 to 60\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 0.168521 - 0.00 seconds\n",
      "Cgl0004I processed model has 53 rows, 157 columns (157 integer (157 of which binary)) and 261 elements\n",
      "Cbc0038I Initial state - 1 integers unsatisfied sum - 0.0975\n",
      "Cbc0038I Pass   1: suminf.    0.09750 (1) obj. 0.168521 iterations 10\n",
      "Cbc0038I Solution found of 0.325821\n",
      "Cbc0038I Before mini branch and bound, 156 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 0 rows 0 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.00 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.310082\n",
      "Cbc0038I Reduced cost fixing fixed 31 variables on major pass 2\n",
      "Cbc0038I Pass   2: suminf.    0.09750 (1) obj. 0.168521 iterations 0\n",
      "Cbc0038I Pass   3: suminf.    0.09030 (1) obj. 0.310082 iterations 14\n",
      "Cbc0038I Pass   4: suminf.    0.96191 (3) obj. 0.310082 iterations 14\n",
      "Cbc0038I Pass   5: suminf.    0.55664 (3) obj. 0.310082 iterations 4\n",
      "Cbc0038I Pass   6: suminf.    0.36350 (1) obj. 0.250322 iterations 8\n",
      "Cbc0038I Pass   7: suminf.    0.02063 (1) obj. 0.310082 iterations 11\n",
      "Cbc0038I Pass   8: suminf.    0.75006 (3) obj. 0.310082 iterations 13\n",
      "Cbc0038I Pass   9: suminf.    0.75006 (3) obj. 0.310082 iterations 3\n",
      "Cbc0038I Pass  10: suminf.    0.49898 (1) obj. 0.310082 iterations 11\n",
      "Cbc0038I Pass  11: suminf.    0.06390 (1) obj. 0.234251 iterations 10\n",
      "Cbc0038I Pass  12: suminf.    0.43542 (2) obj. 0.27832 iterations 18\n",
      "Cbc0038I Solution found of 0.291653\n",
      "Cbc0038I Before mini branch and bound, 113 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 2 rows 24 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.291653 to 0.178012 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.176105\n",
      "Cbc0038I Reduced cost fixing fixed 75 variables on major pass 3\n",
      "Cbc0038I Pass  13: suminf.    0.09750 (1) obj. 0.168521 iterations 0\n",
      "Cbc0038I Pass  14: suminf.    0.14102 (1) obj. 0.176105 iterations 11\n",
      "Cbc0038I Pass  15: suminf.    0.21707 (3) obj. 0.176105 iterations 5\n",
      "Cbc0038I Pass  16: suminf.    0.21707 (3) obj. 0.176105 iterations 2\n",
      "Cbc0038I Pass  17: suminf.    0.14552 (1) obj. 0.176105 iterations 2\n",
      "Cbc0038I Pass  18: suminf.    0.11910 (1) obj. 0.1715 iterations 4\n",
      "Cbc0038I Pass  19: suminf.    0.09750 (1) obj. 0.173009 iterations 2\n",
      "Cbc0038I Pass  20: suminf.    0.09750 (1) obj. 0.173009 iterations 0\n",
      "Cbc0038I Pass  21: suminf.    0.11527 (1) obj. 0.176105 iterations 4\n",
      "Cbc0038I Pass  22: suminf.    0.91278 (3) obj. 0.176105 iterations 2\n",
      "Cbc0038I Pass  23: suminf.    0.91278 (3) obj. 0.176105 iterations 0\n",
      "Cbc0038I Pass  24: suminf.    0.11325 (1) obj. 0.176105 iterations 2\n",
      "Cbc0038I Pass  25: suminf.    0.08940 (1) obj. 0.171948 iterations 3\n",
      "Cbc0038I Pass  26: suminf.    0.47901 (3) obj. 0.176105 iterations 2\n",
      "Cbc0038I Pass  27: suminf.    0.47901 (3) obj. 0.176105 iterations 2\n",
      "Cbc0038I Pass  28: suminf.    0.10277 (1) obj. 0.176105 iterations 2\n",
      "Cbc0038I Pass  29: suminf.    0.07980 (1) obj. 0.172103 iterations 3\n",
      "Cbc0038I Pass  30: suminf.    0.07440 (1) obj. 0.173762 iterations 1\n",
      "Cbc0038I Pass  31: suminf.    0.07440 (1) obj. 0.173762 iterations 0\n",
      "Cbc0038I Pass  32: suminf.    0.08784 (1) obj. 0.176105 iterations 3\n",
      "Cbc0038I Pass  33: suminf.    0.65875 (2) obj. 0.174177 iterations 8\n",
      "Cbc0038I Pass  34: suminf.    0.48895 (2) obj. 0.176105 iterations 3\n",
      "Cbc0038I Pass  35: suminf.    0.08048 (1) obj. 0.172971 iterations 4\n",
      "Cbc0038I Pass  36: suminf.    0.09846 (1) obj. 0.176105 iterations 3\n",
      "Cbc0038I Pass  37: suminf.    0.93526 (3) obj. 0.176105 iterations 7\n",
      "Cbc0038I Pass  38: suminf.    0.93526 (3) obj. 0.176105 iterations 2\n",
      "Cbc0038I Pass  39: suminf.    0.08869 (1) obj. 0.176105 iterations 2\n",
      "Cbc0038I Pass  40: suminf.    0.06578 (1) obj. 0.172111 iterations 3\n",
      "Cbc0038I Pass  41: suminf.    0.32766 (3) obj. 0.176105 iterations 5\n",
      "Cbc0038I Pass  42: suminf.    0.06240 (1) obj. 0.1754 iterations 3\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 126 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 16 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.178012 to 0.173456 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.171968\n",
      "Cbc0038I Reduced cost fixing fixed 86 variables on major pass 4\n",
      "Cbc0038I Pass  42: suminf.    0.09750 (1) obj. 0.168521 iterations 0\n",
      "Cbc0038I Pass  43: suminf.    0.11728 (1) obj. 0.171968 iterations 11\n",
      "Cbc0038I Pass  44: suminf.    0.07238 (1) obj. 0.170143 iterations 5\n",
      "Cbc0038I Pass  45: suminf.    0.07238 (1) obj. 0.170143 iterations 0\n",
      "Cbc0038I Pass  46: suminf.    0.08285 (1) obj. 0.171968 iterations 2\n",
      "Cbc0038I Pass  47: suminf.    0.08048 (1) obj. 0.171203 iterations 3\n",
      "Cbc0038I Pass  48: suminf.    0.08048 (1) obj. 0.171203 iterations 0\n",
      "Cbc0038I Pass  49: suminf.    0.08487 (1) obj. 0.171968 iterations 2\n",
      "Cbc0038I Pass  50: suminf.    0.85685 (3) obj. 0.171968 iterations 7\n",
      "Cbc0038I Pass  51: suminf.    0.10950 (1) obj. 0.17191 iterations 2\n",
      "Cbc0038I Pass  52: suminf.    0.10983 (1) obj. 0.171968 iterations 2\n",
      "Cbc0038I Pass  53: suminf.    0.33517 (3) obj. 0.171968 iterations 5\n",
      "Cbc0038I Pass  54: suminf.    0.16080 (1) obj. 0.171801 iterations 2\n",
      "Cbc0038I Pass  55: suminf.    0.16176 (1) obj. 0.171968 iterations 2\n",
      "Cbc0038I Pass  56: suminf.    0.30534 (3) obj. 0.171968 iterations 2\n",
      "Cbc0038I Pass  57: suminf.    0.46790 (3) obj. 0.171968 iterations 6\n",
      "Cbc0038I Pass  58: suminf.    0.11910 (1) obj. 0.1715 iterations 2\n",
      "Cbc0038I Pass  59: suminf.    0.12179 (1) obj. 0.171968 iterations 2\n",
      "Cbc0038I Pass  60: suminf.    0.81673 (3) obj. 0.171968 iterations 1\n",
      "Cbc0038I Pass  61: suminf.    0.30097 (3) obj. 0.171968 iterations 2\n",
      "Cbc0038I Pass  62: suminf.    0.30097 (3) obj. 0.171968 iterations 0\n",
      "Cbc0038I Pass  63: suminf.    0.11357 (1) obj. 0.171968 iterations 2\n",
      "Cbc0038I Pass  64: suminf.    0.10358 (1) obj. 0.170227 iterations 2\n",
      "Cbc0038I Pass  65: suminf.    0.81579 (3) obj. 0.171968 iterations 4\n",
      "Cbc0038I Pass  66: suminf.    0.11888 (1) obj. 0.171164 iterations 1\n",
      "Cbc0038I Pass  67: suminf.    0.12349 (1) obj. 0.171968 iterations 2\n",
      "Cbc0038I Pass  68: suminf.    0.66211 (3) obj. 0.171968 iterations 5\n",
      "Cbc0038I Pass  69: suminf.    0.66211 (3) obj. 0.171968 iterations 1\n",
      "Cbc0038I Pass  70: suminf.    0.09113 (1) obj. 0.171968 iterations 2\n",
      "Cbc0038I Pass  71: suminf.    0.08438 (1) obj. 0.170791 iterations 2\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 130 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 14 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.173456 to 0.172171 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.170934\n",
      "Cbc0038I Reduced cost fixing fixed 89 variables on major pass 5\n",
      "Cbc0038I Pass  71: suminf.    0.09750 (1) obj. 0.168521 iterations 0\n",
      "Cbc0038I Pass  72: suminf.    0.11135 (1) obj. 0.170934 iterations 11\n",
      "Cbc0038I Pass  73: suminf.    0.05910 (1) obj. 0.170785 iterations 2\n",
      "Cbc0038I Pass  74: suminf.    0.05910 (1) obj. 0.170785 iterations 0\n",
      "Cbc0038I Pass  75: suminf.    0.05996 (1) obj. 0.170934 iterations 1\n",
      "Cbc0038I Pass  76: suminf.    0.88764 (3) obj. 0.170934 iterations 2\n",
      "Cbc0038I Pass  77: suminf.    0.88764 (3) obj. 0.170934 iterations 0\n",
      "Cbc0038I Pass  78: suminf.    0.11336 (1) obj. 0.170934 iterations 2\n",
      "Cbc0038I Pass  79: suminf.    0.10560 (1) obj. 0.169581 iterations 1\n",
      "Cbc0038I Pass  80: suminf.    0.95609 (3) obj. 0.170934 iterations 3\n",
      "Cbc0038I Pass  81: suminf.    0.09398 (1) obj. 0.170637 iterations 1\n",
      "Cbc0038I Pass  82: suminf.    0.09568 (1) obj. 0.170934 iterations 1\n",
      "Cbc0038I Pass  83: suminf.    0.15111 (3) obj. 0.170934 iterations 3\n",
      "Cbc0038I Pass  84: suminf.    0.09548 (1) obj. 0.170913 iterations 1\n",
      "Cbc0038I Pass  85: suminf.    0.09559 (1) obj. 0.170934 iterations 1\n",
      "Cbc0038I Pass  86: suminf.    0.12037 (3) obj. 0.170934 iterations 2\n",
      "Cbc0038I Pass  87: suminf.    0.47084 (3) obj. 0.170934 iterations 2\n",
      "Cbc0038I Pass  88: suminf.    0.47084 (3) obj. 0.170934 iterations 0\n",
      "Cbc0038I Pass  89: suminf.    0.11336 (1) obj. 0.170934 iterations 2\n",
      "Cbc0038I Pass  90: suminf.    0.10560 (1) obj. 0.169581 iterations 1\n",
      "Cbc0038I Pass  91: suminf.    0.47084 (3) obj. 0.170934 iterations 1\n",
      "Cbc0038I Pass  92: suminf.    0.09642 (1) obj. 0.170934 iterations 3\n",
      "Cbc0038I Pass  93: suminf.    0.09210 (1) obj. 0.17018 iterations 1\n",
      "Cbc0038I Pass  94: suminf.    0.09642 (1) obj. 0.170934 iterations 1\n",
      "Cbc0038I Pass  95: suminf.    0.65135 (3) obj. 0.170934 iterations 6\n",
      "Cbc0038I Pass  96: suminf.    0.65135 (3) obj. 0.170934 iterations 0\n",
      "Cbc0038I Pass  97: suminf.    0.09313 (1) obj. 0.170934 iterations 2\n",
      "Cbc0038I Pass  98: suminf.    0.08400 (1) obj. 0.169343 iterations 1\n",
      "Cbc0038I Pass  99: suminf.    0.76000 (2) obj. 0.170868 iterations 2\n",
      "Cbc0038I Pass 100: suminf.    0.75418 (2) obj. 0.170934 iterations 1\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 138 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 10 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.01 seconds)\n",
      "Cbc0038I After 0.01 seconds - Feasibility pump exiting with objective of 0.172171 - took 0.01 seconds\n",
      "Cbc0012I Integer solution of 0.17217062 found by feasibility pump after 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 5 columns\n",
      "Cbc0031I 5 added rows had average density of 16.8\n",
      "Cbc0013I At root node, 5 cuts changed objective from 0.1685205 to 0.17157777 in 10 passes\n",
      "Cbc0014I Cut generator 0 (Probing) - 6 row cuts average 2.0 elements, 3 column cuts (3 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 1 (Gomory) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 2 (Knapsack) - 29 row cuts average 9.7 elements, 0 column cuts (0 active)  in 0.001 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 3 (Clique) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 4 (MixedIntegerRounding2) - 2 row cuts average 15.5 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 5 (FlowCover) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 6 (TwoMirCuts) - 59 row cuts average 19.3 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0010I After 0 nodes, 1 on tree, 0.17217062 best solution, best possible 0.17157777 (0.02 seconds)\n",
      "Cbc0012I Integer solution of 0.17172942 found by rounding after 60 iterations and 2 nodes (0.02 seconds)\n",
      "Cbc0016I Integer solution of 0.17168196 found by strong branching after 74 iterations and 4 nodes (0.02 seconds)\n",
      "Cbc0001I Search completed - best objective 0.1716819585503856, took 74 iterations and 4 nodes (0.02 seconds)\n",
      "Cbc0032I Strong branching done 34 times (71 iterations), fathomed 1 nodes and fixed 0 variables\n",
      "Cbc0035I Maximum depth 1, 98 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from 0.168521 to 0.171578\n",
      "Probing was tried 17 times and created 39 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 10 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 16 times and created 48 cuts of which 0 were active after adding rounds of cuts (0.001 seconds)\n",
      "Clique was tried 10 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 10 times and created 2 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 10 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 16 times and created 114 cuts of which 0 were active after adding rounds of cuts (0.001 seconds)\n",
      "ZeroHalf was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                0.17168196\n",
      "Enumerated nodes:               4\n",
      "Total iterations:               74\n",
      "Time (CPU seconds):             0.02\n",
      "Time (Wallclock seconds):       0.02\n",
      "\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.02   (Wallclock seconds):       0.02\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/mct_quantizers/pytorch/quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  threshold = torch.tensor(threshold, dtype=torch.float32).to(get_working_device())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- PTQ_Pytorch_mixed_precision End -----------------\n"
     ]
    }
   ],
   "source": [
    "# 2. PTQ with Mixed Precision (optimized size/accuracy trade-off for PyTorch)\n",
    "flag, quantized_model2 = PTQ_Pytorch_mixed_precision(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f3cd87-dde5-4463-8e84-1b3b11e447e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Pytorch Start ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Model Compression Toolkit:DepthwiseConv2D is not in model.\n",
      "Statistics Collection: 5it [00:07,  1.41s/it]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 102/102 [00:09<00:00, 10.84it/s]\n",
      "Estimating representative dataset size: 5it [00:00, 31.62it/s]\n",
      "Hessian random iterations: 100%|██████████████████████████████████████████| 100/100 [00:14<00:00,  6.93it/s]\n",
      "Hessian random iterations: 100%|██████████████████████████████████████████| 100/100 [00:08<00:00, 11.92it/s]\n",
      "Running GPTQ optimization:   0%|                                                      | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  4.09it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  5.96it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  7.28it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  8.05it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.41it/s]\u001b[A\n",
      "Running GPTQ optimization:  20%|█████████▏                                    | 1/5 [00:00<00:02,  1.48it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  9.87it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  9.11it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.63it/s]\u001b[A\n",
      "Running GPTQ optimization:  40%|██████████████████▍                           | 2/5 [00:01<00:01,  1.70it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  9.75it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  9.85it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  9.79it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.41it/s]\u001b[A\n",
      "Running GPTQ optimization:  60%|███████████████████████████▌                  | 3/5 [00:01<00:01,  1.78it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00, 10.05it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  9.91it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.89it/s]\u001b[A\n",
      "Running GPTQ optimization:  80%|████████████████████████████████████▊         | 4/5 [00:02<00:00,  1.85it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  9.35it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  8.94it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  9.77it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.34it/s]\u001b[A\n",
      "Running GPTQ optimization: 100%|██████████████████████████████████████████████| 5/5 [00:02<00:00,  1.77it/s]\u001b[A\n",
      "WARNING:Model Compression Toolkit:Attribute 'metadata' not found in the model or its submodules.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/mct_quantizers/pytorch/quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  threshold = torch.tensor(threshold, dtype=torch.float32).to(get_working_device())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Pytorch End -----------------\n"
     ]
    }
   ],
   "source": [
    "# 3. Gradient-based PTQ (improved accuracy through fine-tuning for PyTorch)\n",
    "flag, quantized_model3 = GPTQ_Pytorch(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ecb3322-47d4-4fcd-8e6c-77037206d7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Pytorch_mixed_precision Start ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Model Compression Toolkit:DepthwiseConv2D is not in model.\n",
      "WARNING:Model Compression Toolkit:DepthwiseConv2D is not in model.\n",
      "Statistics Collection: 5it [00:06,  1.34s/it]\n",
      "Calculating quantization parameters: 100%|████████████████████████████████| 102/102 [00:26<00:00,  3.82it/s]\n",
      "53it [00:13,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/ubuntu/.local/lib/python3.10/site-packages/pulp/apis/../solverdir/cbc/linux/i64/cbc /tmp/68ea60d79f67414a878efb09770ebdfd-pulp.mps -sec 60 -timeMode elapsed -branch -printingOptions all -solution /tmp/68ea60d79f67414a878efb09770ebdfd-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 112 COLUMNS\n",
      "At line 961 RHS\n",
      "At line 1069 BOUNDS\n",
      "At line 1229 ENDATA\n",
      "Problem MODEL has 107 rows, 212 columns and 477 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "seconds was changed from 1e+100 to 60\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 0.183763 - 0.00 seconds\n",
      "Cgl0004I processed model has 53 rows, 157 columns (157 integer (157 of which binary)) and 261 elements\n",
      "Cbc0038I Initial state - 1 integers unsatisfied sum - 0.1359\n",
      "Cbc0038I Pass   1: suminf.    0.13590 (1) obj. 0.183763 iterations 8\n",
      "Cbc0038I Solution found of 0.343144\n",
      "Cbc0038I Before mini branch and bound, 156 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.00 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.327197\n",
      "Cbc0038I Reduced cost fixing fixed 36 variables on major pass 2\n",
      "Cbc0038I Pass   2: suminf.    0.13590 (1) obj. 0.183763 iterations 0\n",
      "Cbc0038I Pass   3: suminf.    0.08646 (1) obj. 0.327197 iterations 12\n",
      "Cbc0038I Pass   4: suminf.    0.54378 (3) obj. 0.327197 iterations 13\n",
      "Cbc0038I Pass   5: suminf.    0.47813 (3) obj. 0.327197 iterations 2\n",
      "Cbc0038I Pass   6: suminf.    0.40190 (1) obj. 0.275997 iterations 6\n",
      "Cbc0038I Pass   7: suminf.    0.12432 (1) obj. 0.327197 iterations 9\n",
      "Cbc0038I Pass   8: suminf.    0.74034 (3) obj. 0.327197 iterations 12\n",
      "Cbc0038I Pass   9: suminf.    0.02550 (1) obj. 0.285561 iterations 5\n",
      "Cbc0038I Pass  10: suminf.    0.25123 (1) obj. 0.327197 iterations 9\n",
      "Cbc0038I Pass  11: suminf.    1.19195 (3) obj. 0.327197 iterations 11\n",
      "Cbc0038I Pass  12: suminf.    0.02835 (1) obj. 0.29672 iterations 6\n",
      "Cbc0038I Pass  13: suminf.    0.19358 (1) obj. 0.327197 iterations 10\n",
      "Cbc0038I Pass  14: suminf.    0.91605 (3) obj. 0.327197 iterations 7\n",
      "Cbc0038I Pass  15: suminf.    0.26308 (1) obj. 0.327197 iterations 7\n",
      "Cbc0038I Pass  16: suminf.    0.35920 (1) obj. 0.309468 iterations 8\n",
      "Cbc0038I Pass  17: suminf.    1.07340 (3) obj. 0.327197 iterations 11\n",
      "Cbc0038I Pass  18: suminf.    0.32120 (1) obj. 0.327197 iterations 6\n",
      "Cbc0038I Pass  19: suminf.    0.34960 (1) obj. 0.321959 iterations 6\n",
      "Cbc0038I Pass  20: suminf.    0.77555 (3) obj. 0.327197 iterations 11\n",
      "Cbc0038I Pass  21: suminf.    0.77555 (3) obj. 0.327197 iterations 4\n",
      "Cbc0038I Pass  22: suminf.    0.15855 (1) obj. 0.273088 iterations 5\n",
      "Cbc0038I Pass  23: suminf.    0.45191 (1) obj. 0.327197 iterations 7\n",
      "Cbc0038I Pass  24: suminf.    0.46960 (3) obj. 0.327197 iterations 10\n",
      "Cbc0038I Pass  25: suminf.    0.46960 (3) obj. 0.327197 iterations 3\n",
      "Cbc0038I Pass  26: suminf.    0.48351 (1) obj. 0.327197 iterations 8\n",
      "Cbc0038I Pass  27: suminf.    0.03840 (1) obj. 0.245098 iterations 8\n",
      "Cbc0038I Pass  28: suminf.    0.43725 (3) obj. 0.327197 iterations 12\n",
      "Cbc0038I Pass  29: suminf.    0.01950 (1) obj. 0.304395 iterations 2\n",
      "Cbc0038I Pass  30: suminf.    0.14312 (1) obj. 0.327197 iterations 9\n",
      "Cbc0038I Pass  31: suminf.    1.05277 (4) obj. 0.327197 iterations 17\n",
      "Cbc0038I Rounding solution of 0.287281 is better than previous of 0.343144\n",
      "\n",
      "Cbc0038I Before mini branch and bound, 98 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 3 rows 32 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.287281 to 0.186936 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.186293\n",
      "Cbc0038I Reduced cost fixing fixed 88 variables on major pass 3\n",
      "Cbc0038I Pass  31: suminf.    0.13590 (1) obj. 0.183763 iterations 0\n",
      "Cbc0038I Pass  32: suminf.    0.14962 (1) obj. 0.186293 iterations 9\n",
      "Cbc0038I Pass  33: suminf.    0.08790 (1) obj. 0.184729 iterations 4\n",
      "Cbc0038I Pass  34: suminf.    0.08790 (1) obj. 0.184729 iterations 0\n",
      "Cbc0038I Pass  35: suminf.    0.09638 (1) obj. 0.186293 iterations 2\n",
      "Cbc0038I Pass  36: suminf.    0.28474 (3) obj. 0.186293 iterations 4\n",
      "Cbc0038I Pass  37: suminf.    0.13388 (1) obj. 0.186136 iterations 2\n",
      "Cbc0038I Pass  38: suminf.    0.13473 (1) obj. 0.186293 iterations 2\n",
      "Cbc0038I Pass  39: suminf.    0.93561 (3) obj. 0.186293 iterations 7\n",
      "Cbc0038I Pass  40: suminf.    0.93561 (3) obj. 0.186293 iterations 1\n",
      "Cbc0038I Pass  41: suminf.    0.01953 (1) obj. 0.186293 iterations 2\n",
      "Cbc0038I Pass  42: suminf.    0.01388 (1) obj. 0.18525 iterations 2\n",
      "Cbc0038I Pass  43: suminf.    0.26643 (3) obj. 0.186293 iterations 2\n",
      "Cbc0038I Pass  44: suminf.    0.26643 (3) obj. 0.186293 iterations 0\n",
      "Cbc0038I Pass  45: suminf.    0.51120 (3) obj. 0.186293 iterations 1\n",
      "Cbc0038I Pass  46: suminf.    0.51120 (3) obj. 0.186293 iterations 0\n",
      "Cbc0038I Pass  47: suminf.    0.00519 (1) obj. 0.186293 iterations 2\n",
      "Cbc0038I Pass  48: suminf.    0.00038 (1) obj. 0.185404 iterations 2\n",
      "Cbc0038I Pass  49: suminf.    0.00038 (1) obj. 0.185404 iterations 0\n",
      "Cbc0038I Pass  50: suminf.    0.26577 (2) obj. 0.186293 iterations 3\n",
      "Cbc0038I Pass  51: suminf.    0.00038 (1) obj. 0.185404 iterations 3\n",
      "Cbc0038I Pass  52: suminf.    0.12980 (1) obj. 0.186293 iterations 2\n",
      "Cbc0038I Pass  53: suminf.    0.12038 (1) obj. 0.184555 iterations 1\n",
      "Cbc0038I Pass  54: suminf.    0.12980 (1) obj. 0.186293 iterations 2\n",
      "Cbc0038I Pass  55: suminf.    0.12038 (1) obj. 0.184555 iterations 2\n",
      "Cbc0038I Pass  56: suminf.    0.82769 (3) obj. 0.186293 iterations 6\n",
      "Cbc0038I Pass  57: suminf.    0.82769 (3) obj. 0.186293 iterations 1\n",
      "Cbc0038I Pass  58: suminf.    0.11682 (1) obj. 0.186293 iterations 2\n",
      "Cbc0038I Pass  59: suminf.    0.11010 (1) obj. 0.185053 iterations 2\n",
      "Cbc0038I Pass  60: suminf.    0.11010 (1) obj. 0.184856 iterations 2\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 134 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 12 columns\n",
      "Cbc0038I Mini branch and bound improved solution from 0.186936 to 0.185117 (0.01 seconds)\n",
      "Cbc0038I Round again with cutoff of 0.184704\n",
      "Cbc0038I Reduced cost fixing fixed 94 variables on major pass 4\n",
      "Cbc0038I Pass  60: suminf.    0.13590 (1) obj. 0.183763 iterations 0\n",
      "Cbc0038I Pass  61: suminf.    0.14100 (1) obj. 0.184704 iterations 9\n",
      "Cbc0038I Pass  62: suminf.    0.81196 (3) obj. 0.184704 iterations 3\n",
      "Cbc0038I Pass  63: suminf.    0.61759 (3) obj. 0.184704 iterations 2\n",
      "Cbc0038I Pass  64: suminf.    0.12500 (1) obj. 0.184704 iterations 2\n",
      "Cbc0038I Pass  65: suminf.    0.12240 (1) obj. 0.184224 iterations 2\n",
      "Cbc0038I Pass  66: suminf.    0.62727 (3) obj. 0.184704 iterations 1\n",
      "Cbc0038I Pass  67: suminf.    0.62727 (3) obj. 0.184704 iterations 1\n",
      "Cbc0038I Pass  68: suminf.    0.12630 (1) obj. 0.184407 iterations 3\n",
      "Cbc0038I Pass  69: suminf.    0.12630 (1) obj. 0.184407 iterations 0\n",
      "Cbc0038I Pass  70: suminf.    0.12791 (1) obj. 0.184704 iterations 2\n",
      "Cbc0038I Pass  71: suminf.    0.12630 (1) obj. 0.184407 iterations 2\n",
      "Cbc0038I Pass  72: suminf.    0.11357 (1) obj. 0.184704 iterations 3\n",
      "Cbc0038I Pass  73: suminf.    0.11280 (1) obj. 0.184561 iterations 2\n",
      "Cbc0038I Pass  74: suminf.    0.11357 (1) obj. 0.184704 iterations 2\n",
      "Cbc0038I Pass  75: suminf.    0.11357 (1) obj. 0.184704 iterations 1\n",
      "Cbc0038I Pass  76: suminf.    1.06911 (3) obj. 0.184704 iterations 4\n",
      "Cbc0038I Pass  77: suminf.    0.13388 (1) obj. 0.184401 iterations 2\n",
      "Cbc0038I Pass  78: suminf.    0.13552 (1) obj. 0.184704 iterations 2\n",
      "Cbc0038I Pass  79: suminf.    0.88274 (3) obj. 0.184704 iterations 3\n",
      "Cbc0038I Pass  80: suminf.    0.42734 (3) obj. 0.184704 iterations 4\n",
      "Cbc0038I Pass  81: suminf.    0.42734 (3) obj. 0.184704 iterations 0\n",
      "Cbc0038I Pass  82: suminf.    0.58629 (3) obj. 0.184704 iterations 2\n",
      "Cbc0038I Pass  83: suminf.    0.58629 (3) obj. 0.184704 iterations 1\n",
      "Cbc0038I Pass  84: suminf.    0.12237 (1) obj. 0.184704 iterations 2\n",
      "Cbc0038I Pass  85: suminf.    0.11970 (1) obj. 0.184212 iterations 2\n",
      "Cbc0038I Pass  86: suminf.    0.23156 (3) obj. 0.184704 iterations 4\n",
      "Cbc0038I Pass  87: suminf.    0.01590 (1) obj. 0.184612 iterations 1\n",
      "Cbc0038I Pass  88: suminf.    0.01639 (1) obj. 0.184704 iterations 2\n",
      "Cbc0038I Pass  89: suminf.    0.82522 (3) obj. 0.184704 iterations 3\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 140 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 9 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.01 seconds)\n",
      "Cbc0038I After 0.01 seconds - Feasibility pump exiting with objective of 0.185117 - took 0.01 seconds\n",
      "Cbc0012I Integer solution of 0.18511674 found by feasibility pump after 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0038I Full problem 53 rows 157 columns, reduced to 1 rows 4 columns\n",
      "Cbc0006I The LP relaxation is infeasible or too expensive\n",
      "Cbc0031I 1 added rows had average density of 8\n",
      "Cbc0013I At root node, 1 cuts changed objective from 0.1837628 to 0.18501829 in 2 passes\n",
      "Cbc0014I Cut generator 0 (Probing) - 3 row cuts average 1.3 elements, 2 column cuts (2 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 1 (Gomory) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 2 (Knapsack) - 1 row cuts average 10.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 3 (Clique) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 4 (MixedIntegerRounding2) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 5 (FlowCover) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 6 (TwoMirCuts) - 4 row cuts average 13.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0001I Search completed - best objective 0.1851167384547805, took 5 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0035I Maximum depth 0, 99 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from 0.183763 to 0.185018\n",
      "Probing was tried 2 times and created 5 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 1 times and created 1 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 1 times and created 4 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 1 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Optimal solution found\n",
      "\n",
      "Objective value:                0.18511674\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               5\n",
      "Time (CPU seconds):             0.01\n",
      "Time (Wallclock seconds):       0.01\n",
      "\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.01   (Wallclock seconds):       0.01\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hessian random iterations: 100%|██████████████████████████████████████████| 100/100 [00:14<00:00,  7.01it/s]\n",
      "Hessian random iterations: 100%|██████████████████████████████████████████| 100/100 [00:07<00:00, 12.51it/s]\n",
      "Running GPTQ optimization:   0%|                                                      | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  7.70it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  8.52it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  8.94it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.42it/s]\u001b[A\n",
      "Running GPTQ optimization:  20%|█████████▏                                    | 1/5 [00:00<00:02,  1.81it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  9.97it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  9.37it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  8.87it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  9.11it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.01it/s]\u001b[A\n",
      "Running GPTQ optimization:  40%|██████████████████▍                           | 2/5 [00:01<00:01,  1.81it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  9.23it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  9.41it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 3/5 [00:00<00:00,  9.22it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  9.32it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.49it/s]\u001b[A\n",
      "Running GPTQ optimization:  60%|███████████████████████████▌                  | 3/5 [00:01<00:01,  1.83it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  9.78it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  9.63it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  9.88it/s]\u001b[A\n",
      "Running GPTQ optimization:  80%|████████████████████████████████████▊         | 4/5 [00:02<00:00,  1.88it/s]\u001b[A\n",
      "  0%|                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 1/5 [00:00<00:00,  9.18it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 2/5 [00:00<00:00,  9.23it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 4/5 [00:00<00:00,  9.93it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.87it/s]\u001b[A\n",
      "Running GPTQ optimization: 100%|██████████████████████████████████████████████| 5/5 [00:02<00:00,  1.87it/s]\u001b[A\n",
      "WARNING:Model Compression Toolkit:Attribute 'metadata' not found in the model or its submodules.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/mct_quantizers/pytorch/quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  threshold = torch.tensor(threshold, dtype=torch.float32).to(get_working_device())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- GPTQ_Pytorch_mixed_precision End -----------------\n"
     ]
    }
   ],
   "source": [
    "# 4. GPTQ with Mixed Precision (best accuracy with optimal compression for PyTorch)\n",
    "flag, quantized_model4 = GPTQ_Pytorch_mixed_precision(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "969737a3-5181-4331-953c-243ca965f630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All PyTorch quantization methods completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"All PyTorch quantization methods completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2172853",
   "metadata": {},
   "source": [
    "## Models evaluation\n",
    "In order to evaluate our models, we first need to load the validation dataset. As before, please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19492a4c-6ec7-4643-82c0-ce448ba0b8de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PyTorch model evaluation phase...\n",
      "This evaluation will test all quantized models against the validation dataset\n",
      "\n",
      "=== Original PyTorch Model Evaluation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████████▋                                                          | 142/1000 [00:36<03:54,  3.66it/s]"
     ]
    }
   ],
   "source": [
    "# PyTorch Model Evaluation and Accuracy Comparison\n",
    "print(\"Starting PyTorch model evaluation phase...\")\n",
    "print(\"This evaluation will test all quantized models against the validation dataset\")\n",
    "\n",
    "# Evaluate original floating-point PyTorch model accuracy\n",
    "print(\"\\n=== Original PyTorch Model Evaluation ===\")\n",
    "evaluate(float_model, val_dataloader, 'Float')\n",
    "\n",
    "# Evaluate PTQ quantized PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch PTQ Model Evaluation ===\")\n",
    "evaluate(quantized_model, val_dataloader, 'PTQ_Pytorch')\n",
    "\n",
    "# Evaluate PTQ + Mixed Precision PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch PTQ + Mixed Precision Model Evaluation ===\")\n",
    "evaluate(quantized_model2, val_dataloader, 'PTQ_Pytorch_mixed_precision')\n",
    "\n",
    "# Evaluate GPTQ quantized PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch GPTQ Model Evaluation ===\")\n",
    "evaluate(quantized_model3, val_dataloader, 'GPTQ_Pytorch')\n",
    "\n",
    "# Evaluate GPTQ + Mixed Precision PyTorch model accuracy\n",
    "print(\"\\n=== PyTorch GPTQ + Mixed Precision Model Evaluation ===\")\n",
    "evaluate(quantized_model4, val_dataloader, 'GPTQ_Pytorch_mixed_precision')\n",
    "print(\"Fisish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37baa56",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCTWrapper with a few lines of code.\n",
    "\n",
    "MCT can deliver competitive results across a wide range of tasks and network architectures. For more details, [check out the paper:](https://arxiv.org/abs/2109.09113).\n",
    "\n",
    "## Copyrights\n",
    "\n",
    "Copyright 2025 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

#  Copyright 2025 Sony Semiconductor Solutions, Inc. All rights reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#  ==============================================================================

"""
End-to-End Testing for MCTWrapper with Keras/TensorFlow Framework

This module provides comprehensive end-to-end tests for the MCTWrapper
quantization functionality using Keras/TensorFlow models. It tests various
quantization methods including PTQ, GPTQ, and their mixed-precision variants.

Test Coverage:
- Post-Training Quantization (PTQ)
- PTQ with Mixed Precision (MixP)
- Gradient Post-Training Quantization (GPTQ)
- GPTQ with Mixed Precision (MixP)
- Low-bit Quantization PTQ (LQPTQ)

The tests use a simple CNN model and random data for representative
dataset generation for quantization testing.
"""
import pytest
import tensorflow as tf
import keras
from typing import Callable, List, Any, Tuple, Iterator
import model_compression_toolkit as mct
from model_compression_toolkit.core import QuantizationErrorMethod

@pytest.fixture
def get_model():
    """
    Create a simple CNN model for Keras/TensorFlow quantization testing.
    
    Returns:
        keras.Model: Simple CNN model for testing
    """
    inputs = keras.Input(shape=(32, 32, 3))
    x1 = keras.layers.Conv2D(16, 3, padding='same', activation='relu')(inputs)
    x2 = keras.layers.Conv2D(16, 3, padding='same', activation='relu')(inputs)
    outputs = keras.layers.Concatenate(axis=-1)([x1, x2])
    return keras.Model(inputs, outputs)

@pytest.fixture
def get_representative_dataset(n_iter=5):
    """
    Create representative dataset generator for Keras/TensorFlow quantization.
    
    Returns:
        function: Generator function that yields batches of random data
    """
    def representative_dataset() -> Iterator[List]:
        for _ in range(n_iter):
            yield [tf.random.normal((1, 32, 32, 3)).numpy()]
    return representative_dataset

@pytest.mark.parametrize("quant_func", [
    "PTQ_Keras",
    "PTQ_Keras_mixed_precision",
    "GPTQ_Keras",
    "GPTQ_Keras_mixed_precision",
    # "LQPTQ_Keras",  # Add if needed
])
def test_quantization(
        quant_func: str,
        get_model: Callable[[], keras.Model],
        get_representative_dataset: Callable[[], Iterator[List[Any]]]
        ) -> None:
    """
    Test end-to-end quantization workflows for Keras/TensorFlow models.
    
    Args:
        quant_func (str): Name of quantization function to test
        get_model: Fixture providing simple CNN model
        get_representative_dataset: Fixture providing representative data
        
    Test Methods:
        - PTQ_Keras: Standard Post-Training Quantization
        - PTQ_Keras_mixed_precision: PTQ with Mixed Precision optimization
        - GPTQ_Keras: Gradient-based Post-Training Quantization
        - GPTQ_Keras_mixed_precision: GPTQ with Mixed Precision optimization
    """
    
    # Get model and representative dataset using fixtures
    float_model = get_model
    representative_dataset_gen = get_representative_dataset

    # Decorator to print logs before and after function execution
    def decorator(func: Callable[[keras.Model], Tuple[bool, keras.Model]]) -> Callable[[keras.Model], Tuple[bool, keras.Model]]:
        """
        Decorator for logging quantization function execution.
        
        This decorator wraps quantization functions to provide clear logging
        of when each quantization method starts and ends, and handles any
        failures by terminating execution.
        
        Args:
            func: Quantization function to wrap
            
        Returns:
            function: Wrapped function with logging capabilities
        """
        def wrapper(*args: Any, **kwargs: Any) -> Tuple[bool, keras.Model]:
            print(f"----------------- {func.__name__} Start ---------------")
            flag, result = func(*args, **kwargs)
            print(f"----------------- {func.__name__} End -----------------")
            if not flag:
                exit()
            return flag, result
        return wrapper

    #########################################################################
    # Run PTQ (Post-Training Quantization) with Keras
    @decorator
    def PTQ_Keras(float_model: keras.Model) -> Tuple[bool, keras.Model]:
        """
        Execute Post-Training Quantization using MCT Target Platform Capabilities.
        
        This method applies standard PTQ without mixed precision, using MCT's
        predefined target platform capabilities for optimal quantization settings.
        """
        # Quantization method configuration
        method = 'PTQ'
        framework = 'tensorflow'
        use_internal_tpc = True
        use_mixed_precision = False

        # Configure quantization parameters for optimal model performance
        param_items = [['tpc_version', '1.0', 'The version of the TPC to use.'],
                       ['activation_error_method', QuantizationErrorMethod.MSE, 'ErrorMethod.'],
                       ['weights_bias_correction', True, ''],
                       ['z_threshold', float('inf'), ''],
                       ['linear_collapsing', True, ''],
                       ['residual_collapsing', True, ''],
                       ['save_model_path', './qmodel_PTQ_Keras.tflite', 'Path to save the model.']]

        # Execute quantization using MCTWrapper
        wrapper = mct.wrapper.mct_wrapper.MCTWrapper()
        flag, quantized_model = wrapper.quantize_and_export(float_model, method, framework, use_internal_tpc, use_mixed_precision, representative_dataset_gen, param_items)
        return flag, quantized_model

    #########################################################################
    # Run PTQ + Mixed Precision Quantization with Keras
    @decorator
    def PTQ_Keras_mixed_precision(float_model: keras.Model) -> Tuple[bool, keras.Model]:
        """
        Execute PTQ with Mixed Precision optimization for better accuracy.
        
        Mixed Precision allows different layers to use different bit-widths,
        optimizing the trade-off between model size and accuracy.
        """
        # Quantization method configuration
        method = 'PTQ'
        framework = 'tensorflow'
        use_internal_tpc = True
        use_mixed_precision = True

        # Configure mixed precision parameters for optimal compression
        param_items = [['tpc_version', '1.0', 'The version of the TPC to use.'],
                       ['num_of_images', 5, 'Whether to use Hessian-based scores for weighted average distance metric computation. This is identical to passing'],
                       ['use_hessian_based_scores', False, ' Whether to use Hessian-based scores for weighted average distance metric computation. This is identical to passing'],
                       ['weights_compression_ratio', 0.75, ''],
                       ['save_model_path', './qmodel_PTQ_Keras_mixed_precision.tflite', 'Path to save the model.']]

        # Execute quantization with mixed precision using MCTWrapper
        wrapper = mct.wrapper.mct_wrapper.MCTWrapper()
        flag, quantized_model = wrapper.quantize_and_export(float_model, method, framework, use_internal_tpc, use_mixed_precision, representative_dataset_gen, param_items)
        return flag, quantized_model

    #########################################################################
    # Run GPTQ (Gradient-based PTQ) with Keras
    @decorator
    def GPTQ_Keras(float_model: keras.Model) -> Tuple[bool, keras.Model]:
        """
        Execute Gradient-based Post-Training Quantization for enhanced accuracy.
        
        GPTQ uses gradient information to fine-tune quantization parameters,
        resulting in better model accuracy compared to standard PTQ.
        """
        # Quantization method configuration
        method = 'GPTQ'
        framework = 'tensorflow'
        use_internal_tpc = True
        use_mixed_precision = False

        # Configure GPTQ-specific parameters for gradient-based optimization
        param_items = [['target_platform_version', 'v1', 'Target platform capabilities version.'],
                       ['n_epochs', 5, 'Number of epochs for running the representative dataset for fine-tuning.'],
                       ['optimizer', None, 'optimizer to use for fine-tuning for auxiliary variable.'],
                       ['save_model_path', './qmodel_GPTQ_Keras.tflite', 'Path to save the model.']]

        # Execute gradient-based quantization using MCTWrapper
        wrapper = mct.wrapper.mct_wrapper.MCTWrapper()
        flag, quantized_model = wrapper.quantize_and_export(float_model, method, framework, use_internal_tpc, use_mixed_precision, representative_dataset_gen, param_items)
        return flag, quantized_model

    #########################################################################
    # Run GPTQ + Mixed Precision Quantization (mixed_precision) with Keras
    @decorator
    def GPTQ_Keras_mixed_precision(float_model: keras.Model) -> Tuple[bool, keras.Model]:
        method = 'GPTQ'
        framework = 'tensorflow'
        use_internal_tpc = True
        use_mixed_precision = True

        param_items = [['target_platform_version', 'v1', 'Target platform capabilities version.'],
                       ['n_epochs', 5, 'Number of epochs for running the representative dataset for fine-tuning.'],
                       ['optimizer', None, 'optimizer to use for fine-tuning for auxiliary variable.'],
                       ['num_of_images', 5, 'Whether to use Hessian-based scores for weighted average distance metric computation. This is identical to passing'],
                       ['use_hessian_based_scores', False, ' Whether to use Hessian-based scores for weighted average distance metric computation. This is identical to passing'],
                       ['weights_compression_ratio', 0.75, ''],
                       ['save_model_path', './qmodel_GPTQ_Keras_mixed_precision.tflite', 'Path to save the model.']]

        wrapper = mct.wrapper.mct_wrapper.MCTWrapper()
        flag, quantized_model = wrapper.quantize_and_export(float_model, method, framework, use_internal_tpc, use_mixed_precision, representative_dataset_gen, param_items)
        return flag, quantized_model

    #########################################################################
    # Run LQPTQ (Low-bit Quantizer PTQ) with Keras
    @decorator
    def LQPTQ_Keras(float_model: keras.Model) -> Tuple[bool, keras.Model]:
        method = 'LQPTQ'
        framework = 'tensorflow'
        use_internal_tpc = True
        use_mixed_precision = False

        param_items = [

                       ['learning_rate', 0.0001, ''],
                       ['converter_ver', 'v3.14', ''],
                       ['save_model_path', './qmodel_LQPTQ_Keras.tflite', 'Path to save the model.']]

        wrapper = mct.wrapper.wrap.MCTWrapper()
        flag, quantized_model = wrapper.quantize_and_export(float_model, method, framework, use_internal_tpc, use_mixed_precision, representative_dataset_gen, param_items)
        return flag, quantized_model

    # Execute the selected quantization method
    quant_methods = {
        "PTQ_Keras": PTQ_Keras,
        "PTQ_Keras_mixed_precision": PTQ_Keras_mixed_precision,
        "GPTQ_Keras": GPTQ_Keras,
        "GPTQ_Keras_mixed_precision": GPTQ_Keras_mixed_precision,
        # "LQPTQ_Keras": LQPTQ_Keras,  # Uncomment if needed
    }
    
    # Run the selected quantization method and verify success
    flag, quantized_model = quant_methods[quant_func](float_model)
    assert flag, f"Quantization failed for method: {quant_func}"

    # Success confirmation
    print(f"{quant_func} quantization completed successfully!")
